<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="LucienXian's Blog" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="LucienXian&apos;s Garden">
<meta property="og:type" content="website">
<meta property="og:title" content="LucienXian&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="LucienXian&#39;s Blog">
<meta property="og:description" content="LucienXian&apos;s Garden">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LucienXian&#39;s Blog">
<meta name="twitter:description" content="LucienXian&apos;s Garden">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>LucienXian's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LucienXian's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-/tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/03/05/The-Dataflow-Model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/05/The-Dataflow-Model/" itemprop="url">The Dataflow Model</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-03-05T17:41:19+08:00">
                2022-03-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="The-Dataflow-Model-A-Practical-Approach-to-Balancing-Correctness-Latency-and-Cost-in-Massive-Scale-Unbounded-Out-of-Order-Data-Processing"><a href="#The-Dataflow-Model-A-Practical-Approach-to-Balancing-Correctness-Latency-and-Cost-in-Massive-Scale-Unbounded-Out-of-Order-Data-Processing" class="headerlink" title="The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing"></a>The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing</h1><blockquote>
<p>无界、无序、大规模的数据集在日常业务中越来越普遍，并且，这些数据的消费端也发展出更复杂的需求，例如事件时间排序和数据本身特征的窗口化等，以及对消费速度有了更高的要求。本文介绍了Google在数据流模型的核心设计原则</p>
</blockquote>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>现代的数据处理是一个非常复杂且发展蓬勃的领域，从MapReduce到SQL社区内大量关于流的工作如窗口、查询系统等，再到Spark Streaming、Storm等低延迟领域的发展。然而，现有的模型和系统在许多常见场景中仍然存在不足。批处理系统会遇到导入系统带来的延迟问题，而对于流处理系统，要么缺乏大规模的容错机制，要么缺乏提供exactly-once语义的能力影响数据准确性，又或者缺少窗口所必需的时间原语等等。Lambda架构可以满足很多需求，但由于必须和构建两套系统就会带来简单性的不足。论文提出的观点是，不再关注执行引擎决定系统语义的主流思维，而是通过考虑批处理，微批处理和流传输系统之间潜在的差异（即延迟和资源成本）来选择执行引擎。</p>
<p>本文提出了一个简单统一的模型概念：</p>
<ul>
<li>允许计算event-time的有序结果，并根据数据本身的特征在无边界，无序的数据源上进行窗口聚合处理，在准确性，延迟和成本三者之间平衡；</li>
<li>拆分四个跨维度相关的管道实现：<ul>
<li>what：正在计算什么结果；</li>
<li>where：事件发生时在哪里计算；</li>
<li>when：在哪个处理时间内进行物化；</li>
<li>how：前期结果如何与后续改进相关联；</li>
</ul>
</li>
<li>将数据处理的逻辑概念与底层的物理实现分开，允许基于准确性，延迟和成本的考虑来选择批处理，微批处理或流引擎；</li>
</ul>
<p>具体而言可以分为以下几个部分：</p>
<ul>
<li><strong>A windowing model</strong>：支持未对齐的event-time窗口；</li>
<li><strong>A triggering model</strong>：将事件处理的运行时特征与输出次数坐绑定；</li>
<li><strong>incremental processing model</strong>：将数据更新整合到前面的window model和trigger model中；</li>
<li><strong>Scalable implementations</strong>：基于MillWheel流式引擎和Flume批处理引擎实现了Google cloud Dataflow的SDK；</li>
<li><strong>core principles</strong>：模型设计的核心原则；</li>
</ul>
<h3 id="Unbounded-Bounded-vs-Streaming-Batch"><a href="#Unbounded-Bounded-vs-Streaming-Batch" class="headerlink" title="Unbounded/Bounded vs Streaming/Batch"></a>Unbounded/Bounded vs Streaming/Batch</h3><p>Dataflow统一用Bounded/Unbounded Dataset来描述有限/无限数据集，而Streaming/Batch则用来特指某些执行引擎。</p>
<h3 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h3><p>Window即窗口化，是指将无限的数据集切分为有限的数据片以便进行聚合处理。对于无边的数据集，有些操作如<strong>aggregation</strong>，<strong>outer join</strong>，<strong>time-bounded</strong>都需要窗口。窗口一般是基于时间的，但也有些系统支持基于记录数的窗口，这可以理解为是逻辑时间，其中的元素按顺序依次增加逻辑时间戳。</p>
<p>窗口模型主要由三种主要分为以下三种：</p>
<p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f01d.png" alt></p>
<ul>
<li>Fixed Window：这是按固定窗口的大小定义的，比如说小时窗口或天窗口，通常是对齐窗口，每个窗口都包含了对应时间段范围内的所有数据，可以看到的是每个窗口之间没有重叠；</li>
<li>Sliding Window：这是根据窗口大小和滑动周期大小来定义的，比如说小时窗口，每一分钟滑动一次，通常情况滑动周期会比窗口更小，滑动窗口一般也是对齐的，如上图的五个滑动窗口实际上都包含了对三个键的处理；Fixed Window可以认为是窗口大小等于滑动周期大小的Sliding Window；</li>
<li>Session Window：这种类型的窗口会在数据的子集上捕捉一段时间内的活动，属于非对齐窗口，比如上图的窗口2只包含key 1，窗口3则只包含key2；</li>
</ul>
<h3 id="Time-Domain"><a href="#Time-Domain" class="headerlink" title="Time Domain"></a>Time Domain</h3><p>在流式处理中有两个关于时间的概念需要重点关注：</p>
<ul>
<li>Event Time：事件本身实际发生的时间，系统时钟时间在事件发生时的记录；</li>
<li>Processing Time：事件在系统中被处理的时间；</li>
</ul>
<p>在数据处理过程中，由于系统自身收到的影响如通信延迟，调度算法，处理时长，管道中间数据序列化等，会导致上述两个值之间存在一定的差值，诸如punctuations或watermarks之类的全局进度指标都提供了一种可视化这种差值的好方法，本文则是使用了一种类似MillWheel的水位标记，这是一个时间戳，用来表示小于这个时间戳的数据已经完全被系统处理了。理想情况下，这两个时间的差值应该为0，即事件一旦发生则马上做处理，如下图所示。但实际上，由于前面提到的原因，水位标记会偏离真实时间，这是非常正常的现象。</p>
<p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f020.png" alt></p>
<h2 id="DATAFLOW-MODEL"><a href="#DATAFLOW-MODEL" class="headerlink" title="DATAFLOW MODEL"></a>DATAFLOW MODEL</h2><p>接下来将讨论Dataflow的正式模型。</p>
<h3 id="Core-Primitives"><a href="#Core-Primitives" class="headerlink" title="Core Primitives"></a>Core Primitives</h3><p>首先从经典的批处理模型开始，Dataflow把所有的数据都抽象成键值对，并提出了两个核心的数据转换操作：</p>
<ul>
<li>ParDo：对每个输入元素都用一个用户自定义函数进行处理，生成零个活多个的输出元素，如下图所示：</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f026.png" alt></p>
<ul>
<li>GroupByKey：根据键值将元素重新分组，作为一个聚合操作，由于需要收集到所有需要的数据，需要结合窗口化一起使用；</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f02c.png" alt></p>
<h3 id="Windowing"><a href="#Windowing" class="headerlink" title="Windowing"></a>Windowing</h3><p>支持GroupByKey的系统通常会将其重新定义为GroupByKeyAndWindow，Dataflow在这里的主要贡献是支持未对齐窗口，其底层的优化则是通过下面两部来实现：</p>
<ul>
<li><strong>Set<window> AssignWindows(T datum)</window></strong>：将元素复制给若干个窗口；</li>
<li><strong>Set<window> MergeWindows(Set<window> windows)</window></window></strong>：窗口合并；</li>
</ul>
<p>为了在本地支持事件时间的窗口，这里不再是传递简单的键值对，而是传递(key, value, eventtime, window)4元组。元素进入系统时会带有事件时间的时间戳，并且在最初会分配一个磨人的全局窗口。</p>
<h4 id="Window-Assignment"><a href="#Window-Assignment" class="headerlink" title="Window Assignment"></a>Window Assignment</h4><p>窗口赋值就是指将数据拷贝到对应的窗口。下图就是一个窗口大小为2分，滑动窗口间隔为1分钟的例子。</p>
<p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f033.png" alt></p>
<h4 id="Window-Merge"><a href="#Window-Merge" class="headerlink" title="Window Merge"></a>Window Merge</h4><p>窗口合并是GroupByKeyAndWindow操作的一部分，具体来说这是一个由五部分组成的复合操作：DropTimestamps、GroupByKey、MergeWindows、GroupAlsoByWindow和ExpandToElements，其具体的含义可以参考下图：</p>
<p><img src="https://pic.imgdb.cn/item/62232ff85baa1a80ab65130d.png" alt></p>
<h3 id="Triggers-amp-Incremental-Processing"><a href="#Triggers-amp-Incremental-Processing" class="headerlink" title="Triggers &amp; Incremental Processing"></a>Triggers &amp; Incremental Processing</h3><p>能够构建未对齐的事件时间窗口是一种进步，但仍面临两个问题：</p>
<ul>
<li>为了与其他流式系统保持兼容，需要提供基于processing time和基于tuple的窗口；</li>
<li>由于事件发生时间是无序的，数据可能会慢一步到来，我们需要何时才能将窗口的结果数据发往下游；</li>
</ul>
<p>论文这里主要讨论第二种情况，就是如何保证窗口数据的完整性。最原始的想法是使用某种全局事件时间戳，比如watermark来处理，这里可以立即为一个阈值，但watermark设计的过长过短对数据处理的准确性会有一定的影响，过短会导致水位标记到达后仍有记录到达，过长则可能会使得迟到的数据影响到整个数据处理管道的watermark。</p>
<p>Dataflow的处理方法是用一种叫Trigger的机制，这种机制是受信号激励从而触发GroupByKeyAndWindow执行并输出结果的机制，相对于窗口是决定哪些event time数据被分到一组进行聚合操作，Trigger更多是决定在什么处理时间窗口的结果会被输出。Trigger提供了三种不同的模式来控制不同计算结果之间是如何关联的：</p>
<ul>
<li>Discarding：窗口数据的Trigger之后直接丢弃；</li>
<li>Accumulating：窗口的结果数据在Trigger之后持久化下来，用以支持后面的数据更新；</li>
<li>Accumulating &amp; Retracting：在第二种基础上增加了回撤结果，即窗口再次Trigger时会将上次的结果做回撤，然后将新的结果作为正常数据下发。</li>
</ul>
<h2 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h2><p>无边界的数据时数据处理的未来，有边界的数据本身也是由无边界的对应部分所包含的，并且处理数据的消费者进化的越来越快，因此需要更强大的架构支持例如事件时间顺序和未对齐的窗口等。Dataflow模型把数据处理的逻辑划分了以下几个部分：计算什么、在哪个event time范围内计算、在什么处理时间点触发计算，如果用新的结果修正之前的处理结果，这使得整个数据处理逻辑变得更加透明清晰。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/01/14/Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/01/14/Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing/" itemprop="url">Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-01-14T23:44:53+08:00">
                2022-01-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing"><a href="#Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing" class="headerlink" title="Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing"></a>Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing</h1><blockquote>
<p>Mesa是一个可扩展的分析型数据仓库，可用于存储Google广告业务相关的数据，能满足多数据中心、高可用、近实时等特性。</p>
</blockquote>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>Mesa满足了以下的要求：</p>
<ul>
<li>原子更新；</li>
<li>一致性和正确性；</li>
<li>高可用性，能承受整个数据中心或者园区挂掉；</li>
<li>近实时更新的吞吐，支持全量和增量的更新；</li>
<li>查询性能，以低延时满足高吞吐；</li>
<li>可扩展性，数据读写性能都能随着集群规模实现线性增长；</li>
<li>支持热更新表的schema；</li>
</ul>
<p>Mesa充分利用了Google内部的基础设施，包括Colossus、BigTable和BigTable等。Mesa主要是分shard存储，批量更新，每个更新分配一个版本号来满足MVCC，同时基于Paxos算法做元数据的一致性管理。</p>
<h2 id="MESA-STORAGE-SUBSYSTEM"><a href="#MESA-STORAGE-SUBSYSTEM" class="headerlink" title="MESA STORAGE SUBSYSTEM"></a>MESA STORAGE SUBSYSTEM</h2><p>Mesa存储的数据是多维度的，其中维度属性称为keys，指标属性则是values。</p>
<h3 id="The-Data-Model"><a href="#The-Data-Model" class="headerlink" title="The Data Model"></a>The Data Model</h3><p>Mesa的数据是以表的概念来维护的，每个表都会有一个schema。通常来说，表会有key空间和对应的value空间，分别就是前面提到的维度和对应的指标。同时values列需要定一个聚合函数，例如SUM、MIN、MAX等。聚合函数必须满足结合律，可选择性满足交换律。Mesa往往会存储上千张表，每个表有几百个列。如下图就是三张经典的Mesa表，其中表C是表B的物化视图：</p>
<p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a812f9.png" alt></p>
<h3 id="Updates-and-Queries"><a href="#Updates-and-Queries" class="headerlink" title="Updates and Queries"></a>Updates and Queries</h3><p>为了实现高吞吐，Mesa是通过批量的方式进行更新，上游数据源以分钟级的频率批量更新。每一个更新都有对应的一个递增版本号，更新以串行的方式进行。因此对于Mesa的查询需要提供版本号和Predicate，这样就可以在[0, n]的版本key集合里做filter。</p>
<p>如下图所示，表A和B经历了两个更小的Batch，其中表C是B的物化视图，B的更新，对应的物化视图都能保持和原表的一致性原子更新。</p>
<p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a812fd.png" alt></p>
<p>另外对于一些数据会滚的需求，Mesa支持negative facts，则对一些指标列做减法，以最终一致的实现来实现会滚。</p>
<h3 id="Versioned-Data-Management"><a href="#Versioned-Data-Management" class="headerlink" title="Versioned Data Management"></a>Versioned Data Management</h3><p>数据版本在Mesa的读写过程中扮演着非常重要的角色，但也存在一些问题，一是存储成本会变高，二是无论查询还是更新，聚合所有版本的代价也会比较高。</p>
<p>Mesa的做法就是提出Delta的概念，每一个Delta包含的是不重复key的数据，并使用[V1, V2]表示版本号，V1小于或等于V2，其中的数据就是在版本号V1和V2之间更新的key，value则是这些更新操作聚合后的。另外由于每个delta内部数据都是有序的，因此合并可以以线性的时间完成。</p>
<p>Mesa对于delta的结构分成了三层：每次批量写入都会当作是一个单例delta合并到Mesa，单例delta的V1等于V2。因为Mesa对于指标列都会有相关的聚合函数，因此delta[V1, V2]和delta[V2, V3]可以通过合并key、聚合value的方式合并成delta[V1, V3]，这些就是cumulative delta。另外还存在一个base delta，设它的版本号为[0, B], 其中B大于或等于0，生成base delta，后续任意一个[V1, V2]只要满足0 &lt;= V1 &lt;= V2 &lt;= B就可以被删除（唯一的删除条件），这就是异步的base compaction。这也是为什么Mesa仅仅支持一段时间以内的所有版本，比如24小时内的，因为更早的版本已经可能被聚合到base delta里。</p>
<p>如下图，对于版本n的查询，就可以通过聚合这三层的delta来返回值。任何时刻都存在一个基本delta[0, B], 一系列的累积delta[B+1, B+10], [B+1, B+20],[B+1, B+30],…以及B以后的所有单例delta。这样的好处就是，对于某个版本n的查询，可以方便通过查询cumulative delta来减少IO开销。举个例子，如果查询版本91的数据，在没有cumulative delta的情况下，就需要查询61-91这32次的delta；如果存在cumulative delta，则只需要一次base的查询，61-90的cumulative，外加91这一个单例delta。</p>
<p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a81301.png" alt></p>
<p>Mesa的解决思路总结起来就是：及时删除过期数据，merge小文件。</p>
<h3 id="Physical-Data-and-Index-Formats"><a href="#Physical-Data-and-Index-Formats" class="headerlink" title="Physical Data and Index Formats"></a>Physical Data and Index Formats</h3><p>Mesa的delta，无论哪些类型其存储格式都是一样的，并且都是immutable。因此Mesa关于物理数据的存储主要关注空间成本以及查询性能。关于存储，论文没介绍太多细节，主要是分成index files和data files。Mesa将delta的行按顺序存储在大小受限的data files中，若干行的数据会组织成一个row blocks，每个row blocks则是按照column进行存储（提高压缩率，并且因为查询性能的问题优先考虑解压效率高的）。Index files存储的则是row blocks第一个key的固定长度前缀以及对应row blocks在data files中的偏移量，然后就可以将index files加载进内存通过二分查找去读数据。</p>
<h2 id="MESA-SYSTEM-ARCHITECTURE"><a href="#MESA-SYSTEM-ARCHITECTURE" class="headerlink" title="MESA SYSTEM ARCHITECTURE"></a>MESA SYSTEM ARCHITECTURE</h2><h3 id="Single-Datacenter-Instance"><a href="#Single-Datacenter-Instance" class="headerlink" title="Single Datacenter Instance"></a>Single Datacenter Instance</h3><p>每一个Mesa实例都包含了两个子系统：update/maintenance系统和querying系统，这些子系统可以独立扩展。元数据信息存在BigTable里，数据文件则是存在Google的Colossus。</p>
<h4 id="Update-Maintenance-Subsystem"><a href="#Update-Maintenance-Subsystem" class="headerlink" title="Update/Maintenance Subsystem"></a>Update/Maintenance Subsystem</h4><p>update and maintenance子系统主要负责以下的操作：加载更新数据、执行表压缩，在线进行Schama修改，检查表的checksum等。这些操作都是由下图的controller/worker framework完成的。</p>
<p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a81307.png" alt></p>
<p>controller可以看作是表元数据的cache，同时负责worker的调度，worker队列的管理。controller不做任何数据相关的工作，只负责调度和元数据管理。元数据存在BigTable上，controller会去订阅表的更新，同时也是元数据唯一的修改方。</p>
<p>worker组件则是负责在每个Mesa实例中的具体数据操作工作，不同的worker是隔离的，有自己独立的职责，有一组独立的worker池。空闲的worker会定期轮询controller，请求对应类型的任务，收到工作任务后，会去验证并处理，最后则在任务完成后通知controller。图中还有一个Garbage Collector，主要是负责清理因为worker执行失败而留下的中间状态数据。worker与controller之间通过租约的方式，防止挂掉的worker一直霸占着任务，同时controller也只接受分配的worker的任务结果，确保执行安全。</p>
<h4 id="Query-Subsystem"><a href="#Query-Subsystem" class="headerlink" title="Query Subsystem"></a>Query Subsystem</h4><p>Mesa的query subsystem由下图的查询服务器组成，这些服务器接收用户查询、从元数据和数据集中查找对应内容、执行相关聚合、并在返回client前将数据转换到client协议格式。</p>
<p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a81315.png" alt></p>
<p>Mesa的客户端对不同的请求有不同要求，有些要求低延时、有些要求高吞吐。因此Mesa会通过标记工作负载和隔离、优先级等机制来满足不同的延迟和吞吐量要求。</p>
<p>出于性能的考虑，相似的数据查询往往会路由到某个查询服务器的子集（比如同一张表的查询都由某一批查询服务器负责），这样做的好处就是，查询服务器可以通过预取和缓存的方式来提供低延时保证。在启动时，每个查询服务器都会向Global Locator Service注册所主动缓存的表列表，client可以通过这个列表来决定如何路由。</p>
<h3 id="Multi-Datacenter-Deployment"><a href="#Multi-Datacenter-Deployment" class="headerlink" title="Multi-Datacenter Deployment"></a>Multi-Datacenter Deployment</h3><p>Mesa可以多中心部署，每个数据中心是相互隔离的的，有一份独立的数据。</p>
<h4 id="Consistent-Update-Mechanism"><a href="#Consistent-Update-Mechanism" class="headerlink" title="Consistent Update Mechanism"></a>Consistent Update Mechanism</h4><p>Mesa中的表都是多版本的，上游系统每几分钟生成一批更新数据以供Mesa合并，如下图，Mesa入了committer组件引入了committer组件。committer为每个更新批次分配一个新版本号，并将与更新相关的所有元数据发布到版本数据库（a globally replicated and consistent data store build on top of the Paxos consensus algorithm），应该就是spanner或者F1。committer是无状态的，可以多中心部署。</p>
<p><img src="https://pic.imgdb.cn/item/61e198002ab3f51d91a84866.png" alt></p>
<p>Mesa的controller会监听版本数据库，以检测新更新，然后将相应的工作分配给Update workers，并将更新结果报告回版本数据库。然后committer会检查verion提交的一致性条件是否满足（例如Mesa表的物化视图是否已经更新完成）。当满足提交标准时，committer将在版本数据库里更新版本号。</p>
<p>Mesa的更新机制对性能非常友好：MVCC使得Mesa不需要在查询和更新之间无锁；所有更新数据都由各Mesa实例异步合并，元数据则基于Paxos协议同步更新。</p>
<h4 id="New-Mesa-Instances"><a href="#New-Mesa-Instances" class="headerlink" title="New Mesa Instances"></a>New Mesa Instances</h4><p>Mesa会使用P2P的方法，通过一个load worker去加载新的Mesa实例，可以将表从另一个Mesa实例复制到当前的表。另外这一机制也支持从损坏的表中恢复。</p>
<h2 id="ENHANCEMENTS"><a href="#ENHANCEMENTS" class="headerlink" title="ENHANCEMENTS"></a>ENHANCEMENTS</h2><h3 id="Query-Server-Performance-Optimizations"><a href="#Query-Server-Performance-Optimizations" class="headerlink" title="Query Server Performance Optimizations"></a>Query Server Performance Optimizations</h3><p>论文中心还提到了Mesa关于查询性能的优化：</p>
<ul>
<li>delta pruning：查询服务器会检查描述每个delta包含的键范围的元数据，避免读取不必要的delta；</li>
<li>scan-to-seek：对于非第一个key有filter的查询，可以用scan-to-seek来优化索引，避免读取不必要的数据；</li>
<li>resume key：Mesa 通常以流方式将数据返回给客户端，每一次返回一个block，对于每一个block，Mesa都会附加一个resume key。如果查询超时，受影响的Mesa客户端可以透明地切换到另一个查询服务器，从resume key的地方继续查询，而不是重新执行整个查询；</li>
</ul>
<h3 id="Parallelizing-Worker-Operation"><a href="#Parallelizing-Worker-Operation" class="headerlink" title="Parallelizing Worker Operation"></a>Parallelizing Worker Operation</h3><p>Mesa利用MapReduce框架来并行化处理worker任务。</p>
<h3 id="Schema-Changes-in-Mesa"><a href="#Schema-Changes-in-Mesa" class="headerlink" title="Schema Changes in Mesa"></a>Schema Changes in Mesa</h3><p>Mesa用户经常需要修改schemas，一些常见的更改比如添加或者删除列、添加或删除索引等。Mesa使用两种技术来执行在线的schemas变更：</p>
<ul>
<li>拷贝固定版本的表数据并按照新的schema进行存储；</li>
<li>回放并更新当前版本和之前固定版本的数据；</li>
<li>更新元数据的schema；</li>
<li>直到旧schema没有查询，则删除旧的数据；</li>
</ul>
<p>但这种方法成本很高，需要临时存有两份存储资源，也需要删掉历史数据。另一种技术则是linked schema change，不再是重新灌一遍历史数据，而是对增量数据以新的schema处理。如果是新加的列，对于历史数据则以数据类型的默认值填充。但这种方法无法处理所有情况：比如删除修改列等。</p>
<h3 id="Mitigating-Data-Corruption-Problems"><a href="#Mitigating-Data-Corruption-Problems" class="headerlink" title="Mitigating Data Corruption Problems"></a>Mitigating Data Corruption Problems</h3><p>这一章主要是讲Mesa在容错方面的努力，通过在线写入的检查和定期离线的全量检查来确认数据不会有损坏。当出现数据损坏时，Mesa实例会自动从另一个实例重新加载该表的正常副本。如果都损坏了，则从备份中恢复旧版本的表并重放更新。</p>
<h2 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h2><p>本文介绍了Google近实时、可扩展的数据仓库的设计与实现——Mesa系统。Mesa支持在线查询和批量更新，同时提供强大的一致性和事务正确性保证，并且在数据模型上有非常创新的理论设计。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/01/14/Napa-Powering-Scalable-Data-Warehousing-with-Robust-Query-Performance-at-Google/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/01/14/Napa-Powering-Scalable-Data-Warehousing-with-Robust-Query-Performance-at-Google/" itemprop="url">Napa: Powering Scalable Data Warehousing with Robust Query Performance at Google</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-01-14T23:43:55+08:00">
                2022-01-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Napa-Powering-Scalable-Data-Warehousing-with-Robust-ery-Performance-at-Google"><a href="#Napa-Powering-Scalable-Data-Warehousing-with-Robust-ery-Performance-at-Google" class="headerlink" title="Napa: Powering Scalable Data Warehousing with Robust !ery Performance at Google"></a>Napa: Powering Scalable Data Warehousing with Robust !ery Performance at Google</h1><blockquote>
<p>Google产生的大量应用数据，需要有一个可扩展性强、响应时间短、可用性高和强一致性的存储提供服务。Napa就是Google用来满足这些要求的系统，其核心的数据就是使用一致的物化视图。</p>
</blockquote>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>Napa其实是用来替代Google另一款OLAP产品Mesa的，并且从Mesa迁移了PB级的历史数据，与Mesa相比，Napa的设计需求更为广泛：一是更稳定的查询性能，期望毫秒级的低查询延迟和无论集群负载如何延迟相对稳定；二是更灵活，支持用户根据需求在查询性能、数据新鲜度等方面进行取舍；三是数据写入高吞吐，在海量更新负载下，Napa实现了一个LSM范式的分布式表和视图维护框架。</p>
<h2 id="NAPA’S-DESIGN-CONSTRAINTS"><a href="#NAPA’S-DESIGN-CONSTRAINTS" class="headerlink" title="NAPA’S DESIGN CONSTRAINTS"></a>NAPA’S DESIGN CONSTRAINTS</h2><p>这一章主要是将Napa的设计过程中考虑了哪些关键目标，最理想的情况当然是以尽可能低的成本实现最高的查询性能和最高的数据新鲜度。数据新鲜度是通过将数据添加到表的时间点到可用于查询的时间点之间的维度来衡量的，而成本则主要是数据处理和存储所需要的机器资源成本。</p>
<h3 id="Clients-Need-Flexibility"><a href="#Clients-Need-Flexibility" class="headerlink" title="Clients Need Flexibility"></a>Clients Need Flexibility</h3><p>这一个前面也说了，Napa需要支持client在下面三个因素（数据新鲜度、资源成本和查询性能）之间进行权衡选择。</p>
<p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcad.png" alt></p>
<p>一个重要的考虑点在于数据写入和当前存储是否耦合，如果将新的数据和当前存储结合起来，这意味着数据的写入只有只有在应用到表及其所有视图后才会提交，这是Mesa的设计，但有一个比较大的缺点，即额外添加视图会导致数据写入变慢，这种设计虽然提高了查询速度，但牺牲了一定的数据新鲜度。另一种就是视图的生成可以作为查询的一部分选择性地完成，维护视图的异步模型会在一定程度上影响表及视图之间的一致性。</p>
<p>因此Napa需要提供一个灵活的client，以便随时调整系统对于这些目标的需求。</p>
<h2 id="DESIGN-CHOICES-MADE-BY-NAPA"><a href="#DESIGN-CHOICES-MADE-BY-NAPA" class="headerlink" title="DESIGN CHOICES MADE BY NAPA"></a>DESIGN CHOICES MADE BY NAPA</h2><p>Napa中的一个关键设计选择是依靠物化视图来实现更高的查询性能。Napa的主要架构由下图三个组件组成：</p>
<p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcb1.png" alt></p>
<ul>
<li>Ingestion framework：负责将更新数据提交到表中，这些更新在Napa称为deltas。</li>
<li>Storage framework：将更新应用于表及其视图，Napa以LSM的结构维护deltas，每个表都以一个deltas的集合方式表示。Delta不断合并以形成更大的Delta，即Compaction。视图维护层通过应用相应的SQL转换将表Delta转换为视图Delta，存储层还负责定期压缩表和视图。</li>
<li>Query serving：负责应对客户端的查询，系统在查询时执行表（或视图）的必要Delta合并。存储子系统处理更新的速度越快，查询时需要合并的增量就越少。</li>
</ul>
<p>Napa将数据写入、视图维护与查询处理进行解耦，允许client根据自己的需要在数据新鲜度、性能和成本之间进行权衡。</p>
<h3 id="Providing-Flexibility-to-Clients"><a href="#Providing-Flexibility-to-Clients" class="headerlink" title="Providing Flexibility to Clients"></a>Providing Flexibility to Clients</h3><p>Napa会将client在数据新鲜度、查询性能和成本方面的要求转换为对应的数据库配置、比如视图数量、写入任务的quota限制、查询期间的最大打开delta数量等等，这是一个动态但易于理解的数据库状态指示器。</p>
<p>为此Napa引入了称为可查询时间戳 (QT) 的概念。QT是数据新鲜度的直接指标，因为[Now() - QT]表示数据延迟，客户端可以查询到 QT 时间戳之前的所有数据。下文就介绍了三类Napa client以及系统如何使用QT来调整Napa。</p>
<ul>
<li>Tradeoff freshness：牺牲数据新鲜度即意味着Napa的QT推进会取决于查询执行时保持适度数量的视图和更少的deltas来合并。同时为了保持低资源成本，Napa的执行框架会使用更少的worker和资源进行视图维护。</li>
<li>Tradeoff query performance：牺牲查询性能即意味着Napa的QT推进会取决于更少的视图，但在查询执行时需要合并的Deltas可能相对较多。由于每个表和视图的Deltas较多，查询性能较低。简单来说就是将视图维护和压缩的成本转移到查询；</li>
<li>Tradeoff costs：以更高的成本提供良好的查询性能和数据新鲜度。Napa的QT推进取决于多个视图，并且合并时的Deltas数量非常少，从而确保更短的查询执行时间。并且花费更多的资源来增加worker来满足要求。</li>
</ul>
<h3 id="Data-Availability"><a href="#Data-Availability" class="headerlink" title="Data Availability"></a>Data Availability</h3><p>关于数据可用性，Napa的做法是将数据和元数据操作解耦，在数据中心的每个副本上异步执行数据操作，并定期在Spanner使用元数据操作以确保副本彼此保持同步，确保副本之间的一致性，将同步和异步的模式进行组合。</p>
<h2 id="SYSTEM-ARCHITECTURE"><a href="#SYSTEM-ARCHITECTURE" class="headerlink" title="SYSTEM ARCHITECTURE"></a>SYSTEM ARCHITECTURE</h2><p>如下图，Napa的架构由数据平面和控制平面组成，数据平面则是由上述的数据写入、存储和查询服务组成的。控制平面则负责协调各个子系统之间的工作，多个数据中心同步和元数据事务。</p>
<p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcbb.png" alt></p>
<p>Napa大量依赖了Google现有的基础设施，比如Napa的表数据就是存储在Colossus文件系统上、Spanner负责元数据管理和系统状态存储、F1 Query则用来做查询服务和大规模的数据处理。</p>
<p>Napa客户端使用ETL管道将数据写入表中，数据摄取框架可以承受高达数十GB/s压缩数据的负载。同时，Napa的存储组件通过压缩表和视图的delta，创建更大的delta，从而减少在线查询期间的合并操作。查询服务则在运行时进行必须要缓存、预取和合并Delta的操作，提供低延迟和稳定的查询，前者是通过查询定向到预先计算的物化视图实现的，后者则是通过控制compaction和一些系列IO优化技术来减少长尾。</p>
<p>Napa依赖视图作为获得良好查询性能的主要机制，包含物化视图的Napa表按其主键进行排序、索引和Range分区。与现有数据库更多倾向于使用Scan的查询处理不同，Napa考虑到负载和查询性能等要求，最终选择了按索引key查找，当然这可能会带来热点和负载均衡等问题，但论文中并没有详细讲述如何应对的。</p>
<p>Napa的控制平面则调度compaction和视图的更新任务，以控制表的deltas保持在一个配置值。如前所述，QT构成了数据新鲜度的基础，查询系统使用它来提供稳定的查询性能。如果数据相对落后了（写入慢了），Napa会通过牺牲查询性能来将写入速度拉回到一个合理的配置值。</p>
<h2 id="INGESTING-TRILLIONS-OF-ROWS"><a href="#INGESTING-TRILLIONS-OF-ROWS" class="headerlink" title="INGESTING TRILLIONS OF ROWS"></a>INGESTING TRILLIONS OF ROWS</h2><p>数据写入框架的目标是允许写入管道在没有显着开销的情况下将大量数据写到Napa中。该子系统的目标是接受数据、执行最少的处理并使其持久化，而不考虑后续视图维护的速度。如下图所示，所有写入的行都会被分配一个元数据时间戳用于排序，然后在满足其他持久性条件（例如复制）后标记为已提交。其次，该子系统允许通过配置增加或减少批处理、聚合复制等worker数量的方式来控制机器成本。</p>
<p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcc5.png" alt></p>
<p>客户端将要写入的数据发送到所有的Napa副本，该框架会产生写入优化的delta，但不能立即用于查询。这些就是unqueryable delta，需要在可查询前将它们进行压缩。</p>
<h2 id="QUERYABLE-TIMESTAMP"><a href="#QUERYABLE-TIMESTAMP" class="headerlink" title="QUERYABLE TIMESTAMP"></a>QUERYABLE TIMESTAMP</h2><p>表的可查询时间戳（QT）是一个时间戳，它表示可以查询的数据的新鲜度。假设QT(table) = X，则client可以查询到时间 X之前的所有数据，并且时间X之后摄入的数据属于不可查询数据的一部分，即一个表的新鲜度是[Now() - QT]。一旦在(Y-X)时间范围内写入的数据经过优化后满足了查询性能要求，QT的值将从X变成Y。因此client可以使用Napa的配置选项和这个指标来调整新鲜度、查询性能和成本。如果client想要高查询性能和低成本，但可以牺牲数据新鲜度，则系统优先使用较少的机器资源进行视图维护以降低成本，QT的推进则变得缓慢。</p>
<p>确保更高查询性能的一个重要方法是优化读取的基础数据并确保视图可用以加快查询速度。Napa中的表是其所有delta files的集合，每个delta对应于在一个时间窗口内为该表接收的更新，如下图所示，不可查询的delta对应于最近写入的数据。每个delta都按key排序、范围分区，并具有类似本地B树的索引，在查询时合并需要读取的delta。</p>
<p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcd1.png" alt></p>
<p>一般client查询都会有严格的延迟限制，通常会在查询期间对打开和合并读的最多delta做限制，通常会限制不超过数十个delta，并且会根据对数据的查询性能做自动设置。通过保持给定数据库的增量数量接近恒定，Napa能够提供强大且稳定的查询性能。</p>
<p>QT本质上会依赖于后台操作如compaction和视图维护的进度。数据库的QT是数据库中所有表的QT的最小值。 另外，QT还用于为client提供跨所有Napa副本的一致数据视图。每个副本都有一个局部QT，它基于本地副本中数据的新鲜程度的得出的。 QT的全局值则是根据查询服务可用性要求从本地QT值计算出来的，实际就是一个基于Quorum机制的选择。</p>
<h2 id="MAINTAINING-VIEWS-AT-SCALE"><a href="#MAINTAINING-VIEWS-AT-SCALE" class="headerlink" title="MAINTAINING VIEWS AT SCALE"></a>MAINTAINING VIEWS AT SCALE</h2><p>Napa的存储子系统负责维护视图和delta的compaction，其目标是能有效地管理数千个表和视图，并且数据量通常是PB级的。但视图维护遇到的一个困难是，基表的更新转换为视图更新的过程中，由于key空间的映射带来的数据倾斜问题。另外就是由于数据库的QT往往会受到长尾视图的影响，因此Napa利用了下面的技术来解决视图维护的问题：</p>
<ul>
<li>Use of F1 Query as a “data pump”：使用Google的F1 Query作为data pump来压缩表和维护视图，视图维护使用查询优化器，它可以在备选计划中做出很好的选择。</li>
<li>Replanning to avoid data skews：如果检测到数据倾斜，系统可以即时做新的维护计划，提高视图的维护速度。</li>
<li>Intelligence in the loop：需要具备能力，根据历史负载选择数据中心执行任务，根据进度主动终止任务，并发任务执行以限制长尾。</li>
</ul>
<h3 id="Query-optimizations-challenges-in-View-Maintenance"><a href="#Query-optimizations-challenges-in-View-Maintenance" class="headerlink" title="Query optimizations challenges in View Maintenance"></a>Query optimizations challenges in View Maintenance</h3><p>Napa的视图维护过程有效地利用了输入中的数据属性，由于处理的数据量和特定数据属性使大规模查询处理变得更复杂，视图更新必须解决独特的优化挑战。数据属性的一个例子就是要更新的视图相对于基表的排序顺序。如果基于视图排序顺序对视图key重新排序，这可能会是一个非常昂贵的方案，而尽可能维护输入顺序反而是低成本。因此如下图所示，根据维护它们的成本，存在三类视图：</p>
<p><img src="https://pic.imgdb.cn/item/61e199e72ab3f51d91a9e423.png" alt></p>
<ul>
<li>与基表共享前缀的视图：例如基表具有键(A, B, C)，而视图位于(A, B)上，框架只需要对公共key前缀（A、B）对输入进行聚类则可以完全避免重新排序；</li>
<li>仅有部分公共前缀的视图：例如基表具有key(A, B, C, D)，而视图位于(A, B, D)。即使在这种情况下，我们也可以通过在(A,B)上对输入基表进行聚类，然后在D上对每个唯一的(A,B)组进行排序来部分利用输入顺序；</li>
<li>不共享任意前缀的视图：这一类的优化机会就很少了；</li>
</ul>
<h3 id="Mechanics-of-Compaction"><a href="#Mechanics-of-Compaction" class="headerlink" title="Mechanics of Compaction"></a>Mechanics of Compaction</h3><p>压缩将多个输入delta组合成一个输出delta，异步压缩能有效减少查询时的合并负载，但对于高频写入的表来说，压缩是非常高的代价，会使得数据新鲜度变低。由于delta files是单独排序的，因此压缩本质上是归并排序。合并过程会在各种输入之间分配固定的内存预算，因此输入越多，每个输入流的内存越小，并且当其中一个输入被消耗完时，归并过程将停止。因此大型的归并会使得发送归并终止的可能性越高。</p>
<h2 id="ROBUST-QUERY-SERVING-PERFORMANCE"><a href="#ROBUST-QUERY-SERVING-PERFORMANCE" class="headerlink" title="ROBUST QUERY SERVING PERFORMANCE"></a>ROBUST QUERY SERVING PERFORMANCE</h2><p>维护较低的查询性能是业务使用的关键，本节主要讲Napa如果使用QT、视图等一系列技术实现强大的查询性能。</p>
<p>这里主要讲了几点：</p>
<ol>
<li>尽量使用视图来响应查询，filter和聚合下推，尽可能减少读取的数据量。并且依赖并发加快查询。</li>
<li>通过分布式的cache层和数据预取来进一步减少IO。</li>
<li>并行化的IO调用往往会受到长尾延迟的影响，为了对抗这种延迟，Napa会尽可能合并小IO，并基于下面两种技术去实现：Lazy merging across deltas，当有N个子查询和M个delta时，尽可能避免delta server中的交叉delta合并，每个server只处理一个delta，将NxM的并行IO组合减少到N个冰箱IO；Size-based disk layout，根据delta大小选择不同的磁盘文件布局，PAX适用于小delta，将所有列访问组合到一个IO，大delta则使用按列layout，提高扫描查询的IO效率。</li>
<li>由于Napa建立在多种Google基础设施上，对于这种复杂且相互依赖的系统所有可变性来源，无法完全消除长尾延迟。Napa的做法是采用对冲机制，则记录延迟的状态，尽可能在状态不稳定时将请求发送到不同的服务器/数据中心，以容忍一定的长尾延迟。</li>
</ol>
<h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>Napa是Google新一代的OLAP系统，具备高可用性，并且与其他数据库依赖列存储、并行、压缩等方式不同，其强依赖物化视图提供了更好的查询能力，同时允许client自行在数据新鲜度、查询延迟和成本之间进行权衡选择，总体而言是一篇非常具备启发性的OLAP论文。</p>
<p>### </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/12/05/Greenplum：A-Hybrid-Database-for-Transactional-and-Analytical-Workloads/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/12/05/Greenplum：A-Hybrid-Database-for-Transactional-and-Analytical-Workloads/" itemprop="url">Greenplum：A Hybrid Database for Transactional and Analytical Workloads</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-12-05T16:11:28+08:00">
                2021-12-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Greenplum：A-Hybrid-Database-for-Transactional-and-Analytical-Workloads"><a href="#Greenplum：A-Hybrid-Database-for-Transactional-and-Analytical-Workloads" class="headerlink" title="Greenplum：A Hybrid Database for Transactional and Analytical Workloads"></a>Greenplum：A Hybrid Database for Transactional and Analytical Workloads</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Greenplum是一个老牌的、基于MPP架构的数据仓库系统，主打的OLAP功能，采用了share nothing和sharding的架构，能够处理PB级别的数据。Greenplum存在一个固定的coordinator节点，负责与client交互，查询计划的生成与分布式执行、事务的管理等，是一个比较重要的节点。其他segment节点（单机Postgres）则存储数据与本地查询和事务。为了增强Greenplum的OLTP能力，往HTAP的方向发展，论文中提到了Greenplum对以下几个方面进行了增强：</p>
<ul>
<li>事务能力的增强：加入了全局的死锁检查器，避免了过去由于严格的锁机制导致并发能力不够的问题；对于单个segment server上的事务，由2PC转变为1PC；</li>
<li>提高点查询的性能；</li>
<li>引入了资源管理组，避免OLAP与OLTP两种查询负载相互影响；</li>
</ul>
<p>从某种角度来说，Greenplum演变成HTAP数据库的路径与大多数数据库不一样，它是在传统的OLAP数据库上加强了OLTP功能的支持。</p>
<h2 id="GreenPlum’s-MPP-Architecture"><a href="#GreenPlum’s-MPP-Architecture" class="headerlink" title="GreenPlum’s MPP Architecture"></a>GreenPlum’s MPP Architecture</h2><p>GreenPlum是一个典型的MPP架构，集群由多个worker segments组成，每个segment都是一个增强版的PostgreSQL。下图就是GreenPlum的架构。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73a32ab3f51d91ec3a9e.png" alt></p>
<h3 id="Roles-and-Responsibility-of-Segments"><a href="#Roles-and-Responsibility-of-Segments" class="headerlink" title="Roles and Responsibility of Segments"></a>Roles and Responsibility of Segments</h3><p>一个GreenPlum集群由许多个跨主机的segments组成，其中会有一个segment是coordinator，其他的统称为segment server。coordinator是一个比较重的节点，负责接收client请求，生成分布式查询计划，根据计划生成分布式进程，将计划分配到每个进程，收集结果，返回到client。</p>
<p>segment server则是存储数据，从coordinator接收查询计划。为了提高可用性，segment也可以配置镜像节点，不参与计算，但会接收WAL并回放日志。</p>
<p>作为一个share nothing的架构，GreenPlum中每个segment都会有自己的共享内存和数据目录。</p>
<h3 id="Distributed-Plan-and-Distributed-Executor"><a href="#Distributed-Plan-and-Distributed-Executor" class="headerlink" title="Distributed Plan and Distributed Executor"></a>Distributed Plan and Distributed Executor</h3><p>由于是share nothing的架构，当两个表需要进行join时，通常需要检查不同的segment server的元组是否满足条件，免不了需要在segment server移动数据。GreenPlum引入了一种叫Motion的算子来实现移动。Motion算子会通过网络来接发数据，Motion算子将查询计划切成不同的slice，在slice之间会做数据的分发，每个slice的执行都由一组特定的worker负责，这组进程就是gang。coordinator将查询计划分配个跨集群的进程组，不同的segment server生成不同的进程，都有相关的上下文信息。</p>
<p>如下图，顶部是一个join的分布式计划，下方则是在两个segment server集群的执行过程。在segment server上有两个slice，一个slice会扫描class表并通过redistributed Motion发送元组，两个slice则是从Motion节点接收元组，并扫描Student表执行hash join，将结果发送至顶部的coordinator。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73a42ab3f51d91ec3ab9.png" alt></p>
<h3 id="Distributed-Transaction-Management"><a href="#Distributed-Transaction-Management" class="headerlink" title="Distributed Transaction Management"></a>Distributed Transaction Management</h3><p>Greenplum通过分布式快照和2PC来确保ACID属性，在单个segment节点上，则是Postgres原生的事务机制。</p>
<h3 id="Hybrid-Storage-and-Optimizer"><a href="#Hybrid-Storage-and-Optimizer" class="headerlink" title="Hybrid Storage and Optimizer"></a>Hybrid Storage and Optimizer</h3><p>Greenplum支持三种表类型：PostgreSQL原生的heap表，行存储；还有就是两种新加入的，Append Optimized的行存储和列存储。AO表更有利于批量IO而不是heap表的随机访问模式，因此更适合AP的工作负载。特别是AO column表，可以用不同的压缩算法对不同的列进行压缩。Greenplum的查询引擎不敢直表的存储类型，同一个查询可以join不同的表类型。</p>
<p>表可以按用户指定的key和分区策略（list、Range）进行分区，其中每个分区可以是heap、AO-row、AO-column、甚至是外部表（比如AWS的S3）。以下图的销售表为例，每个分区由日期范围定义，从老到新分别是外部表、heap表和AO-column表。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73a42ab3f51d91ec3ac2.png" alt></p>
<p>至于优化器、Greenplum也提供两种选择（不是自适应的），分别是适合执行时间长的Orca和适合短查询Postgres原生的优化器。</p>
<h2 id="OBJECT-LOCK-OPTIMIZATION"><a href="#OBJECT-LOCK-OPTIMIZATION" class="headerlink" title="OBJECT LOCK OPTIMIZATION"></a>OBJECT LOCK OPTIMIZATION</h2><p>这一节主要讲的是锁优化，这是Greenplum增强OLTP性能的关键，着眼于解决分布式系统的全局死锁。</p>
<h3 id="Locks-in-Greenplum"><a href="#Locks-in-Greenplum" class="headerlink" title="Locks in Greenplum"></a>Locks in Greenplum</h3><p>Greenplum有三种锁：spin锁、LW锁和对象锁。前两种用于保护读写共享内存的临界区，并遵循某些规则来避免死锁。这里主要关注的是操作表、元组或事务等数据库对象时的对象锁。</p>
<p>其锁级别如下，level越高，并发控制粒度更严格。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73a42ab3f51d91ec3ad4.png" alt></p>
<h3 id="Global-Deadlock-Issue"><a href="#Global-Deadlock-Issue" class="headerlink" title="Global Deadlock Issue"></a>Global Deadlock Issue</h3><p>在Greenplum中处理全局死锁时，DML语句的锁定级别非常重要：在分析阶段，事务会对表上锁；在执行阶段，则是用tuplelock。由于Greenplum会夸与多个segment server执行锁，很难避免全局死锁。如下图，在segment server0上，事务B等待事务A，而在segment server1上，事务A等待事务B。但本地的PostgreSQL却没有发现本地死锁。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73a42ab3f51d91ec3acc.png" alt></p>
<p>更复杂的例子如下，包括协调者在内的所有segment server都导致了全局死锁。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec643e.png" alt></p>
<p>在旧版本的Greenplum中，会在coordinator分析阶段，用X模式锁定目标表。因此对于执行写操作的事务来说，它们是以串行的方式运行，而且即便是更小不同元组也会串行运行，降低了OLTP的性能。</p>
<h3 id="Global-Deadlock-Detection-Algorithm"><a href="#Global-Deadlock-Detection-Algorithm" class="headerlink" title="Global Deadlock Detection Algorithm"></a>Global Deadlock Detection Algorithm</h3><p>新版的Greenplum的全局死锁检查方法（GDD）如下：会在coordinator起一个守护进程，然后该进程会定期收集每个segment server上的Wait-for图，并检查是否发送全局死锁，然后用预定的策略终止全局的死锁。（比如终止最新的事务）</p>
<p>对于全局Wait-for图来说，事务就是顶点，其中输出边是顶点的出度，输入边数量则是入度。顶点的局部度是在某单个segment server的wait-for图中计算的值； 顶点的全局度则是在所有segment server的所有局部度的总和。另外考虑收集Wait-for图的过程是异步的，因此在下检测结论的时候，需要判断下涉及的事务是否还存在。如果有事务结束了，则放弃本轮检测，等待下一个周期。</p>
<p>对于Wait-for图有两种类型的边，实边是指等待的锁只有在事务结束的时候才能释放，pg中大多数对象都是这个类型。如果等待的锁无需事务结束则可以释放，比如tuple lock，则对应虚边。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec6443.png" alt></p>
<p>至于具体的检测方法如上图，是一种贪婪的算法，在每一轮的循环中：</p>
<ul>
<li>首先会将全局出度为0的顶点对应的输入边删除掉，出度为0的事务没有等待任何锁，本身可以正常结束，对它的等待也不会导致死锁，这一步可以持续直到没有这种顶点；</li>
<li>接着关注局部graph，接着删除局部出度为0的点所对应的输入虚边删除掉。虚边本身依赖的是tuple lock，但因为没有局部出度，因此该依赖关系可以在事务执行完之前就结束了，可以直接删掉。</li>
</ul>
<p>如果仍然存在无法消除的边，则认为死锁存在，此时再确认下之前的事务是否还存在。</p>
<p>下面就是一个具体的例子，上面一个图是全局和局部的Wait-for图，下图则是GDD算法执行过程。由于事务C没有全局出度，因此删除它和关联的边，变为(b)图。再看局部图，s1中A到B是一个虚边，并且B的局部出度为0，这条边也可以去掉，变成(c)图。再看全局图中B -&gt; A的边，A没有全局出度了，可以继续消除而变为(d)。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec644d.png" alt></p>
<h2 id="DISTRIBUTED-TRANSACTION-MANAGEMENT"><a href="#DISTRIBUTED-TRANSACTION-MANAGEMENT" class="headerlink" title="DISTRIBUTED TRANSACTION MANAGEMENT"></a>DISTRIBUTED TRANSACTION MANAGEMENT</h2><p>Greenplum的事务是由coordinator创建的，并将其分发到各个segment server中。coordinator为每个事务分配了一个单调递增的整数，作为分布式事务id。在每个局部segment上，根据分布式事务id也会利用原生的PG事务机制来生成本地事务标id。</p>
<h3 id="Distributed-Transaction-Isolation"><a href="#Distributed-Transaction-Isolation" class="headerlink" title="Distributed Transaction Isolation"></a>Distributed Transaction Isolation</h3><p>Greenplum利用了Postgres原生的snapshot机制来构建全局的分布式snapshot，可以应付分布式环境下的事务隔离。元组的可见性是由本地事务id和分布式事务id共同决定的。对于一个给定的事务，在修改一个元组时会给该元组创建一个新版本并打上局部事务ID，维护局部事务到分布式事务ID的映射。</p>
<p>考虑到维护本地事务ID映射分布式事务ID的开销较大，因此仅维护一个最大的分布式事务ID，和周期性地截断映射关系的元数据。</p>
<h3 id="One-Phase-Commit-Protocol"><a href="#One-Phase-Commit-Protocol" class="headerlink" title="One-Phase Commit Protocol"></a>One-Phase Commit Protocol</h3><p>一般来说，coordinator会使用2pc来保证事务在所有segment server上要么abort要么commit。至于这里说的一阶段优化，则是指如果事务只会修改单个segment上的数据，则可以省掉不必要的PREPARE过程。如下图，coordinator将跳过PREPARE阶段，直接把Commit命令分发至参与segment server上，节省掉一个网络来回的PREPARE消息：</p>
<p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec6457.png" alt></p>
<p>还有进一步的优化，最后一个Query可以和Prepared/Commit消息合并，多节省一轮roundtrip。</p>
<p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec6460.png" alt></p>
<h2 id="RESOURCE-ISOLATION"><a href="#RESOURCE-ISOLATION" class="headerlink" title="RESOURCE ISOLATION"></a>RESOURCE ISOLATION</h2><p>Greenplum还引入资源组的概念，考虑到TP和AP同时运行时，AP的工作负载会对TP产生极大的影响，通常前者会消耗大量的CPU、内存和IO带宽，并对后者的查询性能产生影响。目前Greenplum主要是实现了对CPU和memory的限制。</p>
<p>CPU的使用隔离是利用cgroup实现的，可以通过cpu.shares来控制着CPU的使用百分比或者优先级，也可以通过cpuset.cpus来指定资源组的cpu的核数。</p>
<p>内存的使用隔离则是基于内存管理模块Vmemtracker实现的，但由于想要显式控制内存的使用不是那么容易，引入了三个层次来管理内存使用情况：</p>
<ul>
<li>slot memory：单个查询的内存使用情况，通过资源组的非共享内存除以并发数得出；</li>
<li>shared memory：在同一资源组中的查询超过slot memory时使用该层；</li>
<li>global shared memory：最后一层，前面的都限制不了时会使用这个配额；</li>
</ul>
<h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>论文主要讲了主要面向OLAP的数据库如何转换成一个HTAP系统，考虑到OLAP的工作负载会极大影响OLTP的性能，论文提出的方法如全局死锁检测器和1PC的提交协议会显著提高OLTP性能。另外就是通过资源组的使用，限制CPU和内存，保证了在单个系统中同时运行OLTP和OLAP工作负载不会有太大的影响。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/12/01/F1-Lightning-HTAP-as-a-Service/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/12/01/F1-Lightning-HTAP-as-a-Service/" itemprop="url">F1 Lightning HTAP as a Service</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-12-01T00:56:22+08:00">
                2021-12-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="F1-Lightning-HTAP-as-a-Service"><a href="#F1-Lightning-HTAP-as-a-Service" class="headerlink" title="F1 Lightning: HTAP as a Service"></a>F1 Lightning: HTAP as a Service</h1><blockquote>
<p>本文介绍了F1 Lightning的设计与经验，该系统不是从头设计一个HTAP系统，而是在已有的若干个事务系统中存在着大量数据的情况，如何由一个独立的联合引擎实现对原事务系统的快速组合查询和事务操作。</p>
</blockquote>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>HTAP系统的研究表明，数据所有者强烈希望能处理对同一数据集的同时处理查询和事务。已经有大量与 HTAP 系统相关的研究和开发（甚至在“HTAP”一词出现之前很久就开始了）。其中大部分工作都是在考虑：理想的 HTAP 系统应该是什么样子，以及需要哪些技术进步才能获得良好的性能。本文则考虑了另一种方法：一个松散耦合的 HTAP 架构，支持 在不同约束条件下的HTAP工作负载。</p>
<p>这样考虑主要是因为在Google中存在着多个事务数据存储系统来处理不同的工作负载，和与这些系统松耦合的联合查询引擎。为了避免高昂的迁移代价和提高事务存储系统的灵活性，需要一个单一的HTAP解决方案，可以跨事务存储来进行启用。</p>
<p>本文提出的Lightning是一种松耦合的HTAP解决方案，即HTAP as-a-service。只需将事务存储中架构中的某些表标记为“Lightning表”，Lightning即可透明地提供应用程序需要的HTAP功能。创建读取优化数据副本的所有工作，都由Lightning 及其集成处理使用的联合查询引擎来提供的，业务使用上甚至不需要知道Lightning的存在。</p>
<h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><p>现有的HTAP系统一般分为<strong>同时承载OLTP和OLAP的单一系统</strong>和<strong>单独的OLTP和OLAP系统</strong>。再进一步的，后者又分为用于OLTP和OLAP的共享存储和解耦存储，共享存储需要对OLTP系统进行修改，以利用现有的分析查询引擎来实现OLAP。又或者是维护一个单独的、离线的ETL过程，使用松解耦的存储来实现HTAP架构，但这容易带来高延迟的问题。F1 Lightning通过与变更数据捕获 (CDC) 机制的集成以及结合内存驻留和磁盘驻留存储层次结构的使用来提高数据新鲜度。</p>
<p>文中介绍了几个类似系统的对比：SAP HANA异步并行表的复制机制；为行存储TiDb添加了一个列式存储和矢量化处理层的TiFlash。Oracle Database In-Memory则是结合OLTP 和 OLAP的单一系统代表，通过为活跃数据维护一个内存中列存储来加速HTAP工作负载，该列存储在事务上与持久行存储保持一致。LinkedIn Databus则是一个数据源不可知的分布式变更数据捕获系统，能将来自真实源系统的变更提供给下游应用程序，以构建用于不同目的的专用存储和索引。</p>
<h2 id="SYSTEM-OVERVIEW"><a href="#SYSTEM-OVERVIEW" class="headerlink" title="SYSTEM OVERVIEW"></a>SYSTEM OVERVIEW</h2><p>该HTAP系统由三个部分组成：一个OLTP，作为数据来源并公开数据变更捕获接口；F1 Query，分布式SQL查询引擎和Lightning，维护和服务读优化的副本。</p>
<p>在Google有两个主要的OLTP数据库：F1 DB和Spanner，前者是在后者基础上实现的。F1 Query则是一个联合查询引擎，它的查询执行依赖的是内部称为GoogleSQL（开源为 ZetaSQL）的SQL语言。F1 Query是一个联合引擎，支持许多不同的内部数据源，包括F1 DB、Spanner、Mesa、ColumnIO , 和BigTable等，用户能够编写无缝连接这些系统的查询。一般来说，F1 DB和Spanner能通过高效的面向行的存储和索引来优化OLTP工作负载，如写入和点查找查询，并且用户也会设计特定的模式以最大化写入吞吐量。虽然F1 Query能够通过多worker的方式来运行分布式分析查询，提高查询性能，但会导致大量的计算资源成本。</p>
<p>针对这些问题，一些团队设置了pipeline，用于将F1 DB的表复制到ColumnIO文件或其他文件格式以供进一步分析。但这种实现也有几个问题，一是导致重复存储资源，多个团队各自保留自己的相同数据副本。二是ColumnIO文件不支持就地更新，副本必须作为一个整体定期全量更新，另外就是数据新鲜度比较差。三是需要显式更改查询模式，因为两个数据源来自不同的系统，具有不同的语义。最后就是权限问题，如何保持权限上的同步。</p>
<p>为了解决这些问题，论文提到了一个新的HTAP系统Lightning，可将OLTP数据库中的数据复制为针对分析查询优化的格式。既可以为单个表启用Lightning，也可以为整个数据库启用Lightning。对于每个启用的表，Lightning的一个组件 Changepump会使用F1 DB或者Spanner公开的数据变更捕获机制来检测新的数据更改。Changepump会将这些变更转发到由单个Lightning服务器管理的分区，每个服务器会维护由分布式文件系统支持的LSM树。当Lightning摄取到这些变更时，它会将相关行数据转换为针对分析优化的列存储格式。</p>
<p><img src="https://pic.imgdb.cn/item/61a657142ab3f51d91364af2.png" alt></p>
<p>Lightning与对应OLTP数据库保持快照一致的方式来读取数据，包括F1 DB和Spanner在内的数据库都支持使用时间戳的多版本并发控制，因此提交到Lightning的每个更改也都保留其原始的提交时间戳。Lightning保证在特定时间戳下的读取将产生与在同一时间戳读取OLTP数据库相同的结果，这就为F1 Query通过重写符合条件的查询以提高性能，也就是某个查询可能会分别从Lightning和对应的OLTP数据库读取数据。</p>
<p>将Lightning添加到查询系统中，具备以下的有点：</p>
<ul>
<li>提高分析查询的资源效率和降低延迟；</li>
<li>配置简单，Lightning支持标准的流程从而更改启用HTAP；</li>
<li>透明的用户体验：用户无需更改SQL文本，甚至无需感知Lightning的存在；</li>
<li>数据一致性和数据新鲜度；</li>
<li>数据安全：F1 Query在重写查询以使用Lightning 之前，会以原始OLTP数据库的访问权限为准；</li>
<li>Lightning是一个独立的系统，无需维护其他OLTP数据库；</li>
<li>可扩展性，原则上，Lightning可以扩展到任何提供数据变更捕获机制的OLTP数据库上运行；</li>
</ul>
<h2 id="LIGHTNING-ARCHITECTURE"><a href="#LIGHTNING-ARCHITECTURE" class="headerlink" title="LIGHTNING ARCHITECTURE"></a>LIGHTNING ARCHITECTURE</h2><p>Lightning由以下组件组成：</p>
<ul>
<li>Data storage：数据存储层负责将更改应用到Lightning副本。它会在分布式文件系统中的创建相关的读取优化文件，提并供一个 API，允许查询引擎以与OLTP数据库相同的语义读取存储的数据，并处理后台维护相关操作，如数据压缩。</li>
<li>Change replication：Change replication负责跟踪OLTP数据库提供的事务日志，并对变更进行分区以分发到相关数据存储服务器。</li>
<li>Metadata database：相关元数据，比如数据存储状态和Change replication组件的状态会放在该数据库中。</li>
<li>Lightning masters：负责协调Lightning服务器之间的状态。</li>
</ul>
<h3 id="Read-semantics"><a href="#Read-semantics" class="headerlink" title="Read semantics"></a>Read semantics</h3><p>Lightning支持具有快照隔离的MVCC。所有针对Lightning特定表的查询都指定了一个读取时间戳，并且Lightning返回与该时间戳的OLTP数据库一致的数据。这一点主要是与Google内部的OLTP数据库保持一致。</p>
<p>由于Lightning会异步应用来自OLTP数据库的更改日志，因此在OLTP数据库中所做的更改对Lightning上的查询可见之前会存在延迟。此外，Lightning支持控制单个查询的读取时间上限。这个上限可以是无限的（即Lightning可以存储所有的更改），但实际上大多数查询都集中在最近的数据上，限制Lightning中的数据量可以节省成本。</p>
<p>Lightning可查询的时间戳称为安全时间戳。最大安全时间戳表示Lightning已经获取到该时间戳之前的所有更改，最小安全时间戳即表示可以查询到的最旧版本时间戳。</p>
<h3 id="Tables-and-deltas"><a href="#Tables-and-deltas" class="headerlink" title="Tables and deltas"></a>Tables and deltas</h3><p>Lightning将数据以表的形式组织，数据库表、索引和视图在Lightning中都被视为物理表。每个Lightning表都按range partitioning划分为一组分区。每个分区都存储在多组件的LSM树中。LSM树中的每个组件称为delta。</p>
<p>Delta包含其相应Lightning表的部分行版本数据，每个行版本由相应行的主键和该版本在OLTP数据库中提交时的时间戳标识。Lightning存储三种类型的版本，对应于对源数据所做的更改：</p>
<ul>
<li>Inserts：插入包含所有列的值，每行的第一个版本是一个插入。</li>
<li>Updates：更新至少包含一个非键列的值，并省略未修改列的值。</li>
<li>Deletes：删除不包含非键列的任何值，仅做墓碑。</li>
</ul>
<p>单个Delta可能包含同一键的多个版本，并且同一分区的不同Delta之间可能存在重复版本。在Delta内，部分行是由  (hkey,timestamp)唯一标识，并且为了支持对特定时间戳的快速查找，Delta按键升序、时间戳降序排序。</p>
<h3 id="Memory-resident-deltas"><a href="#Memory-resident-deltas" class="headerlink" title="Memory-resident deltas"></a>Memory-resident deltas</h3><p>当Lightning接收更改时，生成的部分行数据首先写入内存驻留的、按行构造的B树，类似于C-Store的写优化存储。</p>
<p>一个Memory-resident Delta最多有两个活跃的Writer，以及许多读取者。另外还有后台线程应用来自OLTP事务日志的新更改和定期运行垃圾收集过程。</p>
<p>一旦数据写入Memory-resident Delta，它就可以立即用于查询，这取决于Changepump提供的一致性协议。Memory-resident Delta并不持久，在系统故障的情况下，存储在内存中的更改可能会丢失。Lightning会通过OLTP重放的方式恢复，同时为了提高恢复速度，会将B树按原样定期checkpoint到磁盘。</p>
<p>当Delta变得太大时，当到达每个Delta的大小限制或者服务器内存限制时，Lightning都会将它们写入磁盘，并且会转换成为读取优化的列格式。</p>
<h3 id="Disk-resident-deltas"><a href="#Disk-resident-deltas" class="headerlink" title="Disk-resident deltas"></a>Disk-resident deltas</h3><p>含有Lighting数据的Disk-resident deltas被存储在读取优化的文件中，Lighting通过构建了一个具有通用接口的抽象层，允许使用许多不同的文件格式来存储Delta。文中只介绍了一种文件格式，每个增量文件存储两部分：数据部分和索引部分。数据部分以PAX风格的行列混合。索引部分包含主键上的稀疏B树索引，其中叶节点跟踪每个row bundles的键范围。索引比较小，通常在缓存中。</p>
<h3 id="Delta-merging"><a href="#Delta-merging" class="headerlink" title="Delta merging"></a>Delta merging</h3><p>Delta合并包含两个逻辑操作：merging和collapsing。merging对源Delta中的更改进行重复数据删除，还可能执行Schama变更。collapsing将同一key的多个版本合并为一个版本。</p>
<p>此过程使用的是LSM逻辑的矢量化执行，先枚举需要参与合并的Deltas，然后从每个输入Delta读取一个Block，进行多路归并，但这里需要确认在这一轮中可以collapsing的key范围。</p>
<p><img src="https://pic.imgdb.cn/item/61a657142ab3f51d91364af8.png" alt></p>
<p>比如这两个Deltas的合并，当前轮Lightning只能collapse小于K2的数据。K2的数据可能需要等到下一轮collapse，读入下一个Block再说。</p>
<h3 id="Schema-management"><a href="#Schema-management" class="headerlink" title="Schema management"></a>Schema management</h3><p>由于Lightning是复制OLTP数据库并透明地提供查询服务，因此它必须处理与OLTP数据具有相同语义的Schema演变。Lightning监控源数据库schema的更改并自动应用该更改。</p>
<p>为了实现这一点，Lightning使用了两级schema抽象。第一级是逻辑架构，它将OLTP的schema映射到Lightning表schema。逻辑schema包含复杂类型、比如Protocol Buffer，还有一些简单类型。对于特定的逻辑schema，Lightning会生成一个或多个物理schema。物理schema只包含基本类型，例如整数、浮点数和字符串。Lightning的文件格式接口仅在物理schema级别运行，降低工程实现难度。</p>
<p>如下图，逻辑schema和物理schema通过逻辑映射连接。映射指定如何将逻辑行转换为物理行，反之亦然，数据在读取期间从逻辑行转换为物理行。</p>
<p><img src="https://pic.imgdb.cn/item/61a657142ab3f51d91364b02.png" alt></p>
<p>映射的一个好处就是能为相同的逻辑数据实现使用不同的存储布局。例如，在存储protocol buffer时，有两种选择，一是序列化后存储，另一种则是将各字段单独存储，甚至可以同时存储，以提供极致的性能。映射还有助于仅元数据schema的更改。 Lightning可以适应许多常见的schema更改，而无需明确重写磁盘上的数据，比如增删一列。</p>
<p>因此，每当发生schema更改时，Lightning都会创建一个新的逻辑schema，schema更改后创建的增量会使用新的物理schema进行写入。当schema映射关系太多时，Lighting也会在compaction时将数据转换为新schema，从而减少数据转换的开销。</p>
<h3 id="Delta-compaction"><a href="#Delta-compaction" class="headerlink" title="Delta compaction"></a>Delta compaction</h3><p>Lightning支持四种不同的压缩方式：active compaction, minor compaction, major compaction和base compaction。</p>
<ul>
<li>active compaction：在Lighting服务器上进行，将内存delta持久化到磁盘上；</li>
<li>minor compaction：压缩小的和新的磁盘deltas；</li>
<li>major compaction：压缩大的和旧的磁盘deltas；</li>
<li>base compaction：将最小可查询时间戳之前的数据生成新的数据snapshot；</li>
</ul>
<p>其他三个任务都不在Lighting服务器上进行，由Lighting服务器调度，但在专门的任务worker上执行，</p>
<h3 id="Change-replication"><a href="#Change-replication" class="headerlink" title="Change replication"></a>Change replication</h3><p>Changepump提供跨不同OLTP源的统一接口，将各个OLTP的CDC接口细节从主要的Lightning数据存储层抽象出来，并提供了一种可扩展且有效的方式将事务变更呈现给对应的变更订阅者。作用包括了：隐藏了各个OLTP数据库的详细信息；面向事务的变更日志适应为面向分区的变更日志，一个事务可能对应不同的Lighting分区；维护事务一致性，跟踪已应用于Lightning服务器的所有更改时间戳，并发出检查点，以提高每个分区的最大安全时间戳。</p>
<h4 id="Subscriptions"><a href="#Subscriptions" class="headerlink" title="Subscriptions"></a>Subscriptions</h4><p>对于每个partitions，Lightning都会对Changepump做一个订阅。订阅会指定partitions的表和key范围，Changepump则负责将这些更改传送到Lightning服务器。订阅会有一个开始时间戳，Changepump只会返回在该时间戳之后提交的更改。</p>
<h4 id="Change-data"><a href="#Change-data" class="headerlink" title="Change data"></a>Change data</h4><p>Changepump订阅会返回两种数据：change updates和checkpoint timestamp updates。前者就是数据本身的修改，同一个key的修改按时间戳升序排序，跨行则没有严格顺序保证。后者则是用来表明该时间戳之前的修改都已经传递完成，方便Lighting服务器提交其最大的安全时间戳。</p>
<h4 id="Schema-changes"><a href="#Schema-changes" class="headerlink" title="Schema changes"></a>Schema changes</h4><p>Lightning使用两种机制来检测Schema的变化：lazy detection和eager detection。前者则是在收到OLTP数据库传来的修改时，如果发现它引用了没见过的架构，则会停止对该partition的更改处理，直到加载并分析了新schema。这种机制会增加处理延时。后者则是用后台线程轮询OLTP数据库以查看是否发生了任何新的schema更改。</p>
<h4 id="Sharding"><a href="#Sharding" class="headerlink" title="Sharding"></a>Sharding</h4><p>Changepump本身也是一个sharding的服务，单个订阅可以在内部连接到多个Changepump服务器，Changepump客户端会将多个这样的连接合并成一个单一的变更流中。与Lightning服务器不同，Changepump是根据数据量划分sharding的，后者是按全量数据量。</p>
<h4 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h4><p>Changepump会对增量的修改记录做缓存，好处是方便一个partition的不同副本能够共享这些数据，另外也可以加速partition的failover。</p>
<h4 id="Secondary-indexes-and-views"><a href="#Secondary-indexes-and-views" class="headerlink" title="Secondary indexes and views"></a>Secondary indexes and views</h4><p>Lighting还会维护二级索引和物化视图，这是与正常表同等看待的，但生成方式不一样，不是通过Changepump订阅产生，而是Lightning需要根据基础表的变更日志计算派生数据的修改。由于派生表需要按key排序，但基表可能是由不同的Lighting服务器维护的，因此Lighting的做法是将其写进BigTable。目前Lightning只能支持有限的几种物化视图。</p>
<h4 id="Online-repartitioning"><a href="#Online-repartitioning" class="headerlink" title="Online repartitioning"></a>Online repartitioning</h4><p>Lightning支持在线重分区，以期达到负载均衡，这里的重分区方案基本是元数据操作。Lighting的分区在达到数据大小阈值或者流量瓶颈的时候，就会进行分区分裂。在分裂分区时，Lightning就会新分区标记为非活跃。这是一个meta only的操作——新分区会共享老分区的所有delta，之后新的分区会从Changepump订阅数据并应用为新的delta，直到追上所有的新数据后才会被标记为活跃。至于老的分区，则会被标记为非活跃，然后等待所有请求都服务完后再被清理掉。由于新分区是继承自老分区的，因此读取时需要一个过滤器处理超过新分区边界范围的数据。</p>
<p>分区也会被触发合并，操作过程类似。</p>
<h3 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault tolerance"></a>Fault tolerance</h3><h4 id="Coping-with-query-failures"><a href="#Coping-with-query-failures" class="headerlink" title="Coping with query failures"></a>Coping with query failures</h4><p>对于查询失败，Lighting使用集群内和跨集群复制来处理这些故障。在一个数据中心内，Lightning为每个partition分配给多个Lightning服务器。这些Lightning服务器都从Changepump订阅相同的更改，并且它们独立维护其内存delta，但它们共享一组相同的磁盘delta和内存delta checkpoint。每个partition都只有一个主副本可以执行compaction来写入delta。当一个副本完成写入时，它会通知其他副本更新它们的LSM树。</p>
<p>这些副本中任一个都可以为查询提供服务，查询请求可以在这些副本之间维持负载均衡，Lighting可以为流量大的partition增加更多的副本。在服务器更新，每个partition最多同时重新启动一个副本，以保持数据的高可用性，并且重启时会尝试从其他副本加载修改。</p>
<p>Lighting也可以部署在多个数据中心。单个数据中心拥有自己的一套Changepump服务器、Lightning服务器等等。这些服务器独立于其他数据中心运行，在每个数据中心维护数据的完整副本，所有数据中心共享同一个元数据数据库（假设meta DB永远可用）。如果出现数据中心级别的故障，也可以将请求转给其他数据中心的相同partition。</p>
<h4 id="Coping-with-ingestion-failures"><a href="#Coping-with-ingestion-failures" class="headerlink" title="Coping with ingestion failures"></a>Coping with ingestion failures</h4><p>数据摄取也会出现相关的故障，比如OLTP系统的CDC出现问题或者Changepump服务器崩溃。对于前者，Changepump会自动连接另一个datacenter的OLTP系统（Google的OLTP系统本身也是跨数据中心部署的）。对于后者，Changepump是无状态的，可以连接另一个服务器继续服务。</p>
<p>Lightning master还会监控每个分区上所有数据中心的Changepump延迟，如果延迟过大，会重启这个数据中心的分区，使得可以从其他数据中心搬运数据过来。</p>
<h4 id="Table-level-failover"><a href="#Table-level-failover" class="headerlink" title="Table-level failover"></a>Table-level failover</h4><p>Lighting也会配备表级别的failover，当某张表本身有问题时（比如到达资源瓶颈，或者数据坏了），系统可以自动将查询路由回 OLTP 系统。但用户可以选择是否进行这样的容错，以防AP的查询压力突然打垮TP系统。</p>
<h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>本文提到的Lighting是HTAP系统的另一种实现方式，与同时承载OLTP和OLAP的单一系统不同，它是直接基于多个OLTP系统透明地提高HTAP性能的能力，而无需修改OLTP系统，也无需用户数据迁移到新系统。通过与F1 Query的结合，能够无感知地重写query plan（甚至在在逻辑计划阶段对查询也是生成一样的计划，到了物理计划阶段才会决定访问OLTP和OLAP系统），向量化列式处理查询请求，下推subplan。Google大规模部署Lighting后，在不影响查询语义的情况下，计算资源和查询延迟方面都实现了数量级的收益。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/10/10/Beyond-malloc-efficiency-to-fleet-efficiency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/10/Beyond-malloc-efficiency-to-fleet-efficiency/" itemprop="url">Beyond malloc efficiency to fleet efficiency</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-10-10T22:58:29+08:00">
                2021-10-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Beyond-malloc-efficiency-to-fleet-efficiency"><a href="#Beyond-malloc-efficiency-to-fleet-efficiency" class="headerlink" title="Beyond malloc efficiency to fleet efficiency"></a>Beyond malloc efficiency to fleet efficiency</h1><blockquote>
<p>内存分配的优化可以带来巨大的成本效益。一般有两种做法，一是提高分配器的效率，减少分配器代码中的周期；一种是通过数据放置的策略来提高应用的整体性能。这篇文章主要关注的是hugepage，提出了一个叫TEMERAIRE的hugepage机制，以最大化hugapage的覆盖率和最小化碎片开销。</p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文主要关注的是通过提高内存分配器的大页面覆盖率来提升应用性能。Cache miss和TLB miss是现代系统中最主要的性能开销，Hugepages的出现可以显著减少TLB未命中的数量，增加大页的大小使得相同数量的TLB条目能够映射更大范围的内存，另外大页还能减少miss+填充的时间，因为页表遍历更快了。论文提出了一种TEMERAIRE 的设计，作为TCMALLOC的一部分减少应用代码的CPU开销，最大化大页覆盖、减少内存碎片。</p>
<h2 id="The-challenges-of-coordinating-Hugepages"><a href="#The-challenges-of-coordinating-Hugepages" class="headerlink" title="The challenges of coordinating Hugepages"></a>The challenges of coordinating Hugepages</h2><p>虚拟内存是通过TLB来将用户空间地址转换为物理地址的，TLB条目有限，如果使用默认页面大小，整个TLB覆盖的内存范围很小。现代的处理器通过在TLB中支持hugepage来加大覆盖范围，完整的大页（比如X86时2MB）仅仅占用一个条目。</p>
<p><img src="https://pic.imgdb.cn/item/6162ff542ab3f51d9128f6c5.png" alt></p>
<p>传统的分配器以页面大小的块来管理内存，Transparent Huge Pages机制为内核使用大页来覆盖连续页面提供了可能性。但内存的释放面临着更大的挑战，对于非大页区域来说，内存的释放要求内核使用较小的页面来表示剩余的内存。又或者大页的返回需要整个页面都变成空闲状态，这就带来内存碎片的问题。因此对于大页的设计需要在内存碎片和TLB使用率之间权衡。</p>
<h2 id="Overview-of-TCMALLOC"><a href="#Overview-of-TCMALLOC" class="headerlink" title="Overview of TCMALLOC"></a>Overview of TCMALLOC</h2><p>下图展示了TCMALLOC的内存组织，TCMALLOC将内存按spans分区，并且与页面大小对齐。</p>
<p>足够大的分配请求由仅仅包含分配对象的spans实现，至于其他的span则会包含多个相同大小的小对象，小对象的边界是256KB，小于这个的请求会四舍五入到100个大小类别中去。TCMALLOC将对象存储在一系列缓存中，如下图所示。span是从一个简单的pageheap分配的，它跟踪所有未使用的页面并进行best fit分配。pageheap还会负责定期释放内存回操作系统，减少过多的系统内存分配。</p>
<p>TCMALLOC会首先从local cache中分配，这里用的是per-hyperthread local cache，本地缓存会存着不同大小的空闲对象列表。如果请求不能满足要求，会路由到对应大小类别的central cache。这里有两个组件，一个是小的、快速的、互斥保护的transfer cache，另一个则是大的、互斥保护的central列表，包含了该大小对应的每一个span。当一个span的所有对象都返回到central list的一个span时，该span就会返回到pageheap。</p>
<p>TCMALLOC的pageheap有一些简单的内存接口：New(N)分配一个N页的span；Delete(S)向分配器返回一个新的span；Release(N)将pageheap缓存的&gt;=N个未使用的页返回给操作系统；</p>
<h2 id="TEMERAIRE’s-approach"><a href="#TEMERAIRE’s-approach" class="headerlink" title="TEMERAIRE’s approach"></a>TEMERAIRE’s approach</h2><p>TEMERAIRE就是基于TCMALLOC提出的大页优化，将分配请求尽可能打包到频繁使用的大页上，同时形成完全未使用的大页面以便遍返回给操作系统。并且根据malloc的使用情况和TCMALLOC的结构制定了一些TEMERAIRE的选择原则：</p>
<ul>
<li>总内存的需求随时发生变化，并且是不可预测的；</li>
<li>将不是几乎为空的大页面返回给操作系统的成本是比较高的，因此其设计必须能够将分配密集地打包到高度使用的区域中；另外虽然我们的目标是专门使用大页大小的二进制，但malloc也必须支持大于单个大页的分配大小；</li>
<li>当一个大页完全为空时，可以选择是保留它以供将来分配内存，也可以将其返回给操作系统。适应性地做出这个决策非常重要；</li>
<li>很少有分配请求会直接接触pageheap，但所有分配都通过pageheap支持；</li>
</ul>
<p>TCMALLOC分配器通过委托给几个子组件来实现它的接口，如下图所示，每个组件都是根据上述原则构建的，都对最适合它处理的分配类型进行了设计。虽然TEMERAIRE的特定实现与TCMALLOC内部结构相关联，但大多数现代分配器都有类似的大页面分配支持。</p>
<h3 id="The-overall-algorithm"><a href="#The-overall-algorithm" class="headerlink" title="The overall algorithm"></a>The overall algorithm</h3><p>这一章主要描述各个组件，其主要目标是最小化或重用生成的slack（slack就是大页之间的多余空间区域）。所有组件的背后是HugeAllocator组件，它负责处理虚拟内存和操作系统之间的关系，为其他组件提供了可备份可传递的内存。HugeCache则是一个完全为空的大页缓存。HugeFiller则是一个存着部分填充空间的大页列表。HugeRegion则是用来应对大页边界的分配请求的。TEMERAIRE使用下图算法根据请求大小将分配决策定向到其子组件。</p>
<p><img src="https://pic.imgdb.cn/item/6162ff542ab3f51d9128f6d4.png" alt></p>
<p>为大页面大小的精确倍数，或那些足够大以至于slack无关紧要的分配请求由HugeCache负责；中等大小的分配由HugeCache负责（1MiB到1GiB）；例如，来自HugeCache的4.5MiB分配会产生1.5MiB的slack，这里的开销是比较高的。TEMERAIRE通过假装请求的最后一个大页面有一个单一的“前导”分配，将这个slack交由HugeFiller负责。如下图：</p>
<p><img src="https://pic.imgdb.cn/item/6162ff542ab3f51d9128f6dd.png" alt></p>
<p>对于某些中等分配的请求来说，其往往会产生更多的slack。如下图，例如，许多1.1MiB的分配将产生0.9MiB的每个大页的slack。当检测到这种模式时，HugeRegion分配器会跨越大页边界进行分配，以最大限度地减少这种开销。</p>
<p><img src="https://pic.imgdb.cn/item/6162ff542ab3f51d9128f6eb.png" alt></p>
<p>小请求(&lt;= 1MB的)始终由HugeFiller提供服务。对于1MB和大页之间的分配，TEMERAIRE会评估了几个选项：</p>
<ul>
<li>如果有足够的可用空间，就会尽量使用HugeFiller；</li>
<li>如果HugeFiller不能满足请求，接下来会考虑HugeRegion；如果已分配的HugeRegion能满足要求，TEMERAIRE就会使用它。如果不存在满足要求区域，就会考虑分配一个区域；</li>
<li>否则就从HugeCache中分配一个完整的大页面，当然这样会产生slack，但预期其会在未来被填平；</li>
</ul>
<p>对于TEMERAIRE来说，1个1GB的空闲范围内存和512 个不连续的free大页是被同等对待。</p>
<h3 id="HugeAllocator"><a href="#HugeAllocator" class="headerlink" title="HugeAllocator"></a>HugeAllocator</h3><p>HugeAllocator会跟踪记录所映射的虚拟内存，所有操作系统的映射都在此进行。</p>
<h3 id="HugeCache"><a href="#HugeCache" class="headerlink" title="HugeCache"></a>HugeCache</h3><p>HugeCache以完整的大页粒度来追踪返还的内存范围，HugeFiller填充和释放大页后，需要决定何时将大页返回给操作系统，需要权衡后续是否需要使用来做决定。HugeCache的做法是在 2 秒的滑动窗口内跟踪请求的周期性，并计算记录最大值和最小值，每当内存返回到HugeCache时，如果cache此时大于Demand_max-Demand_min，则将大页返回给操作系统。</p>
<h3 id="HugeFiller"><a href="#HugeFiller" class="headerlink" title="HugeFiller"></a>HugeFiller</h3><p>HugeFiller用来满足较小的分配请求，每个分配请求都尽量在单个大页完成。HugeFiller满足了大部分的分配请求，是真哥哥系统中最重要的组件。对于给定的大页，使用best-fit的算法来进行分配。</p>
<p>HugeFiller的两个目标，一是使得一部分大页尽可能地满，另一部分大页面尽可能为空；第二个目标是最小化每个大页内的碎片，使得新分配请求尽可能得到满足。因为几乎为空的大页非常宝贵，通过尽可能保留具备最长空闲内存范围的大页来满足上面的目标，将相应的大页组织到一个排序列表中，充分利用每个大页的统计数据。</p>
<p>在每个的大页中，HugeFiller会记录使用的页面位图。为了填充某个大页的请求，HugeFiller会从该位图进行best-fit的搜索。并且还记录了以下几个数据：尚未分配的连续页数，最长的空闲内存范围L；分配总数A；已使用的页面总数U。</p>
<p>通过上述三个统计信息来确定分配大页的优先级顺序——选择具有最小的合适L和最大A的大页。这个的决策选择是根据大量实验做出来的。</p>
<h3 id="HugeRegion"><a href="#HugeRegion" class="headerlink" title="HugeRegion"></a>HugeRegion</h3><p>HugeCache与HugeAllocator足以满足大内存分配，HugeFiller适用于可以打包成单个大页面的小内存分配，HugeRegion则是用来处理两者不好应付的场景。</p>
<p>考虑对1.1MiB内存的分配请求，这可以通过HugeFiller分配，对于2MiB的大页会留下0.9 MiB未使用的内存：这里会预期slack会被小于1.1MB的分配请求填充。但极限情况下，很可能会有一个二进制只请求1.1MB的请求。</p>
<p>HugeRegion是一个固定大小的分配（当前为1GB），与HugeFiller使用位图类似，以小页粒度进行追踪。对于内存请求，一样是采用best-fit策略来应对。出于与HugeFiller相同的原因，其保留了这些区域的列表，按最长空闲内存范围进行排序。</p>
<p>大多数分配不需要HugeRegion，只有积累了大量比程序小分配数量更多的slack，才会分配HugeRegion。对于键值存储来说，它会将一些大块数据加载到内存，并为服务请求进行一些短期分配。如果没有HugeRegion，请求相关的分配很可能产生大量的空缺。</p>
<h3 id="Memory-Release"><a href="#Memory-Release" class="headerlink" title="Memory Release"></a>Memory Release</h3><p>如上所述，Release(N)由后台线程定期调用。为了实现接口的Release(N)方法，TEMERAIRE通常只是从HugeCache中释放大页的内存范围。释放的页数量超过提示也不会有问题，后台线程会以实际释放的数量作为反馈，并调整未来的调用以达到合适的总体数量。如果HugeCache不能释放N页内存，HugeFiller将会释放最空的大页上的空闲小页。</p>
<p>从部分填充的大页面中释放小页是减少内存占用的最后手段，因为该过程在很大程度上是不可逆的。通过在大页上返回部分填充的小页，使操作系统用剩余页面的小条目替换跨越整个页面的单页表条目，这会增加TLB的miss概率，减慢对剩余内存的访问，即便后续重新使用前面释放的内存，Linux内核仍然只使用小的页表项。</p>
<p>HugeFiller会对部分释放的大页有单独的处理，除非没有其他大页可用，否则不会从它们进行分配，直到这些大页完全为空。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>TEMERAIRE通过更改内存分配器的使用方式来优化TLB的查找性能，从而极大提高了应用程序的性能。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/10/10/CockroachDB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/10/CockroachDB/" itemprop="url">CockroachDB</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-10-10T22:52:37+08:00">
                2021-10-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CockroachDB-The-Resilient-Geo-Distributed-SQL-Database"><a href="#CockroachDB-The-Resilient-Geo-Distributed-SQL-Database" class="headerlink" title="CockroachDB: The Resilient Geo-Distributed SQL Database"></a>CockroachDB: The Resilient Geo-Distributed SQL Database</h1><blockquote>
<p>本文介绍了一个叫CockroachDB的数据库系统。</p>
</blockquote>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>对于现代数据库来说，OLTP的工作负载越来越与地理分布有关。一些应用可能针对不同的地区分布有着不同的数据库需求，比如某些地区的数据需要有更严格的权限控制满足法律法规，有些地区则是处于快速增长的阶段，需要考虑成本、延时、性能等等的情况。</p>
<p>CockroachDB作为一款商业DBMS，满足了全球化公司关于数据库系统的种种需求：</p>
<ul>
<li><strong>容错和高可用性</strong>：在不同的地区为每个分区至少维护三个副本。节点故障，能自动恢复；</li>
<li><strong>地理分布和副本放置</strong>：CockroachDB支持水平伸缩，添加节点时自动增加容量并迁移数据。并根据需求选择最优的数据放置方法，同时支持用户自定义选择；</li>
<li><strong>高性能的事务</strong>：CockroachDB的事务协议支持跨多个分区的分布式事务，而不需要特定的硬件；</li>
</ul>
<p>除此之外，CockroachDB还实现了最新的查询优化器和分布式SQL执行引擎来支持更全面的SQL标准。</p>
<h2 id="SYSTEM-OVERVIEW"><a href="#SYSTEM-OVERVIEW" class="headerlink" title="SYSTEM OVERVIEW"></a>SYSTEM OVERVIEW</h2><h3 id="Architecture-of-CockroachDB"><a href="#Architecture-of-CockroachDB" class="headerlink" title="Architecture of CockroachDB"></a>Architecture of CockroachDB</h3><p>CockroachDB是典型的shared-nothing架构，其所有节点都参与计算和存储。这些节点可由同个数据中心或者多个数据中心组成，client可以连接到任意的节点。在单个节点内部具备以下的分层架构：</p>
<h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><p>SQL层是最上层，负责与client进行交互，包括了解析器、优化器和SQL执行引擎，其将SQL语句转化成对KV存储的读写请求。SQL层并不清楚数据的具体分区情况。</p>
<h4 id="Transactional-KV"><a href="#Transactional-KV" class="headerlink" title="Transactional KV"></a>Transactional KV</h4><p>SQL层的请求会来到事务KV层，负责确保跨KV对的更新原子性。</p>
<h4 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h4><p>该层根据key排序呈现了一个key空间的抽象，包括系统元数据和用户数据都可以在该key空间内查找。CockroachDB使用范围分区的方法将数据划分为大小约为64MiB的连续有序块，称之为Range。Range之间的顺序在一组系统Range内的两级索引结构（前面说过用于内部数据结构和元数据的系统数据也会组成Range）中维护。Range会被缓存起来用于快速查找，同时该层也主要负责对上层查询做路由。</p>
<p>Range大小为64MB，并根据需要对Range进行合并和拆分（数据太多就拆分，太少就合并，还有根据一些热点策略做Range划分）。</p>
<h4 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h4><p>默认情况下，每个Range都是三个副本，每个副本存储在不同的节点上。</p>
<h4 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h4><p>最底下的就是存储层，CockroachDB主要依赖了RocksDB。</p>
<h3 id="Fault-Tolerance-and-High-Availability"><a href="#Fault-Tolerance-and-High-Availability" class="headerlink" title="Fault Tolerance and High Availability"></a>Fault Tolerance and High Availability</h3><h4 id="Replication-using-Raft"><a href="#Replication-using-Raft" class="headerlink" title="Replication using Raft"></a>Replication using Raft</h4><p>CockroachDB使用Raft算法来进行一致性的复制，其中Range的副本会组成一个Raft组，每个副本要么是leader，要么是follower。CockroachDB的复制单元是Command，它表示的是对存储引擎进行的一系列底层修改，当Raft commit日志的时候，每个副本会将Command应用到存储引擎上。</p>
<p>CockroachDB使用Range级别的租约，其中一般是Raft组的leader来充当租约持有者，它是唯一可以提供最新读取信息或者发起提议的副本，所有的写入也是通过租约持有者进行。对于用户Range，节点会每隔4.5秒在系统范围内发送一套特殊记录的心跳；对于系统Range，则是每9秒更新一次租约。</p>
<p>同时为了保证一致性，一次只有一个副本存留，尝试获取租约的副本会通过提交特殊的租约获取日志来达成目的。</p>
<h4 id="Membership-changes-and-automatic-load-re-balancing"><a href="#Membership-changes-and-automatic-load-re-balancing" class="headerlink" title="Membership changes and automatic load (re)balancing"></a>Membership changes and automatic load (re)balancing</h4><p>集群支持节点的添加和删除，节点的变更会导致负载在集群活动节点之间重新分布。</p>
<p>对于短暂的故障，只要大多数副本可用，Raft算法能保证CockroachDB正常运行。如果leader失败，则依赖Raft重新选举leader。对于故障节点重新加入集群，则可以通过以下方式追上更新：发送完整Range数据的快照；发送一组要Apply的Raft缺失日志。二选一。</p>
<p>对于长期故障，CockroachDB则是会根据不受影响的副本，创建一个新的副本，将其放到合适的位置。</p>
<h4 id="Replica-placement"><a href="#Replica-placement" class="headerlink" title="Replica placement"></a>Replica placement</h4><p>CockroachDB支持手动和自动来控制副本的放置。自动放置副本的机制会根据一定的约束条件进行，并且会尽量平衡负载。</p>
<h3 id="Data-Placement-Policies"><a href="#Data-Placement-Policies" class="headerlink" title="Data Placement Policies"></a>Data Placement Policies</h3><p>CockroachDB的副本和租约holder的放置策略可以由用户根据性能和容错能力决定：</p>
<ul>
<li>副本按地理位置分区；</li>
<li>租约holder按地理位置分区；</li>
<li>重复索引：通过基于表来复制索引，并将每个索引的lease holder固定到特定地区，CockroachDB就可以通过较快的本地读取功能，同时提高容错性，但可能会带来写放大和跨区写延时的提高；</li>
</ul>
<h2 id="TRANSACTIONS"><a href="#TRANSACTIONS" class="headerlink" title="TRANSACTIONS"></a>TRANSACTIONS</h2><p>CockroachDB的事务可以跨越整个key空间，能在保证ACID的前提下访问到跨越整个集群的数据。CockroachDB是使用MVCC的变形来支持串行化隔离的。</p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>SQL事务从SQl连接的节点开始，该节点负责扮演事务协调者的角色。</p>
<h4 id="Execution-at-the-transaction-coordinator"><a href="#Execution-at-the-transaction-coordinator" class="headerlink" title="Execution at the transaction coordinator"></a>Execution at the transaction coordinator</h4><p>下面的算法给出了基于协调者角度的事务处理逻辑，在执行事务过程中，协调者接受一些列来自SQL层的KV操作。</p>
<p><img src="https://pic.imgdb.cn/item/6162fdf52ab3f51d91263fb5.png" alt></p>
<p>从途中可以看出，在下一个操作发出前，需要对当前操作进行应答，为了提高性能，协调者做了两个优化：写流水线和并行提交。因此，协调者需要跟踪还没完全复制成功的操作和维护了事务时间戳（第一行）。</p>
<p>写流水线：如果某个操作未尝试提交，并且该操作与先前任何操作都没有重叠，则可能立即执行该操作（第七行）。这就是为什么写流水线可以对不同key的进行多个操作。如果该操作依赖于先前的操作完成复制，则需要pipeline等待。inflightOps就是对运行中的状态进行追踪。第九行就是协调者将操作发送给租约持有者并等待回应。如果返回的时间戳更待，意味着存在了另一个事务影响了租约持有者的时间戳。协调者此时会调整事务时间戳，并通过一轮RPC去验证新时间戳返回的值是否相同，如果不同则事务失败。（第十到十四行）</p>
<p>本质就是事务内语句流水线并行执行，如果两条语句有操作重叠，则通过事务上下文追踪执行过的key，如果重叠则需要等待其执行成功。</p>
<p>并行提交：一个标准的2PC，往往需要两轮consensus，一轮完成Prepare，一轮将事务设置为Committed。并行提交的做法是使用了staging来表示一个事务操作的所有写入是否已经都复制完成，持久化staging状态和Prepare写数据同时进行（第五行），假设两者都成功，则协调者可以立即确认事务已经提交（第十五行），并且在终止前会异步将事务状态记录为commit。（十六和十七行）</p>
<h4 id="Execution-at-the-leaseholder"><a href="#Execution-at-the-leaseholder" class="headerlink" title="Execution at the leaseholder"></a>Execution at the leaseholder</h4><p>如下，当lease holder接收到来自协调者的操作请求时，会按下图执行。第二行险检查租约有效性，第三行会去获取操作依赖的所有key的latch。第四行会校验相关依赖的操作完成。第五和第六行则是在执行写操作时，保证时间戳在任何冲突读取之后，并且根据需要增加时间戳。</p>
<p><img src="https://pic.imgdb.cn/item/6162fdf52ab3f51d91263fd2.png" alt></p>
<p>接下来则是评估操作需要转换为对存储引擎的哪些操作。如果不是事务则无需要等待复制，lease holder可以直接响应协调者。如果是写操作则会被复制，达成共识后会应用到本地引擎。最后释放锁，响应协调者。</p>
<h3 id="Atomicity-Guarantees"><a href="#Atomicity-Guarantees" class="headerlink" title="Atomicity Guarantees"></a>Atomicity Guarantees</h3><p>CRDB通过观察所有事务的write intents来实现原子提交。write intents就是一个常规的MVCC kv对，所有写入都先写到临时的write intents，其带有的一个元数据指示是intents。该元数据会指向一个事务记录，事务记录就是一个特殊的key，保存了事务当前的状态：pending, staging, committed和aborted。通过write intents，一个Reader会读取其事务记录，如果事务记录已提交，则Reader会将intents视为常规值，并执行清除操作。如果事务是pending的，则会阻塞直至完成。如果是事务是Staging，则可能是commit，也可能是Abort，Reader尝试中止该事务。（如果所有写操作都已经完成复制，实际上就是commit，并对其进行更新）。</p>
<h3 id="Concurrency-Control"><a href="#Concurrency-Control" class="headerlink" title="Concurrency Control"></a>Concurrency Control</h3><p>如前所述，CRDB对每个事务都以commit时间戳来执行写入和读取。这样可以实现串行化的执行，但同时也会因为事物之间的冲突需要调整时间戳。</p>
<h4 id="Write-read-conflicts"><a href="#Write-read-conflicts" class="headerlink" title="Write-read conflicts"></a>Write-read conflicts</h4><p>读请求遇到未commit的intent，如果其时间戳更小，则需要等待事务结束。否则可以忽略，并不需要等待。</p>
<h4 id="Read-write-conflicts"><a href="#Read-write-conflicts" class="headerlink" title="Read-write conflicts"></a>Read-write conflicts</h4><p>以时间戳tb写一个key的时候，如果存在对该key有更大时间戳tb的读请求，则无法直接写入。CRDB强制将写入事务的commit时间戳增加到tb以后。</p>
<h4 id="Write-write-conflicts"><a href="#Write-write-conflicts" class="headerlink" title="Write-write conflicts"></a>Write-write conflicts</h4><p>写请求如果遇到一个时间戳更小的未commit intent，则等到较早的事务完成。相反，遇到较大时间戳的committed key，则会推进该时间戳。写写冲突可能导致冲突，CRDB会采用分布式死锁检测算法来中止循环等待中的一个事务。</p>
<h3 id="Read-Refreshes"><a href="#Read-Refreshes" class="headerlink" title="Read Refreshes"></a>Read Refreshes</h3><p>前面提到的冲突会导致事务的commit timestamp推进。如果可以证明事务在ta读取的数据于(ta, tb]时间间隔内没有更小，可以将事务的read timestamp推进到tb &gt; ta。如果发生了数据变更，则会导致事务重启。</p>
<p>为了确定时间戳是否可以推进，CRDB会在事务的读取集中维护键的集合，并会发出一个Read Refreshes请求来确认key在某个时间间隔内有没有被更新。</p>
<h3 id="Follower-Reads"><a href="#Follower-Reads" class="headerlink" title="Follower Reads"></a>Follower Reads</h3><p>CRDB允许非租约持有者的副本通过查询修饰符“AS OF SYSTEM TIME”来为带有时间戳的只读查询提供服务。为了提供该功能，其要求在给定时间戳T上执行读取操作的非租约持有者副本确认：将来不存在任何写操作使得读操作无效，并且具备读取所需要的所有数据。即要求Follower提供在时间戳T上的读取内容，并且租约持有者不再接受时间戳T’&lt;=T的写操作，Follower还要追上影响到MVCC快找的Raft前缀日志。</p>
<p>因此，租约持有者需要记录所有传入请求的时间戳，并定期在节点级别生成closed timestamp，在该时间戳以前，将不接受进一步的写操作。closed timestamp与当时的Raft日志索引一起在副本之间定期交换，这样Follower副本就可以使用该状态来决定它们是否剧本在特定时间戳下提供一致性读取所需的所有数据。</p>
<h2 id="CLOCK-SYNCHRONIZATION"><a href="#CLOCK-SYNCHRONIZATION" class="headerlink" title="CLOCK SYNCHRONIZATION"></a>CLOCK SYNCHRONIZATION</h2><p>CRDB不依赖特定的硬件来进行时钟同步，通过NTP或者Amazon Time Sync Service在公有云或者私有云的服务器即可。</p>
<h3 id="Hybrid-Logical-Clocks"><a href="#Hybrid-Logical-Clocks" class="headerlink" title="Hybrid-Logical Clocks"></a>Hybrid-Logical Clocks</h3><p>CRDB在集群的每个节点里都维护一个混合逻辑时钟HLC，该时钟提供了物理时间和逻辑时间组合的时间戳。物理时间基于节点的粗同步系统时钟，洛基适中则是基于Lamport时钟。</p>
<p>HLC提供了一些重要的属性：</p>
<ol>
<li>HLC在每次节点交换时钟时通过其逻辑组建提供了因果追踪。节点在发送消息的时候会附加上HLC时间戳，并使用接收到的信息里的时间戳来更新本地时钟。</li>
<li>HLC在单个节点上的重启内或者重启之间都提供了严格的单调性。</li>
<li>在瞬态时钟的偏斜波动情况下，HLC能提供自我稳定的功能。</li>
</ol>
<h3 id="Uncertainty-Intervals"><a href="#Uncertainty-Intervals" class="headerlink" title="Uncertainty Intervals"></a>Uncertainty Intervals</h3><p>CRDB不支持严格的可串行化，而是通过追踪每个事务的不确定间隔来满足单key线性化属性，事务协调者的本地HLC会分配一个commit_ts，不确定性间隔为[commit_ts, commit_ts + max_offset]。</p>
<p>当事务遇到在高于commit_ts且在不确定间隔内遇到key时，它会执行不确定性重启，将commit_ts移动到高于不确定值的位置，并保持不确定间隔的上限不变。</p>
<h3 id="Behavior-under-Clock-Skew"><a href="#Behavior-under-Clock-Skew" class="headerlink" title="Behavior under Clock Skew"></a>Behavior under Clock Skew</h3><p>这里主要是考虑时钟偏离范围的系统行为。本身在单个Range内，通过Raft日志的读写是能够保持任意时钟偏斜下的一致性。但因为引入了Range的租约，如果存在较大的时钟偏移，多个节点可能会发生脑裂，都认为自己拥有租约。CRDB采用两种保护措施来做保护：</p>
<ol>
<li>Range租约包含了开始和结束时间戳。租约持有者不能为超出租约间隔的读写提供服务；</li>
<li>每次写入Range的Raft日志时，都会包含建议使用的Range租约序列号。由于Range本身的租约变更也会被写入Range的Raft日志，因此某个时刻内只有一个租约持有者能够对Range进行变更；</li>
</ol>
<h2 id="SQL-1"><a href="#SQL-1" class="headerlink" title="SQL"></a>SQL</h2><h3 id="SQL-Data-Model"><a href="#SQL-Data-Model" class="headerlink" title="SQL Data Model"></a>SQL Data Model</h3><p>每个SQL表或者索引都是存储在若干个Ranges里。所有的用户数据则存储在若干个索引中，其中一个是primary index，primary index在主key上，其他列存储在value里。Secondary Index则是在索引key上，并且还会存储主key以及所以模式所示定的任意数量列。</p>
<h3 id="Query-Optimizer"><a href="#Query-Optimizer" class="headerlink" title="Query Optimizer"></a>Query Optimizer</h3><p>CRDB中的转换规则时通过Optgen的DSL编写的，提供了用于定义、匹配等简洁语法。Optgen编译为Go，然后与其余CRDB代码库无缝衔接。另外，考虑到CRDB的某些规则涉及到地理分布和分区性质，因此优化器会将数据分布考虑到cost model内，会复制辅助索引以使每个地区都有其自己的副本，从而提高性能，减少跨地区的数据通信。</p>
<h3 id="Query-Planning-and-Execution"><a href="#Query-Planning-and-Execution" class="headerlink" title="Query Planning and Execution"></a>Query Planning and Execution</h3><p>CRDB的SQL查询执行存在两种模式： gateway-only mode和distributed mode。</p>
<p>由于分布层提供了一个全局key空间的抽象视图，SQL层可以对任何节点上的Ranges执行读写操作。CRDB根据需要的网络带宽来决定采用哪种模型。对于distributed mode，CRDB通过物理计划阶段，将查询优化器的计划转换为物理SQL运算符的有向无环图。物理计划将逻辑扫描操作分为多个TableReader操作符，每个节点都会包含一个扫描需要读取的Range。扫描被分割后，剩余的逻辑运算符会被安排到与TableReader相同的节点上，从而将filters, joins, and aggregations这些推到尽可能接近物理数据的位置。</p>
<p><img src="https://pic.imgdb.cn/item/6162fdf52ab3f51d91263fe2.png" alt></p>
<p>在数据流内部，CRDB根据输入基数和计划复杂性采用两种不同的执行引擎： Row-at-a-time execution engine和Vectorized execution engine。前者主要是基于Volcano迭代器模型，一次一行。后者则是采用受MonetDB/X100启发的向量化执行引擎，主要面向的是列数据，而不是行，从CRDB的KV层读取时，会将磁盘数据从行格式转换为列格式，才发送会有用户之前再次将列格式转换为行格式。</p>
<h3 id="Schema-Changes"><a href="#Schema-Changes" class="headerlink" title="Schema Changes"></a>Schema Changes</h3><p>CRDB使用一种协议来执行Schema Changes，例如添加列或二级索引，该协议允许在Schema Changes期间保持表能提供在线的读写服务，允许不同的节点在不同时间内异步过渡到新表。具体的实现是将每个Schema Changes分解为一系列增量变更的协议来实现。</p>
<h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>CockroachDB是一个开源的SQL DMBS，支持全球性分布的OLTP业务，并提供了灵活的SQL使用，保证了更好的伸缩性和高可靠性、高性能。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/08/01/Rocksdb源码学习一/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/08/01/Rocksdb源码学习一/" itemprop="url">RocksDB源码学习一</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-08-01T23:59:54+08:00">
                2021-08-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h2><p>第一章先来看一下Rocksdb的编码情况，具体的实现在：<code>util/coding.cc, util/coding.h, util/coding_lean.h</code>。Rocksdb的编码实现与leveldb基本一致的，由于需要将Key、Value等数据按序写入到内存中，并最终flush到磁盘上，因此需要一个高效的编解码实现。这里的编解码很多会用在key size、value size的整型编码上面。</p>
<p>Rocksdb的整型编码方式很简单，主要支持两种方案：定长编码和变长编码。</p>
<ul>
<li>定长编码：定长编码的实现比较简单，比如可以直接将4字节/8字节的整型直接按序写入到指定的位置；</li>
<li>变长编码：节省空间，如果用定长编码的方式，实际上对于小数来说，很多位其实是没必要存储。结合ASCII码的特点，所有字符ASCII码的最高位都是0，可以利用最高位去做一些特别的标记；</li>
</ul>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="字节端序"><a href="#字节端序" class="headerlink" title="字节端序"></a>字节端序</h4><p>关于编码，首先需要了解计算机在存储字节时分为大端字节序和小端字节序两种，Rocksdb是用小端序存储（低位放在较小的地址处，高位放在较大的地址处）的，因此需要提供根据不同平台对内存存储模型进行转换的选项。</p>
<p>在port/port.h文件内包含了一些平台相关的的头文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">if</span> defined(ROCKSDB_PLATFORM_POSIX)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"port/port_posix.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">elif</span> defined(OS_WIN)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"port/win/port_win.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>以<code>port_posix.h</code>为例，这里包含了posix平台关于大小端的定义：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> PLATFORM_IS_LITTLE_ENDIAN</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> defined(OS_MACOSX)</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;machine/endian.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">if</span> defined(__DARWIN_LITTLE_ENDIAN) &amp;&amp; defined(__DARWIN_BYTE_ORDER)</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN \</span></span><br><span class="line">        (__DARWIN_BYTE_ORDER == __DARWIN_LITTLE_ENDIAN)</span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">elif</span> defined(OS_SOLARIS)</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/isa_defs.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">ifdef</span> _LITTLE_ENDIAN</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN true</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN false</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;alloca.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">elif</span> defined(OS_AIX)</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/nameser_compat.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN (BYTE_ORDER == LITTLE_ENDIAN)</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;alloca.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">elif</span> defined(OS_FREEBSD) || defined(OS_OPENBSD) || defined(OS_NETBSD) || \</span></span><br><span class="line">    defined(OS_DRAGONFLYBSD) || defined(OS_ANDROID)</span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/endian.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN (_BYTE_ORDER == _LITTLE_ENDIAN)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;endian.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> kLittleEndian = PLATFORM_IS_LITTLE_ENDIAN;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> PLATFORM_IS_LITTLE_ENDIAN</span></span><br></pre></td></tr></table></figure>
<h4 id="定长编码"><a href="#定长编码" class="headerlink" title="定长编码"></a>定长编码</h4><p>定长编码比较简单，首先判断大小端序，对于小端，value本身就是按小端排放的，可以直接拷贝；对于大端，则是将value的最低位放置在内存的最低地址端。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">EncodeFixed32</span><span class="params">(<span class="keyword">char</span>* buf, <span class="keyword">uint32_t</span> value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (port::kLittleEndian) &#123;</span><br><span class="line">    <span class="built_in">memcpy</span>(buf, &amp;value, <span class="keyword">sizeof</span>(value));</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    buf[<span class="number">0</span>] = value &amp; <span class="number">0xff</span>;</span><br><span class="line">    buf[<span class="number">1</span>] = (value &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xff</span>;</span><br><span class="line">    buf[<span class="number">2</span>] = (value &gt;&gt; <span class="number">16</span>) &amp; <span class="number">0xff</span>;</span><br><span class="line">    buf[<span class="number">3</span>] = (value &gt;&gt; <span class="number">24</span>) &amp; <span class="number">0xff</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>解码也一样，Rocksdb提供了三种整型编解码：<code>uint16_t,uint32_t,uint64_t</code>。解码时，gcc会优化里面的memcpy的操作，变成inline copy loops，提高效率。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> uint32_t <span class="title">DecodeFixed32</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* ptr)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (port::kLittleEndian) &#123;</span><br><span class="line">    <span class="comment">// Load the raw bytes</span></span><br><span class="line">    <span class="keyword">uint32_t</span> result;</span><br><span class="line">    <span class="built_in">memcpy</span>(&amp;result, ptr, <span class="keyword">sizeof</span>(result));  <span class="comment">// gcc optimizes this to a plain load</span></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(<span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>&gt;(ptr[<span class="number">0</span>]))) |</span><br><span class="line">            (<span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(<span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>&gt;(ptr[<span class="number">1</span>])) &lt;&lt; <span class="number">8</span>) |</span><br><span class="line">            (<span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(<span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>&gt;(ptr[<span class="number">2</span>])) &lt;&lt; <span class="number">16</span>) |</span><br><span class="line">            (<span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(<span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>&gt;(ptr[<span class="number">3</span>])) &lt;&lt; <span class="number">24</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="变长编码"><a href="#变长编码" class="headerlink" title="变长编码"></a>变长编码</h4><p>前面提到的，为了节省空间，Rocksdb使用变长的编码方式<em>varint</em>。Rocksdb使用最高位来表示编码是否结束，而低7bit则存储实际的数据。根据统计来说，小整型出现的概率更高，这样就能节省更多的空间，比如0-127的整数都可以只用一个字节来表示，而uint32较大的数字则需要5个字节。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">0001 0001 ====&gt;&gt; 表示33</span><br><span class="line">^                           A: 最高位为0，表示结束；</span><br><span class="line">A</span><br><span class="line">=======================================================</span><br><span class="line">1000 0011 0110 1111 ====&gt;&gt; 表示1007</span><br><span class="line">^         ^                 A: 最高位为1，表示未结束，实际值是000 0011；</span><br><span class="line">A         B                 B: 最高位为0，表示结束，实际值是110 1111；</span><br></pre></td></tr></table></figure>
<p>具体的编码以<code>uint32_t</code>为例，将<code>uint32_t</code>编码成变长字符：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">EncodeVarint32</span><span class="params">(<span class="keyword">char</span>* dst, <span class="keyword">uint32_t</span> v)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Operate on characters as unsigneds</span></span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">char</span>* ptr = <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>*&gt;(dst);</span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> B = <span class="number">128</span>;</span><br><span class="line">  <span class="keyword">if</span> (v &lt; (<span class="number">1</span> &lt;&lt; <span class="number">7</span>)) &#123;</span><br><span class="line">    *(ptr++) = v;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span> &lt;&lt; <span class="number">14</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = v &gt;&gt; <span class="number">7</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span> &lt;&lt; <span class="number">21</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = v &gt;&gt; <span class="number">14</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span> &lt;&lt; <span class="number">28</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">14</span>) | B;</span><br><span class="line">    *(ptr++) = v &gt;&gt; <span class="number">21</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">14</span>) | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">21</span>) | B;</span><br><span class="line">    *(ptr++) = v &gt;&gt; <span class="number">28</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">char</span>*&gt;(ptr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>uint64_t</code>的变长编码实现，作者用了循环来编码，每7bit一组，并在最低位判断是否需要置位1。因此对于64位整型，最多需要10个字节：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">char</span>* <span class="title">EncodeVarint64</span><span class="params">(<span class="keyword">char</span>* dst, <span class="keyword">uint64_t</span> v)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> B = <span class="number">128</span>;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">char</span>* ptr = <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>*&gt;(dst);</span><br><span class="line">  <span class="keyword">while</span> (v &gt;= B) &#123;</span><br><span class="line">    *(ptr++) = (v &amp; (B - <span class="number">1</span>)) | B; <span class="comment">// leveldb的实现是v | B; 不明白区别在哪</span></span><br><span class="line">    v &gt;&gt;= <span class="number">7</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  *(ptr++) = <span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>&gt;(v);</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">char</span>*&gt;(ptr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>解码的实现也有一些优化，主要是利用内联函数提高效率，当数字小于等于127时，直接返回结果：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">GetVarint32Ptr</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* p,</span></span></span><br><span class="line"><span class="function"><span class="params">                                  <span class="keyword">const</span> <span class="keyword">char</span>* limit,</span></span></span><br><span class="line"><span class="function"><span class="params">                                  <span class="keyword">uint32_t</span>* value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (p &lt; limit) &#123;</span><br><span class="line">    <span class="keyword">uint32_t</span> result = *(<span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">char</span>*&gt;(p));</span><br><span class="line">    <span class="keyword">if</span> ((result &amp; <span class="number">128</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">      *value = result;</span><br><span class="line">      <span class="keyword">return</span> p + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> GetVarint32PtrFallback(p, limit, value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">GetVarint32PtrFallback</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* p, <span class="keyword">const</span> <span class="keyword">char</span>* limit,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   <span class="keyword">uint32_t</span>* value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">uint32_t</span> result = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">uint32_t</span> shift = <span class="number">0</span>; shift &lt;= <span class="number">28</span> &amp;&amp; p &lt; limit; shift += <span class="number">7</span>) &#123;</span><br><span class="line">    <span class="keyword">uint32_t</span> byte = *(<span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">char</span>*&gt;(p));</span><br><span class="line">    p++;</span><br><span class="line">    <span class="keyword">if</span> (byte &amp; <span class="number">128</span>) &#123;</span><br><span class="line">      <span class="comment">// More bytes are present</span></span><br><span class="line">      result |= ((byte &amp; <span class="number">127</span>) &lt;&lt; shift);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      result |= (byte &lt;&lt; shift);</span><br><span class="line">      *value = result;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">const</span> <span class="keyword">char</span>*&gt;(p);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>编解码这里还是比较简单和高效的，比较有意思的是实现了一个variant编码，源码值得一看。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/07/26/Dynamo-Amazon’s-Highly-Available-Key-value-Store/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/07/26/Dynamo-Amazon’s-Highly-Available-Key-value-Store/" itemprop="url">Dynamo: Amazon’s Highly Available Key-value Store</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-07-26T00:10:16+08:00">
                2021-07-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Dynamo"><a href="#Dynamo" class="headerlink" title="Dynamo"></a>Dynamo</h1><blockquote>
<p>原文是2007年SOSP上Amazon发布的分布式存储经典论文<a href="https://sites.cs.ucsb.edu/~agrawal/fall2009/dynamo.pdf" target="_blank" rel="noopener">《<strong>Dynamo: Amazon’s Highly Available Key-value Store</strong>》</a>。这是一个高可用的分布式KV存储——Dynamo，Amazon的一些核心服务就是依赖Dynamo提供持续可用的服务，为了达到这种可用级别，Dynamo牺牲了几种特定场景下的一致性。并且，Dynamo大量地使用了对象版本化和应用层面的冲突解决机制。</p>
</blockquote>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>支撑着Amazon电商发展的是建立在分布于全球数据中心成千上万的服务器基础上的，因此对性能、可靠性和效率都有很高的要求。同时为了支撑业务的持续增长和避免因故障导致的经济损失，平台需要有足够好的可扩展性、可靠性。</p>
<p>Amazon使用的是去中心化的、松耦合的、面向服务的架构，这种服务架构对持续可用的存储技术有着强烈的诉求。例如，即便是磁盘故障、路由抖动、数据中心被摧毁，用户仍然能够向购物车添加和查看商品。因此Amazon推出了一款高可用的kv存储组件——Dynamo。Dynamo用于管理对可靠性要求非常高的服务，这些服务还要求对一致性、成本效率和性能有很强的控制能力。</p>
<p>Dynamo使用了一些常见的技术来实现了可扩展性和高可用性：</p>
<ul>
<li>数据通过一致性哈希来分区和复制；</li>
<li>通过对象版本化来实现一致性；</li>
<li>副本之间的一致性使用了类似quorum的技术和一个去中心化的副本同步协议；</li>
<li>gossip-based分布式故障检测和成员检测协议；</li>
</ul>
<p>Dynamo是一个极少需要人工管理的、完全去中心化的系统，向Dynamo添加或者删除节点不需要人工调整哈希节点和重分布节点间数据。</p>
<h2 id="BACKGROUND"><a href="#BACKGROUND" class="headerlink" title="BACKGROUND"></a>BACKGROUND</h2><p>传统上生产系统会使用关系型数据库来存储状态，但这并不是一种理想的方式，大多数服务并不需要RDBMS提供的复杂查询和管理功能，这些额外的支持带来的硬件成本并不经济，并且这类数据库的复制功能有局限，往往是通过牺牲可用性来换取一致性。</p>
<h3 id="System-Assumptions-and-Requirements"><a href="#System-Assumptions-and-Requirements" class="headerlink" title="System Assumptions and Requirements"></a>System Assumptions and Requirements</h3><p>Dynamo对于使用它的服务有几点假设：</p>
<ul>
<li>Query Model：通过唯一的key对数据进行读写，存储状态是binary objects。没有relational schema需求，无跨data items的操作。存储对象较小，往往小于1MB；</li>
<li>ACID Properties：Dynamo的设计目标是使用部分一致性来换取更高的可用性；</li>
<li>Efficiency：存储系统必须满足那些严格的SLA；</li>
<li>Other Assumptions：内部使用，假设环境足够安全；</li>
</ul>
<h3 id="Service-Level-Agreements-SLA"><a href="#Service-Level-Agreements-SLA" class="headerlink" title="Service Level Agreements (SLA)"></a>Service Level Agreements (SLA)</h3><p>在Amazon去中心化的基础设施中，SLA会扮演着重要的角色，客户端和服务端会定义一个 SLA协议。Amazon不是使用传统的平均值、中位数和方差来描述面向性能的SLA，而是更多使用了P99.9分布，来确定性能的长尾结果。</p>
<h3 id="Design-Considerations"><a href="#Design-Considerations" class="headerlink" title="Design Considerations"></a>Design Considerations</h3><p>前面提过，很多系统中数据复制算法一般是同步的，为了提供一个强一致性的数据访问结果，往往会牺牲掉某些场景下的可用性。考虑到这一点，Dynamo最终被设计为最终一致的数据存储系统。</p>
<p>在分布式系统中，需要关注的是机器或者网络故障时可能会导致数据冲突，需要检测和解决冲突。一些传统的数据库可能会在写的时候解决冲突，牺牲一点可用性。但Dynamo的目标是提供一个持续可写的存储，因此将解决冲突的逻辑放到了读操作，从而避免写操作被拒绝。同时Dynamo可以配置是存储系统来解决冲突还是应用选择自行实现冲突解决操作。</p>
<h2 id="SYSTEM-ARCHITECTURE"><a href="#SYSTEM-ARCHITECTURE" class="headerlink" title="SYSTEM ARCHITECTURE"></a>SYSTEM ARCHITECTURE</h2><p>本文主要介绍了Dynamo用到的部分分布式系统技术：包括partitioning, replication, versioning, membership, failure handling 和 scaling。</p>
<h3 id="System-Interface"><a href="#System-Interface" class="headerlink" title="System Interface"></a>System Interface</h3><p>Dynamo的存储接口非常简单，只有两个：</p>
<ul>
<li>get()：会返回存储key对应的所有对象副本，以及一个context；</li>
<li>put()：确定对象的存储位置，写入到相应的磁盘。</li>
</ul>
<p>Dynamo将调用方提供的key和对象都视作是opaque array of bytes，其对key应用MD5哈希得到一个128bit的ID，并根据ID计算应该存储到哪个存储节点。</p>
<h3 id="Partitioning-Algorithm"><a href="#Partitioning-Algorithm" class="headerlink" title="Partitioning Algorithm"></a>Partitioning Algorithm</h3><p>Dynamo的设计有一个核心诉求：支持增量扩展。这就要求有一种机制能够将数据分散到系统的不同节点中，Dynamo的方案是基于一致性哈希，其哈希函数的输出是一个固定范围，作为一个循环空间环，每个节点会随机分配一个循环空间内的值，代表着节点在环上的节点。</p>
<p>Dynamo寻找item对应节点的方法：</p>
<ul>
<li>首先对key做哈希得到哈希值；</li>
<li>然后在环上沿着顺时针方向找到第一个多带值被这个哈希值更大的节点；</li>
</ul>
<p>这种方法有一个缺陷，就是每个节点随机分配的位置可能会导致数据不均匀分布负载，也没有考虑到节点的异构因素。为了解决这些问题，Dynamo做了优化，每个节点不是映射到环上的一个点，而是多个点。Dynamo使用了虚拟节点的概念，一个新节点加入到系统后，会在环上分配多个位置（对应多个token）。</p>
<p>虚拟节点的好处就是：</p>
<ul>
<li>当一个节点不可用离开时，会将该节点管理的虚拟节点平均分配给其他真实节点均衡管理；</li>
<li>同理，新节点加入时，会从现有虚拟节点中拿出虚拟节点分配给新节点；</li>
<li>一个节点负责的虚拟节点数量可以根据节点容量来决定，充分利用机器的异构性信息；</li>
</ul>
<h3 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h3><p>为了实现高可用性和持久性，Dynamo会将数据复制到N台机器上，N可配置。</p>
<p>具体的做法是，每个Key都会分配一个coordinator节点，coordinator负责落到它管理范围内的复制，除了自己存储一份之外，还会沿着顺时针方向的其他N-1个节点存储一份副本。</p>
<p>如下图，B除了自己存储一份之外，还会将数据存储到C和D节点。D实际存储的数据，其key范围包括了<code>(A, B]</code>、<code>(B, C]</code> 和 <code>(C, D]</code>。</p>
<p><img src="https://pic.imgdb.cn/item/60fd8cb75132923bf82b2916.png" alt></p>
<p>存储某个特定key的所有节点会组成一个<strong>preference list</strong>，为了防止节点的failure，整这个preference list可能多于N个节点，另外由于引入了虚拟节点机制，preference list会保证N个节点不落在相同的物理机上。</p>
<h3 id="Data-Versioning"><a href="#Data-Versioning" class="headerlink" title="Data Versioning"></a>Data Versioning</h3><p>Dynamo提供最终一致性，所有更新操作会异步地传递给所有的副本。put()操作返回时，更新可能还没有应用到所有的副本，后续的get操作可能去不到最新数据。Amazon有些应用是可以容忍这种不一致性的，应用在这种情况下能继续运行。以操作购物车威力，如果购物车的最新状态不可用，用户对一个老版本的购物车状态做了修改，这种修改是需要保留的，由后续的步骤来解决冲突。</p>
<p>为了解决冲突，Dynamo将每次修改结果都作为一个全新的版本，允许系统多个版本共存。使得冲突一致化的两种方式：syntactic reconciliation和semantic reconciliation。在大多数情况下，新版本都包含了老版本的数据，而且系统可以判断哪个是权威版本，这就是syntactic reconciliation。</p>
<p>但是在发生故障并且并发更新的情况下，版本可能发生分叉，系统无法处理这种情况，需要客户端介入，从而将多个版本分支合并成一个，这就是semantic reconciliation。这种操作的好处是写操作永远可用，但会导致业务应用上一些奇怪现象，比如已经删除的商品偶尔又在购物车中冒出来。</p>
<p>Dynamo使用<strong>向量时钟（vector clock）</strong>来追踪同一个对象不同版本之间的因果关系，一个向量时钟就是一个 (node, counter) 列表。一个向量时钟关联了一个对象的所有版本，可以用来判断对象两个版本是并行分支还是具备因果关系。如果对象的第一个时钟上的所有 counter 都小于它的第二个时钟上的 counter，那第一 时钟就是第二个的祖先，可以安全的删除。否则需要进行reconciliation。</p>
<p>在Dynamo中，客户端更新一个对象需要指明基于哪个版本进行更新。流程是先读拿到context，context带有vector clock，写的时候把context带下去。在读的时候如果发现了多个版本，并且系统无法reconcile这些版本，就会返回所有的版本，待解决了冲突将多个版本分支合并起来。</p>
<p>以下图为例</p>
<p><img src="https://pic.imgdb.cn/item/60fd8cb75132923bf82b2923.png" alt></p>
<ul>
<li>客户端写入一个对象。处理这个key的写请求节点Sx增加key的counter，系统有了一个对象D1和它的时钟[(Sx, 1)]；</li>
<li>客户端更新这个对象。假设还是Sx处理这个请求。此时，系统有了对象D2和它的时钟 [(Sx, 2)]，但可能D1在其他节点的副本还没有看到这个更新；</li>
<li>假设这个客户端，再次更新了对象，并且这次是由另外的一个节点Sy处理 请求。此时，系统有了D3和它的时钟[(Sx, 2), (Sy, 1)]。假设另一个客户端读取D2，并尝试更新它，写请求由另一个节点Sz处理。 现在，系统有D4（D2的后代），版本clock是[(Sx, 2), (Sz, 1)]。</li>
<li>此时，D3和D4各自的改动都没有反映在对方之中。因此这两个版本都应当被保留，然后交给客户端，由客户端在下次读取的时候执行semantic reconciliation；</li>
<li>假设某个客户端读到了D3和D4，即[(Sx, 2), (Sy, 1), (Sz, 1)]。如果客户端执行 reconciliation，并且节点Sx执行协调写，Sx会更新自己在clock中的序列号。最终新生成的数据D5的clock格式如下：[(Sx, 3), (Sy, 1), (Sz, 1)]。</li>
</ul>
<p>vector clock的一个潜在问题是，如果有多个节点先后协调同一个对象的写操作，那这个对象的clock vector会变得很长。这种情况发生的可能性不大，只有在网络分裂或多台服务器挂掉的情况下，写操作才可能由非preference list前N个节点来执行，导致vector clock变长。</p>
<p>为了避免这个问题，Dynamo采用的方法是clock truncation scheme，另外保存一个和<code>(node, counter)</code> 对应的时间戳，记录最后一次更新该记录的时间，当vector clock达到阈值时就删掉最老的一个。这种方案可能会导致无法精确判断部分后代的因果关系，但论文说生产环境没遇到过这个问题。</p>
<h3 id="Execution-of-get-and-put-operations"><a href="#Execution-of-get-and-put-operations" class="headerlink" title="Execution of get () and put () operations"></a>Execution of get () and put () operations</h3><p>Dynamo中所有存储节点都可以接受key的读写操作。</p>
<p>读写操作由Amazon基础设施相关的请求处理框架发起HTTP请求。客户端有两种选择：</p>
<ul>
<li>将请求路由到负载均衡器，由后者根据负载信息选择后端节点；</li>
<li>使用能感知partition的客户端，直接路由到coordinator节点；</li>
</ul>
<p>前者是负载均衡器转发到环上任意一个节点，如果收到请求的节点不是preference list前N个节点中的一个，那它就不会处理这个请求，而是转发到preference list第一个节点。</p>
<p>读写操作需要preference list中有不可用节点，就跳过。优先访问preference list中编号较小的节点。</p>
<p>为了保证副本的一致性，Dynamo使用了一种类似quorum的一致性协议。这个协议有两个配置参数：<code>R</code> 和 <code>W</code>：</p>
<ul>
<li>R：允许执行一次读操作所需的最少节点数；</li>
<li>W：允许执行一次写操作所需的最少节点数；</li>
</ul>
<p>设置<code>R + W &gt; N</code>，就得到了一个quorum系统。在这种模型下，读写请求由R/W副本中最慢的一个决定。</p>
<p>当收到写请求后，coordinator 会为新版本生成 vector clock，并将其保存到节点本地。然后将新版本（和对应的vector clock）发送给N个排在最前面的、可用的节点，只要有至少W-1个节点返回成功，就认为写操作成功。</p>
<p>读操作类似，如果coordinator收集到多个版本，它会将所有系统判断没有因果关系的版本返 回给客户端。客户端需要对版本进行reconcile，合并成一个最新版本，然后将结果写回 Dynamo。</p>
<h3 id="Handling-Failures-Hinted-Handoff"><a href="#Handling-Failures-Hinted-Handoff" class="headerlink" title="Handling Failures: Hinted Handoff"></a>Handling Failures: Hinted Handoff</h3><p>如果使用传统的quorum算法，Dynamo无法在节点不可用时保持可用。Dynamo采用了一种更为宽松quorum：所有读和写操作在preference list的前N个健康节点上执行，遇到不可用节点，会沿着哈希环的顺时针方向顺延。</p>
<p>以下图为例，如果A临时不可用，正常情况下发到A的请求会发送到D，发到D副本的元数据中会提示这个副本数据应该发到A，然后这个数据会被D保存到本地的一个独立数据库中，并且有一个定期任务不断扫描，一旦A可用了，就将这个数据发送回 A，然后D就可以从本地数据库中将其删除了。</p>
<h3 id="Handling-permanent-failures-Replica-synchronization"><a href="#Handling-permanent-failures-Replica-synchronization" class="headerlink" title="Handling permanent failures: Replica synchronization"></a>Handling permanent failures: Replica synchronization</h3><p>如果出现了在hinted副本移交给原副本节点之前就变的不可用，就会威胁到持久性。Dynamo基于Merkle trees实现了一种逆熵（副本同步）协议来保证副本是同步的。</p>
<h3 id="Membership-and-Failure-Detection"><a href="#Membership-and-Failure-Detection" class="headerlink" title="Membership and Failure Detection"></a>Membership and Failure Detection</h3><h4 id="Ring-Membership"><a href="#Ring-Membership" class="headerlink" title="Ring Membership"></a>Ring Membership</h4><p>Amazon使用显式机制来向Dynamo环增删节点，管理员通过命令行或web方式连接到 Dynamo node，然后下发一个成员变更命令增删节点。负责处理这个请求的 node 将成员变动信息和对应的时间写入持久存储。成员变动会形成历史记录。Dynamo使用一个gossip-based的算法传播成员变更信息。</p>
<h4 id="External-Discovery"><a href="#External-Discovery" class="headerlink" title="External Discovery"></a>External Discovery</h4><p>上面的逻辑会有个问题，假设管理员同时添加两个节点，那么它们不会立即感知到对方，导致临时的逻辑分裂。为了避免这个问题，论文将部分Dynamo节点作为种子节点，所有节点都知道种子节点的存在，因为所有节点最终都会和种子节点reconcile成员信息，所以逻辑分裂就几乎不可能发生了。种子是从静态配置文件或者配置中心获取的。</p>
<h4 id="Failure-Detection"><a href="#Failure-Detection" class="headerlink" title="Failure Detection"></a>Failure Detection</h4><p>故障检测在Dynamo中的读写操作或者partition和hinted replica时移跳过不可用的节点。其做法是，节点B只要没有应答节点A的消息，A就认为B不可达。在有持续的client请求时，Dynamo Ring上的节点会有持续的交互，能定期检查及诶但是否恢复。使用简单的gossip协议就可以感知到节点的增删。</p>
<h3 id="Adding-Removing-Storage-Nodes"><a href="#Adding-Removing-Storage-Nodes" class="headerlink" title="Adding/Removing Storage Nodes"></a>Adding/Removing Storage Nodes</h3><p>当新节点加入系统后，会获得一些随机分散到Ring上的token，此时原本负责某些key range的节点会将此时负责的key转移到新节点。</p>
<h2 id="IMPLEMENTATION"><a href="#IMPLEMENTATION" class="headerlink" title="IMPLEMENTATION"></a>IMPLEMENTATION</h2><p>Dynamo支持选择不同的本地存储组件来作为存储引擎，其实以插件的方式引入的，包括了BDB、Mysql、in-memory buffer with persistent backing store等。应用能够为不同的访问类型选择最合适的存储引擎。</p>
<p>coordinator会代替客户端执行读写请求，每个客户端请求都会在收到这个请求的节点上创建一个状态机，包括了所有相关的逻辑。</p>
<p>对于写请求，前面提到过由preference list前N个节点中任一个coordinate，总是由第一个来coordinate的好处是使得在同一个地方完成写操作的顺序化，但可能会导致复杂不均匀。为了解决这个问题，preference list内的所有N个节点都可以coordinate写操作，另外由于写操作前面都带有读操作，写操作的coordinator会选择前一次读操作返回最快的节点（这个信息会被存在返回的上下文中）。由于这项优化使得前一次读操作选中的存储节点更容易被选中，提高了read-your-writes的概率。</p>
<h2 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h2><p>本文介绍了Dynamo作为一个高可用、高可扩展性的数据存储系统，在Amazon的诸多核心系统中都有应用。Dynamo提供了期望的可用性与性能，能够很好处理节点不可用、网络分裂的情况。Dynamo最大的意义是证明了：一些去中心化的技术结合起来能够提供一个高可用的系统，并且在很多应用环境中投产了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/07/26/TiDB-A-Raft-based-HTAP-Database/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LucienXian">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LucienXian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/07/26/TiDB-A-Raft-based-HTAP-Database/" itemprop="url">TiDB: A Raft-based HTAP Database</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-07-26T00:03:14+08:00">
                2021-07-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TIDB"><a href="#TIDB" class="headerlink" title="TIDB"></a>TIDB</h1><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>Hybrid Transactional and Analytical Processing (HTAP，混合事务和分析处理)数据库要求独立地处理事务和分析查询，避免相互干扰。为了实现这一点，需要为两类查询维护不同的数据副本。然而，为存储系统中的分布式副本提供一致性视图并不容易。在该系统中，分析请求可以大规模地、高可靠性地从事务工作负载中读取一致的实时数据。</p>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>关系型数据库（RDBMS）一直因其关系模型、强力的事务保证和SQL接口而广受好评，但它不具备高可扩展性和高可用性。因此NoSQL就应运而生，像Google bigtable和DynamoDB之类的放宽了一致性要求，提供了更高的可扩展性。但由于业务又需要事务处理能力、数据一致性和SQL接口等，就慢慢出现了像CockroachDB和Spanner之类的NewSQL。此外，像许多架构在Hadoop之上的SQL系统一样，在线分析处理系统（OLAP）也在迅速发展。</p>
<p>以前一般认为针对OLAP和OLTP应该采用的不同的数据模型和技术方案，但维护多个系统的成本很高。业界开始探索OLTP和OLAP的混合系统HTAP。HTAP系统需要满足几个关键点：一是数据新鲜度，即OLAP需要拿到最新的数据；二是隔离，即为单独的OLTP或者OLAP查询提供不同的硬件资源处理，避免性能相互影响。</p>
<p>本文就是基于上面的考虑，提出了一个以Raft为共识算法的HTAP数据库——TiDB，在Raft中引入了一个专门的节点Learner，Learner会异步地复制Leader节点的事务日志，并为OLAP查询构造新副本，并将日志中的行格式元组转换为列格式，便于查询。</p>
<h2 id="RAFT-BASED-HTAP"><a href="#RAFT-BASED-HTAP" class="headerlink" title="RAFT-BASED HTAP"></a>RAFT-BASED HTAP</h2><p>使用共识算法如Raft、Paxos等可以基于复制状态机在服务器之间实时可靠地复制数据，通过调整，该论文的做法可以针对不同的 HTAP 工作负载将数据复制到不同的服务器，并且保持资源隔离和数据新鲜度。</p>
<p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf82635ad.png" alt></p>
<p>如上图所示，TiDB将数据按行格式存储在多个Raft Group里，每个Raft Group由一个Leader和多个Follower组成，另外为每个组增加一个Learner角色，可以异步复制Leader的数据，并将行格式转换为列格式。另外，扩展的查询优化器用来构建访问列存和行存的物理计划。</p>
<p>在该实现中，Leader不参与Leader选举，也不计入日志复制的个数，不参与Quorum。读数据时，需要保证Leader和Learner之间的强一致性和日志复制的低延迟。行列格式的转换也有优点，行格式可以利用索引来高效进行事务查询，列格式可以有效利用数据压缩和矢量化处理。由于Learner部署在单独的物理资源中，OLAP和OLTP可以在独立的资源中得到处理。</p>
<p>总的来说，这个设计克服了几个困难：一是基于Raft的系统如何应对高并发读写；二是资源独立如何保证数据新鲜度；另外就是如何构建查询计划在行式存储和列式存储中选择最优的。</p>
<h2 id="ARCHITECTURE"><a href="#ARCHITECTURE" class="headerlink" title="ARCHITECTURE"></a>ARCHITECTURE</h2><p>如下图所示，这是TIDB的架构，兼容MySQL协议，由三个核心组件组成：分布式存储层、Placement Driver布局驱动器和计算引擎层。</p>
<p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf82635cf.png" alt></p>
<ul>
<li>分布式存储层</li>
</ul>
<p>由行式存储（TiKV）和列式存储（TiFlash）组成，存储在TiKV的数据是有序的key-value对，key由两个整数table id和row id组成，其中row id就是主键列，value就是真实的行数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: &#123;table&#123;tableID&#125; record&#123;rowID&#125;&#125;</span><br><span class="line">Value: &#123;col0, col1, col2, col3&#125;</span><br></pre></td></tr></table></figure>
<p>为了保证可扩展性，TiDB采用了range partition的策略，将kv对映射分割成若干个连续的范围，每个范围称为一个region，每个region都有多个副本来保证可用性，Raft就是用于维护每个region中若干个副本的一致性的。</p>
<p>PD负责管理region，包括提供每个key对应的region和其物理位置，以及自动转移region以平衡负载。同时PD也是时间戳分配器，提供了严格递增全剧唯一的时间戳。PD不具备持久状态。</p>
<p>计算引擎层是无状态、可扩展的，其SQL引擎由cost-based query optimizer和distributed query executor组成。另外TiDB基于Percolator实现了2PC协议。</p>
<p>除此之外，TiDB还与Spark集成，方便集成TiDB和HDFS中存储的数据。</p>
<h2 id="MULTI-RAFT-STORAGE"><a href="#MULTI-RAFT-STORAGE" class="headerlink" title="MULTI-RAFT STORAGE"></a>MULTI-RAFT STORAGE</h2><p>下图展示了分布式存储层的架构，其由TiKV和TiFlash组成。每个Region的副本之间都使用Raft来维护数据一致性。当数据复制到TiFlash的时候，为了方便扫描，多个Regions会被合并成一个Partition。</p>
<p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf82635e8.png" alt></p>
<h3 id="Row-based-Storage"><a href="#Row-based-Storage" class="headerlink" title="Row-based Storage"></a>Row-based Storage</h3><p>TiKV是由多个TiKV服务器组成的，每个TiKV服务器都可以是不同Region的Raft Leader或者Follower，另外在TiKV服务器上，数据和原数据都会保存在RocksDB上。</p>
<p>基于Raft算法响应读写请求的过程如下：</p>
<ol>
<li>Region的Leader从SQL引擎层接受请求；</li>
<li>Leader将请求Append到日志中；</li>
<li>Leader向Follower发送新的日志条目，Follower会将接收到的日志Append到自己的日志中；</li>
<li>Leader等待Follower回应，满足指定数量的节点响应成功后，Leader会在本地commit并Apply；</li>
<li>Leader将结果发送给客户端；</li>
</ol>
<h4 id="Optimization-between-Leaders-and-Followers"><a href="#Optimization-between-Leaders-and-Followers" class="headerlink" title="Optimization between Leaders and Followers"></a>Optimization between Leaders and Followers</h4><p>为了提高吞吐，可以在Leaders和Followers之间的操作做一些优化。首先是，上述过程的第二步和第三步可以并行进行，即便第二步失败了，只要第三步成功了仍然可以提交日志。另外就是，第三步中Leader可以缓冲这些日志条目，并批量发送。并且发送日志后也不需要等待Follower响应，可以假设发送成功，并利用预测的日志索引发送后来的日志。即便出现错误，Leader可以调整日志索引进行重发。还有就是，第四步中，Leader应用日志可以交给其他线程去做。整体流程就变成：</p>
<ol>
<li>Region的Leader从SQL引擎层接受请求；</li>
<li>Leader并行地向Follower发送日志，并同时Append本地日志；</li>
<li>Leader继续接收请求，重复2；</li>
<li>Leader commit日志，并将应用逻辑交给另外的线程去做；</li>
<li>应用日志后，Leader返回结果；</li>
</ol>
<h4 id="Accelerating-Read-Requests-from-Clients"><a href="#Accelerating-Read-Requests-from-Clients" class="headerlink" title="Accelerating Read Requests from Clients"></a>Accelerating Read Requests from Clients</h4><p>从Leader读取数据具有线性化的语义，但通过常规的Raft流程来保证会导致很高的网络IO开销。为了提高性能，可以避免日志同步阶段。</p>
<p>Raft保证：一旦Leader成功写入数据，就可以响应任何读取请求，而无需同步日志。但Leader是可能改变的。为了实现从Leader读取，TiKV做了以下优化：</p>
<ul>
<li>read index：Leader响应请求时，会将当前的提交索引记录为本地read index，然后向Follower发送heartbeat确认Leader地位。如果确实是Leader，并且应用的索引大于或等于read index，就可以返回值。</li>
<li>lease read：为了减少heartbeat开销，Leader和Follower之间维护一个Lease期限，Follower在这期间不发出选举请求，因此Leader在此期间也无需与Follower进行heartbeat交流。</li>
</ul>
<p>Follower也可以响应client的读请求，但需要向Leader询问read index，如果本地应用索引等于或大于read index，则Follower可以将值返回给客户端。</p>
<h4 id="Managing-Massive-Regions"><a href="#Managing-Massive-Regions" class="headerlink" title="Managing Massive Regions"></a>Managing Massive Regions</h4><p>为了实现跨机器平衡Region，Plancement Driver会对Region副本数量和位置施予限制。一个就是必须要在不同的TiKV实例上放置至少三个Region副本。PD通过Heartbeat从服务器收集信息、监控服务器负载，并将热Region进行转移。</p>
<p>另一方面，维护大量Region涉及Heartbeat信息和元数据管理导致的大量的网络和存储开销，会被PD根据负载情况调整心跳频率。</p>
<h4 id="Dynamic-Region-Split-and-Merge"><a href="#Dynamic-Region-Split-and-Merge" class="headerlink" title="Dynamic Region Split and Merge"></a>Dynamic Region Split and Merge</h4><p>这里主要设计Region的拆分和合并。热点Region或者大型Region会被拆成小Region，小或者访问少的Region，会被合并成大Region，以减少由于维护小Region心跳和元数据带来的网络和CPU开销。</p>
<p>PD操作Region，是通过向TiKV发送拆分和合并指令，然后以Raft流程来完成更新请求。Region拆分比较简单，只需要更改元数据。合并的话，PD会移动两个Region的副本，放到单独的服务器上。然后通过两阶段操作，在每台服务器上本地合并两个Region的并置副本：即停止其一Region的服务并将其与另一Region合并。</p>
<h3 id="Column-based-Storage-TiFlash"><a href="#Column-based-Storage-TiFlash" class="headerlink" title="Column-based Storage (TiFlash)"></a>Column-based Storage (TiFlash)</h3><p>考虑到TiKV中的行存数据并不适合OLAP，因此将列存储TiFlash合并到TiDB中。TiFlash由Learner节点组成，仅从Raft组接收Raft日志，并将行格式的元祖转换为列存数据。</p>
<p>用户可以通过SQL语句为表设置列格式副本，<code>ALTER TABLE x SET TiFLASH REPLICA n;</code>其中x是表名，n是副本数量。在TiFlash中，每个表会按partitions进行划分，每个partitions包括几个连续Regions，更大的Regions便于范围扫描。</p>
<p>当初始化一个TiFlash实例时，相关Region的Leader开始讲数据复制到新的Learner。一旦初始化完成后，TiFlash开始监听Raft组的更新。Learner收到日志后，会将日志应用到本子状态机，包括日志重放、转换数据格式和更新本地存储中的引用值。</p>
<h4 id="Log-Replayer"><a href="#Log-Replayer" class="headerlink" title="Log Replayer"></a>Log Replayer</h4><p>由于在Raft中，Learner接收到的日志时可线性化的，因此重放日志也会按照FIFO的策略重放日志，具体步骤包括：</p>
<ul>
<li>压缩日志：事务日志分为三种状态：预写、提交或回滚。回滚的日志不需要写盘；</li>
<li>解码元组：缓冲区中的日志被解码为行格式的元组，去除关于事务的冗余信息；</li>
<li>转换数据格式：行元组会被转换为列数据；</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/60fd8be25132923bf8275779.png" alt></p>
<p>更具体的步骤可以参考上图。</p>
<h4 id="Schema-Synchronization"><a href="#Schema-Synchronization" class="headerlink" title="Schema Synchronization"></a>Schema Synchronization</h4><p>为了实时将元组转换为列格式，Learner需要知道最新的schema，因此TiFlash会通过Schema syncer与TiKV中最新的Schema同步。同时为了减少TiFlash向TiKV的请求次数，每个Learner都会维护一个schema cache。这里采取两阶段策略：</p>
<ul>
<li>Regular synchronization：定期同步；</li>
<li>Compulsive synchronization：synced检测到不匹配的schema，就会主动从TiKV拉去最新的Schema；</li>
</ul>
<h4 id="Columnar-Delta-Tree"><a href="#Columnar-Delta-Tree" class="headerlink" title="Columnar Delta Tree"></a>Columnar Delta Tree</h4><p>TiFlash设计了一个新的列存储引擎——Delta Tree，该引擎支持快速追加增量更新，然后将其与每个Partitions的稳定版本合并。如下图所示，在Stable space中，Partitions以chunks的形式存储，每个chunk都覆盖了一个较小的元组范围。TiFlash将元组的列数据及其元数据存储到不同的文件中，以同时更新文件。</p>
<p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf82635fd.png" alt></p>
<p>新进来的增量更新时插入或者删除数据的原子批处理，这些增量会缓存到内存中，并持久化道磁盘。另外TiFlash会定期将小增量压缩成大增量，并持久化。传入增量的内存副本有助于读取最新数据。</p>
<p>另外，读取的时候由于需要合并增量文件和稳定空间中的数据，而且增量文件本身也可能存在空间放大的问题。TiFlash需要定期将增量合并到稳定空间中，每个增量文件及其相关chunks被读入内存进行合并，合并后的chunks自动替换磁盘中的原始chunks。</p>
<p>由于相关的键在增量空间中是无序的，合并成本较高，并且也会影响合并读的速度，因此会在增量空间构建B+ Tree索引，每个增量更新项都被插入到 B+ Tree 中，并按其关键字和时间戳进行排序。便于快速响应读请求，和有序数据也更易与稳定块合并。</p>
<h4 id="Read-Process"><a href="#Read-Process" class="headerlink" title="Read Process"></a>Read Process</h4><p>与Follower read类似，Learner提供snapshot isolation，在接收到读取请求后，Learner向其Leader发送read index请求，以获取涵盖所请求时间戳的最新数据。Learner拿到日志后，回放日志，写入Delta Tree，就可以读到特定数据响应请求了。</p>
<h2 id="HTAP-ENGINES"><a href="#HTAP-ENGINES" class="headerlink" title="HTAP ENGINES"></a>HTAP ENGINES</h2><p>为了提供OLAP和OLTP查询，TiDB引入了SQL引擎来为了查询计划做决策。SQL引擎使用Percolator模型来实现分布式集群的乐观和悲观锁，并基于优化器、索引和下推算子来支持OLAP查询。</p>
<h3 id="Transactional-Processing"><a href="#Transactional-Processing" class="headerlink" title="Transactional Processing"></a>Transactional Processing</h3><p>TiDB为ACID事务提供了snapshot isolation语义或者repeatable read语义，前者允许每个请求读取版本一致的数据，后者则是事务中的不同语句可能为同一个key读取不同的值，但重复读取将会读取到相同的值。这是基于MVCC实现。</p>
<p>如下图，TiDB中的事务由SQL引擎、TiKV和PD三个组件共同完成：</p>
<p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf8263619.png" alt></p>
<p>TiDB对于悲观锁和乐观锁的实现启发自Percolator模型。</p>
<ol>
<li>client接收到Begin命令后，SQL引擎向PD请求一个start_ts时间戳；</li>
<li>SQL引擎从TiKV读取数据并在本地内存中执行SQL DMLs。TiKV提供在start_ts之前最近的commit_ts数据；</li>
<li>SQL引擎收到client的commit命令后，启动2PC协议。随机选择一个主键，并行锁定所有的key，并将prewrite发送TiKV节点；</li>
<li>如果所有的prewrite都成功了，SQL引擎会向PD请求一个commit_ts，并向TiKV发送commit命令。TiKV commit主键，并响应成功回到SQL引擎；</li>
<li>SQL引擎向Client响应成功；</li>
<li>SQL引擎向TiKV发送commit命令，异步并行地提交从键和清除锁；</li>
</ol>
<p>悲观事务和乐观事务的区别在于获取锁的实际，前者是在第二步执行DMLs的时候获取，后者则是第三步prewrite的时候。</p>
<h3 id="Analytical-Processing"><a href="#Analytical-Processing" class="headerlink" title="Analytical Processing"></a>Analytical Processing</h3><p>TiDB通过两阶段查询优化来实现优化器：rule-based optimization生成逻辑计划， cost-based optimization将逻辑计划转换为物理计划。由于TiDB有两类存储、因此扫描表往往有三种选项：扫描TiKV的行格式表、扫描TiKV中有索引的表和骚婊TiFlash的列。</p>
<p>索引可以提高点查询和范围查询的性能，TiDB实现了可扩展性的索引，由于维护索引会消耗大量资源，因此会在后台以非同步的形式构建或删除索引。索引是以与数据相同的方式按Regions分割，并作为键值存储在TiKV 中。唯一键索引上的索引项编码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: &#123;table&#123;tableID&#125; index&#123;indexID&#125; indexedColValue&#125;</span><br><span class="line">Value: &#123;rowID&#125;</span><br></pre></td></tr></table></figure>
<p>非唯一索引上的索引项被解码为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: &#123;table&#123;tableID&#125; index&#123;indexID&#125; indexedColValue rowID&#125;</span><br><span class="line">Value: &#123;null&#125;</span><br></pre></td></tr></table></figure>
<p>物理计划的执行是由SQL引擎层使用pulling iterator model进行的，通过将部分计算下推到存储层，可以进一步优化执行。在存储层，执行计算的组件称为coprocessor，coprocessor在不同的服务器上并行执行substrees of an execution 破烂，这减少了必须从存储层发送到引擎层的元组数量。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要是介绍了一个投入生产环境的HTAP数据库——TiDB。TiDB建立在TiKV上，这是一个基于Raft的分布式行式存储，并引入一个TiFlash组件用于实时分析，作为Raft的learner从TiKV异步复制日志，并将行格式的数据转换为列格式。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="LucienXian">
          <p class="site-author-name" itemprop="name">LucienXian</p>
           
              <p class="site-description motion-element" itemprop="description">LucienXian's Garden</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">263</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/LucienXian" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/feng-shao-37-35/activities" target="_blank" title="Zhihu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      Zhihu
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LucienXian</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
