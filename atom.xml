<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LucienXian&#39;s Blog</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2022-11-15T17:29:53.387Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>LucienXian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Building a Fast and Efficient LSM-tree Store by Integrating Local Storage with Cloud Storage</title>
    <link href="http://yoursite.com/2022/11/16/Building-a-Fast-and-Efficient-LSM-tree-Store-by-Integrating-Local-Storage-with-Cloud-Storage/"/>
    <id>http://yoursite.com/2022/11/16/Building-a-Fast-and-Efficient-LSM-tree-Store-by-Integrating-Local-Storage-with-Cloud-Storage/</id>
    <published>2022-11-15T17:29:26.000Z</published>
    <updated>2022-11-15T17:29:53.387Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Building-a-Fast-and-Efficient-LSM-tree-Store-by-Integrating-Local-Storage-with-Cloud-Storage"><a href="#Building-a-Fast-and-Efficient-LSM-tree-Store-by-Integrating-Local-Storage-with-Cloud-Storage" class="headerlink" title="Building a Fast and Efficient LSM-tree Store by Integrating Local Storage with Cloud Storage"></a>Building a Fast and Efficient LSM-tree Store by Integrating Local Storage with Cloud Storage</h1><blockquote><p>本文提出了一个叫RocksMash的LSM存储，使用本地存储来存放经常访问的数据和元数据，云存储保存其他数据以降低成本效益。</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>LSM KV存储像RocksDB、LevelDB、Dynamo等已经被广泛应用到了数据库存储系统中。随着云存储的出现，提高效率，出灾速度越来越受到关注，如何设计一个本地存储和云存储混合的存储系统也成为了一个关注的重要话题。然而构建这样的一个存储服务仍然具备不少的挑战：首先是性能和成本，虽然云存储的成本更低数据可靠性更高，但其读写性能都会有严重的下滑；其次是需要开发一种有效的缓存来弥补内存空间不足的局限从而提高性能；最后是可用性的问题，需要尽可能快地从远程存储完成数据的恢复。</p><p>本文提出了一个叫RocksMash的系统，结合本地和云存储同时实现高性能和低成本，并保证一定的可用性。RocksMash包含了一个读取优先的数据布局，高效的LSM持久缓存LAP、节省空间且快速的元数据MashMeta以及并行恢复技术。RocksMash是基于RocksDb实现的，通过使用节点的NVMe SSD作为本地存储和AWS Elastic Block Store<br>(EBS) gp2作为云存储。</p><h2 id="BACKGROUND-AND-MOTIVATION"><a href="#BACKGROUND-AND-MOTIVATION" class="headerlink" title="BACKGROUND AND MOTIVATION"></a>BACKGROUND AND MOTIVATION</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>这一章主要讲述LSM的存储架构，如下图所示主要由多层的sstable组成，从L0到Ln逐层变大。</p><p><img src="https://pic.imgdb.cn/item/62bc97211d64b070663375f5.png"></p><h3 id="Motivating-Observations"><a href="#Motivating-Observations" class="headerlink" title="Motivating Observations"></a>Motivating Observations</h3><p><img src="https://pic.imgdb.cn/item/62bc97211d64b070663375fe.png"></p><ul><li>Imbalanced Cost and Performance between Local and Cloud Storage</li></ul><p>本地存储和云存储表现出不同的性能和成本，文中提到了云存储能降低八成的成本，但基于本地存储的Rocksdb其读取性能和写入吞吐量都要比云存储高出80倍和4成，因此使用本地存储和云存储的高效LSM存储变成了优先考虑的事。</p><ul><li>Similar Prefixes among Keys</li></ul><p>为了减少对云存储的读取IO，其重要的避免对sstable元数据的访问。如下图所示，在将SQL表数据映射到各种数据库中的键值存储时，这些复合键的公共前缀是很常见的。此外，LSM存储压缩操作进一步巩固了sstable中键之间的前缀相似性，因此这使得底层sstable中的键倾向于共享更长的前缀。在这种情况下，sstable中的索引键本质上是索引具有高相似性的排序字符串，并且越底层的sstable共享了更多前缀。</p><p><img src="https://pic.imgdb.cn/item/62bc97211d64b0706633760b.png"></p><ul><li>Slow Recovery</li></ul><p>由于原生WAL不会记录各层的所有数据，如果云实例失败然后走正常的恢复流程，则存储在本地存储中的从L0到Li的sstable将丢失。简单地扩展WAL以包含从L0到Li sstables的数据非常低效的，一是耗时很久，二是replay导致重复compaction进一步加大写放大，因此论文提到的做法是直接构建sstables，而不是通过WAL间接恢复。</p><h2 id="DESIGN"><a href="#DESIGN" class="headerlink" title="DESIGN"></a>DESIGN</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>RocksMash将LSM的上面i层放在更快成本更高的本地存储，其余部分则由云存储维护。由于上层比下层数据更少访问更热，LSM的顶层放在本地存储进一步降低了操作的耗时。</p><p>下图是RocksMash的存储架构，内存用仍然保留了原始的LSM MemTable，此外还使用了细粒度的LAP缓存来进一步减少对云存储的访问。LAP缓存分为MetaCache和DataCache， 由于元数据相比LSM的访问频率更高，因此MetaCache会存储云上sstable的元数据块，而DataCache则将精彩访问的数据块缓存起来。</p><p><img src="https://pic.imgdb.cn/item/62bc97211d64b0706633761f.png"></p><p>下图描述了写入和读取的请求处理，写入请求基本与常规的LSM处理流程一样，但在触发compaction的过程中，RocksMash会讲sstable的元数据插入到本地存储的LAP MetaCache中。而对于读请求，从依次从L0层到LN层搜索候选sstable，如果要读取的sstable在云存储上，则将block读到LAP DataCache以供访问。</p><p><img src="https://pic.imgdb.cn/item/62bc97211d64b07066337616.png"></p><h3 id="LSM-Aware-Persistent-Cache"><a href="#LSM-Aware-Persistent-Cache" class="headerlink" title="LSM-Aware Persistent Cache"></a>LSM-Aware Persistent Cache</h3><p>LSM存储支持使用SSD来作为持久化缓存，以此在内存不够时尽可能提高读性能。如上图所示，rocksdb的持久化缓存包含了多个缓存文件，这些文件以LRU的形式组织，一个kv对在读不到的情况会从磁盘加载到rocksdb的持久化缓存中，即append到CacheFile i+1中。如果可写的缓存文件满了，则通过LRU的策略逐出。这个实现存在一个文件，即持久化缓存并未感知compaction，比如上图，sst被压缩后kv3和kv1已经无效了，但仍存留在文件中导致大量的浪费和所需键被过早逐出。</p><p>RocksMash的解决方法是在本地存储上使用LSM-tree- aware持久化缓存LAP。LAP分为MetaCache和DataCache。MetaCache存粗云上sstable的所有元数据块，避免远程的元数据访问，MetaCache将这些元数据分组并使用哈希表去做管理。DataCache则是一种能感知compaction的缓存，用于存储热数据块来加速读请求。如上图所示，存储在DataCache的kv对被组织称LRU列表，无效的kv对则通过位图标记失效，而加载数据到DataCache则可以直接覆盖那些无效的kv对。</p><h3 id="Space-Efficient-and-Fast-Indexing-and-Filtering-MashMeta"><a href="#Space-Efficient-and-Fast-Indexing-and-Filtering-MashMeta" class="headerlink" title="Space-Efficient and Fast Indexing and Filtering (MashMeta)"></a>Space-Efficient and Fast Indexing and Filtering (MashMeta)</h3><p>这一章主要是介绍为了提高缓存空间效率，RocksMash使用了一种叫MashMeta的succinct tree的技术，从而为每一个云存储的sstable构建一个压缩前缀树。这种trie能以深度优先的一元度序列进行编码，下图就是一个样例。本质是为了以最小的空间消耗避免filter的误报，提高对每个key的精准查询速度。<br><img src="https://pic.imgdb.cn/item/62bc97671d64b0706633ce0b.png"></p><h3 id="Parallel-Recovery"><a href="#Parallel-Recovery" class="headerlink" title="Parallel Recovery"></a>Parallel Recovery</h3><p>基于RocksMash的布局设计，快速恢复对存储在鼓掌节点本地存储上的sstable访问是非常重要。这里假设云存储的sstable是可靠的，WAL文件则存储在快速且专用的云存储上，然而由于本地存储的sstable数据相对WAL来说较小，从大量WAL文件快速恢复会使得这个设计变得更加困难。</p><p>一般的恢复方法如下图所示，按顺序重做写入操作，但这种逻辑对于大型数据集的效率很低，因为WAL的每一个kv对都会写入一次，并经过memtable一步一步flush下来，产生更多的写放大。</p><p><img src="https://pic.imgdb.cn/item/62bc97671d64b0706633ce1c.png"></p><p>因此RocksMash提出通过并行化恢复L0到Li的sstables来解决效率问题，这里仍然存在两个挑战：</p><ul><li>如何并行构建sstables：因为WAL的kv对和manifest记录的有效sstable列表并不存在联系，需要漫长的compaction操作才能匹配两者；</li><li>有了上面的连接，如何确保在读操作于恢复后能读到正确的kv对时能实现最大的并行化：如果按照时间顺序去搜索WAL文件，那很有可能会读取到大量的过时kv对，但这些数据最终会被compaction删除，因此RocksMash需要提高它对sstables的扫描效率；</li></ul><p>如上图所示，RocksMash扩展了WAL以包含从L0到Li的所有sstable数据，当生成这些sstable时，RocksMash会讲更改记录到MANIFESY，然后将其key列表同时记录到WAL以便能连接到sstable。因为除非所有新的sstable都成功持久化，否则compaction不会被认为是完整的，所以RocksMash可以批处理新sstable的键列表以减少写入它们的I&#x2F;O数量。这样，RocksMash可以确保安全回滚到与 MANIFEST 和key列表一致的最新状态，并且不会丢失任何有效数据，</p><p>逆时间顺序搜索：RocksMash为每个需要恢复sstable分配一个worker，每个worker由新到旧搜索WAL，这种方式能LSM存储的读取路径首先读到key的最新版本。通过逆顺序搜索WAL，RocksMash进一步减少WAL的读取数量。</p><p>并行构建sstable：当实例节点发生故障时，RocksMash使用新实例将恢复原来存储在故障实例中的L0-Li sstables。 RocksMash读取MANIFEST和扩展的WAL以获取最后一致状态的sstable文件列表。 RocksMash然后借助key列表从扩展的WAL中并行重建这些sstables。下图是具体的步骤</p><p><img src="https://pic.imgdb.cn/item/62bc97671d64b0706633ce26.png"></p><ul><li>第一步是读取WAL：RocksMash将L0-Li sstable的key列表和所有WAL文件拉到新的实例节点。</li><li>第二步是查找key- values：在获取到有效sstable的键列表后，RocksMash为每个sstable分配一个worker来扫描 WAL 文件并获取其列表中的key。扫描应该从较新的WAL文件开始到较旧的文件，以确保worker首先读到最新的key。至于存在key range重叠的L0 stables，则控制L0旧sstable对应的worker不允许超过新sstable对应worker去扫描；</li><li>第三步是构建stables：worker在获得所有键值对后开始构建sstable。恢复的sstable会在重建后加载到RocksMash；</li><li>第四步是构建MemTable：恢复sstables 后，RocksMash开始恢复MemTable和不可变的MemTables。 RocksMash利用最新的L0 sstable的信息来减少要重放的WAL记录的大小。由于每个更改都按时间顺序记录到WAL中，因此最新L0 sstable中的最新key表示一个分水岭的位置，其中早于该时间的key保存在sstables中，而较新的key仍在MemTable或不可变 MemTables中。</li></ul><p>经过以上步骤，RocksMash就完成了将故障实例的内存和本地存储中的数据恢复到最新的一致状态，并大幅度减少了恢复重建的时间。</p><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>本文集中研究了集成云存储会如何影响LSM存储的性能，由于存储成本的问题，云存储的性能上限严重影响了读取性能，因此提出了一种快速高效的LSM存储RocksMash。RocksMash可以将LSM tree在本地存储和云存储之间进行拆分，从而在成本和性能之间达到平衡。此外，为了减少元数据的内存占用并提高读取性能，RocksMash使用能感知LSM结构的持久缓存，并以节省空间的方式（succinct tree）存储元数据，并使用了能感知压缩情况的缓存来存储热数据块。为了实现快速并行的数据恢复，RocksMash还扩展了WAL的使用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Building-a-Fast-and-Efficient-LSM-tree-Store-by-Integrating-Local-Storage-with-Cloud-Storage&quot;&gt;&lt;a href=&quot;#Building-a-Fast-and-Efficien</summary>
      
    
    
    
    
    <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>The RocksDB Experience</title>
    <link href="http://yoursite.com/2022/04/03/The-RocksDB-Experience/"/>
    <id>http://yoursite.com/2022/04/03/The-RocksDB-Experience/</id>
    <published>2022-04-03T15:13:41.000Z</published>
    <updated>2022-04-03T15:14:16.333Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evolution-of-Development-Priorities-in-Key-value-Stores-Serving-Large-scale-Applications-The-RocksDB-Experience"><a href="#Evolution-of-Development-Priorities-in-Key-value-Stores-Serving-Large-scale-Applications-The-RocksDB-Experience" class="headerlink" title="Evolution of Development Priorities in Key-value Stores Serving Large-scale Applications: The RocksDB Experience"></a>Evolution of Development Priorities in Key-value Stores Serving Large-scale Applications: The RocksDB Experience</h1><blockquote><p>RocksDB是一个针对大规模分布式系统的kv存储，并针对SSD的特性进行了优化。本文描述了在过去八年中RocksDB的开发过程。这种演变是硬件趋势的结果，也跟许多组织大规模使用RocksDB有关。本文描述了 RocksDB的资源优化目标是如何以及为什么从写放大转变到到空间放大，再到CPU利用率。通过一系列的使用经验总结得知，资源分配需要跨不同的RocksDB实例进行管理，数据格式需要保持向后和向前兼容以允许新软件推出，并且需要对数据库复制和备份的进行支持。故障处理的经验则可以归结为需要在系统的每一层及早发现数据损坏错误。</p></blockquote><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>RocksDB起源于LevelDB，并针对SSD的一些feature做了定制优化，同时也会被设计成可以嵌入到其他系统的KV库，每个RocksDB节点只管理自己的数据，相互之间没有交互。RocksDB在Database、Stream processing、Logging&#x2F;queuing services、Index services等领域都有一定的应用。使用RocksDB作为底下的存储引擎也有利有弊，弊端在于每个系统都需要在RocksDB之上处理好一系列复杂的failover recover的操作，优势则是可以复用很多基础功能。下图是针对不同应用特性的总结和负载相关的信息：</p><p><img src="https://pic.imgdb.cn/item/6249b959239250f7c5aa09a9.png"></p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>基于flash特性的考虑，RocksDB在设计上采用了flash友好的数据结构，并对当前的硬件进行了优化。</p><h3 id="Embedded-storage-on-flash-based-SSDs"><a href="#Embedded-storage-on-flash-based-SSDs" class="headerlink" title="Embedded storage on flash based SSDs"></a>Embedded storage on flash based SSDs</h3><p>由于flash-based SSD的发展，提供了更高吞吐和更低延迟的设备，使得软件设计需要考虑如何充分使用期全部功能。SSD提供了数十万IOPS和数百MB的带宽，在很多场景下使得性能瓶颈从IO转移到了网络，这就增加了对嵌入式kv存储引擎的需求，RocksDB就是如此应运而生的。</p><h3 id="RocksDB-architecture"><a href="#RocksDB-architecture" class="headerlink" title="RocksDB architecture"></a>RocksDB architecture</h3><p>LSM tree是RocksDB存储数据的主要数据结构。</p><ul><li>Writes：数据的写入首先会写入到一个叫MemTable的内存buffer和磁盘的WAL上，其中MemTable基于跳表实现，WAL则是用于故障恢复；持续的写入会使得MemTable到达设定的大小阈值，这时MemTable和WAL就会变成immutable，并分配一个新的MemTable和WAL接收新的写入；Immutable MemTable则会flush到磁盘的SSTable中，并丢弃旧的MemTable和WAL。每个SSTable按需排列数据，并划分为大小相同的block，每个SSTable也会有一个索引块，其中的索引项则是通过二分查找定位到数据块。</li><li>Read：数据读取则是依次从MemTable开始，逐层查找更高层次的SSTable。其中会使用bloomfilter优化读取；</li><li>Compaction：如下图所示，L0的SSTable是由MemTable flush后创建的，而更高级的SSTable则是通过compaction诞生的。每个层级的sstable大小都会收到配置参数的限制，超过阈值则会与更高一级的重叠SSTable进行合并，合并后deleted和overwritten的数据会被删除，因此能在一定程度上提高读性能和空间效率。这里的compaction可以并行化，提高压缩效率。L0的SSTable会有重叠的key范围，因为其覆盖了完整的sorted run（一个sorted run内部的数据必定有序）。其后的每个层级只包含一个sorted run。其中RocksDB支持不同类型的compacttion：Leveled Compaction如下图所示，每层一个最多一个sorted run；Tiered Compaction与Cassandra、HBase的策略类似，允许一层存在多个sorted run，每次往更高层级压缩时，不会读取高层次的数据；FIFO Compaction：当DB大小达到某个阈值限制时直接丢弃以前的文件并只执行轻量压缩；使用不同的compaction策略，RocksDB能被配置为读友好或者写友好。</li></ul><p><img src="https://pic.imgdb.cn/item/6249b959239250f7c5aa0997.png"></p><h2 id="Evolution-of-resource-optimization-targets"><a href="#Evolution-of-resource-optimization-targets" class="headerlink" title="Evolution of resource optimization targets"></a>Evolution of resource optimization targets</h2><p>本章描写了RocksDB的资源优化目标：从写放大到空间放大，再到CPU利用率。</p><h3 id="Write-amplification"><a href="#Write-amplification" class="headerlink" title="Write amplification"></a>Write amplification</h3><p>一开始RocksDB主要关注如何节省flash- based SSD的擦拭次数来减少内部的写放大。这里的写放大主要集中在两个方面，一是SSD本身的写放大（1.1-3），二是软件自身的写放大，有时可能会到100，比如小于100Bytes的修改也需要写入一个4K&#x2F;16K的页。</p><p>Leveled Compaction在RocksDB中通常会引入10-30的写放大，虽然Tiered Compaction能将写放大降低到4-10，但这也降低了写性能。如下图所示：</p><p><img src="https://pic.imgdb.cn/item/6249b959239250f7c5aa099e.png"></p><p>RocksDB通常会选择一种自适应的压缩方法，写频率高时减少写放大，写频率低时更积极地压缩。</p><h3 id="Space-amplification"><a href="#Space-amplification" class="headerlink" title="Space amplification"></a>Space amplification</h3><p>再到后面考虑到flash的写周期和开销都没有做限制，Space amplification的问题会更加明显。RocksDB的策略是Dynamic Leveled Compaction，即LSM中每个Level的大小会根据最后一个Level的实际大小自动调整，而不是更死板的设置每个level的大小。</p><h3 id="CPU-utilization"><a href="#CPU-utilization" class="headerlink" title="CPU utilization"></a>CPU utilization</h3><p>有一些观点会认为性能瓶颈已经从SSD转移到了CPU上，但该论文并不认为这是一个问题，一是因为只有少部分应用会被SSD提供的IOPS所限制，大多数应用还是受限于空间；二是任何具有高端CPU的服务器都有足够的计算能力来饱和一个高端SSD。但同时论文也认为减少CPU开销是一个很重要的优化目标，这是因为减少Space amplification已经做得差不多了，而提高CPU利用率可以尽量优化成本。论文提到的关于CPU优化的努力包括引入prefix bloom filter和其他bf的改进。</p><h3 id="Adapting-to-newer-technologies"><a href="#Adapting-to-newer-technologies" class="headerlink" title="Adapting to newer technologies"></a>Adapting to newer technologies</h3><p>论文提到了RocksDB发展过程中采用或考虑过的一些新技术。</p><ul><li>SSD架构的改进，比如改进查询延迟和节省flash擦拭周期；</li><li>存储内计算，但RocksDB要适应存储内计算会是一个挑战，因为可能需要对整个软件堆栈进行API更改；</li><li>远程存储，这是一个当前比较重要的优化目标。考虑到网络技术的发展能提供更多的远程IO，并且远程存储可以同时充分利用CPU和SSD资源。目前正在通过尝试合并和并行IO来解决长尾延迟，论文提到了已经对RocksDB进行改造，以处理瞬时故障，将QoS要求传递给底层系统，并报告分析信息。然而这一块可以做的还有很多；</li><li>SCM也是一个很有前途的技术，但需要考虑以下几点：将SCM作为DRAM的延伸，需要考虑如何利用好混合的DRAM和SCM，提供更理想的数据结构，并且如果利用了持久性会带来哪些开销；使用SCM作为主要存储部分，但RocksDB往往会受到空间或CPU的瓶颈而不是IO，似乎效果也不会很明显；为WAL使用SCM，然而对于WAL只需要在转移到SSD之前的一小块staging区域，这里的成本是否理想；</li></ul><h3 id="Main-Data-Structure-Revisited"><a href="#Main-Data-Structure-Revisited" class="headerlink" title="Main Data Structure Revisited"></a>Main Data Structure Revisited</h3><p>目前得出的结论是LSM tree仍然是更适合SSD的存储引擎，用CPU或DRAM交换SSD也不是一个普遍的现象。当然RocksDB在发展过程中也不断收到用户关于降低写放大的需求，当对象很大时，可以通过key value分离来减少写放大，RocksDB也添加了BlobDB作为相关的支持。</p><h2 id="Lessons-on-serving-large-scale-systems"><a href="#Lessons-on-serving-large-scale-systems" class="headerlink" title="Lessons on serving large-scale systems"></a>Lessons on serving large-scale systems</h2><p>RocksDB作为需求各异的大型分布式系统的基石，也需要在包括资源管理、WAL处理、文件批量删除、数据格式兼容和配置管理方面进行改进。</p><h3 id="Resource-management"><a href="#Resource-management" class="headerlink" title="Resource management"></a>Resource management</h3><p>大规模的分布式系统通常需要对数据进行分片，每个分片分布在多个存储节点上，并且需要限制大小，因为考虑到需要进行备份和负载均衡，同时也会由于原子性做一些一致性的保证。一个RocksDB通常只会对应一个分片，因此一个节点上可能会运行着多个RocksDB实例，这就对资源管理有一定的影响。共享主机的资源，则需要对资源进行全局的管理，以确保能公平使用，这里的资源管理包括：write buffer和block cache的内存使用、compaction的IO带宽、compaction的线程数、磁盘使用情况、文件的删除比例等。为了确保单个实例不会占用过多资源，RocksDB为每种类型的资源都提供了若干个资源控制器，同时也会支持一些优先级策略。</p><p>另一个运行多实例的教训是，大量的非池化的线程可能会给CPU带来过载，因此论文建议若需要使用一个可能会休眠或等待某个条件的线程来执行一些工作，最好使用一个线程池，便于限制线程的大小和资源使用。</p><p>考虑到每个分片只有局部的信息，当RocksDB运行在单进程里时，全局的资源管理将会变得更加困难。这里可以采用两种策略：为每个实例配置更为谨慎的资源使用；让实例之间共享资源使用信息，并进行相应的调整。</p><h3 id="WAL-treatment"><a href="#WAL-treatment" class="headerlink" title="WAL treatment"></a>WAL treatment</h3><p>传统数据库倾向于在每个写操作都强制执行WAl，而大规模的分布式系统为了可用性和性能，往往会通过各种一致性保证来做到这一点，比如从其他正常副本重建本机的损坏副本，或者通过自己的复制日志（比如分布式系统通常会有的Paxos日志等），这种情况下一般不需要RocksDB的WAL。</p><p>考虑到这点，RocksDB提供了不同的WAL操作模式：同步WAL写，缓冲WAL写，不进行WAL写。</p><h3 id="Rate-limited-file-deletions"><a href="#Rate-limited-file-deletions" class="headerlink" title="Rate-limited file deletions"></a>Rate-limited file deletions</h3><p>RocksDB通过文件系统与底层设备交互，每当删除文件时，可以发送TRIM命令到SSD，这会改善SSD性能和Flash性能，但也会引起性能问题。这是因为TRIM会更新地址映射，并且还需要将修改写入带flash的FTL日志，这会进一步触发SSD内部的GC，并进一步对IO延迟造成负面影响。因此RocksDB引入了文件删除的速度限制，以防止多个文件同时被删除。</p><h3 id="Data-format-compatibility"><a href="#Data-format-compatibility" class="headerlink" title="Data format compatibility"></a>Data format compatibility</h3><p>这一章主要讲RocksDB需要确保数据格式的前后兼容，这样在一个大规模的分布式应用中，可以逐步灰度设计各个实例，也方便在出现问题时进行回退。</p><h3 id="Managing-configurations"><a href="#Managing-configurations" class="headerlink" title="Managing configurations"></a>Managing configurations</h3><p>关于配置管理的方面，RocksDB具备高度的可配置性，但原来继承自leveldb的方法将参数选项嵌入到代码中，也容易造成以下两个问题：参数配置通常与存储在磁盘的文件强绑定，当使用一个选项创建的数据文件可能无法被新配置了另一个选项的RocksDB实例打开，这会带来兼容性的问题；当RocksDB更新时可能会改变默认配置参数。为了解决这些问题，RocksDB引入了对随数据库存储选项文件的可选支持，同时也加入了一些验证和迁移工具来对不同配置进行兼容。</p><p>高度的可配置性带来的另一个问题就是配置参数过多，难以针对不同类型的应用进行选择。因此RocksdDB在改进开箱即用性能和简化配置上花费了大量的功夫，并尽可能提供自适应的配置。</p><h3 id="Replication-and-backup-support"><a href="#Replication-and-backup-support" class="headerlink" title="Replication and backup support"></a>Replication and backup support</h3><p>RocksDB作为一个单节点存储引擎，分布式系统通常会有复制和备份的需求，因此RocksDB也需要对此进行支持，基于已有副本生成新副本的方式有两种：扫源副本的所有数据，然后将其写入目标副本；直接物理复制sstable和其他需要的文件来建立一个新的副本。</p><h2 id="Lessons-on-failure-handling"><a href="#Lessons-on-failure-handling" class="headerlink" title="Lessons on failure handling"></a>Lessons on failure handling</h2><p>基于生产环境的经验，论文提到了三个关于故障处理的教训：尽早检测到数据损坏；数据完整性保护需要覆盖到整个系统；错误需要以不同的方式进行处理。</p><h3 id="Frequency-of-silent-corruptions"><a href="#Frequency-of-silent-corruptions" class="headerlink" title="Frequency of silent corruptions"></a>Frequency of silent corruptions</h3><p>基于性能的考虑，RocksDB不实用SSD的数据保护（如DIF&#x2F;DIX），而是通过RocksDB的block checksum进行校验和检测。CPU&#x2F;内存的损坏很少发生，并且也很难量化。使用RocksDB的应用程序经常会进行数据的一致性检查，比较副本间的一致性，捕获的错误可能是由RocksDB或client引入的。另外传输数据时也会发生数据损坏，比如处理网络故障时，底层存储系统的错误会在一段时间后显现出来，每PB级别的物理数据大约会有17个checksum miss。</p><h3 id="Multi-layer-protection"><a href="#Multi-layer-protection" class="headerlink" title="Multi-layer protection"></a>Multi-layer protection</h3><p>越快检查到数据损坏可以尽可能减少停机时间和数据丢失，对于分布式系统而言，当检测到checksum miss的时候可以丢弃损坏的副本，然后用正常的副本做替换。如下所示，RocksDB进行了多层的文件数据校验，并同时使用了块校验和文件校验。</p><p><img src="https://pic.imgdb.cn/item/6249b959239250f7c5aa09a2.png"></p><ul><li>Block integrity：每个SSTable块和WAL segment都附加了一个checksum，数据创建时生成，每次读取数据都要验证；</li><li>File integrity：如SSTable等文件内容有可能会在传输操作期间被破坏，为了解决这个问题，SSTable会在元数据的文件条目中记录checksum，在传输时使用SSTable文件进行验证。但WAL文件没有使用这种保护方式；</li><li>Handoff integrity：早期检测写损坏的一种技术时对将要写入底层文件系统的数据生成一个Handoff checksum，并与数据一起传递下去，由底层进行验证，这样就可以很好地保护WAL的写操作，但很少有本地文件系统支持这一点。另一方面使用远程存储的时候，写API可以更改为接收checksum，并hook住存储服务的内部ECC，这样RocksDB就可以基于现有的WAL segment checksum上使用校验和组合技术来高效地计算write handoff checksum，进一步降低在读时检测到损坏的可能性；</li></ul><h3 id="End-to-end-protection"><a href="#End-to-end-protection" class="headerlink" title="End-to-end protection"></a>End-to-end protection</h3><p>到目前为止，文件IO层以上的数据还是缺乏保护的，例如MemTable中的数据和块缓存，此级别的损坏数据无法检测，会进一步暴露给用户，也会因为flush和compaction导致损坏被持久化。为了解决这个问题，RocksDB正在为每个kv对实现checksum。</p><h3 id="Severity-based-error-handling"><a href="#Severity-based-error-handling" class="headerlink" title="Severity-based error handling"></a>Severity-based error handling</h3><p>RocksDB遇到的大多数错误都是底层存储系统返回的错误，比如文件系统只读，访问远程存储的网络问题等。因此RocksDB的目标是在错误不能在本地恢复的情况下中断RocksDB的操作，比如网络错误时进行周期性重试恢复，而不需要用户手动重启。</p><h2 id="Lessons-on-the-key-value-interface"><a href="#Lessons-on-the-key-value-interface" class="headerlink" title="Lessons on the key-value interface"></a>Lessons on the key-value interface</h2><p>KV接口的用途非常广泛，几乎所有的存储系统都可以通过一个KV接口的API来提供存储服务，其是通用的，key和value都是变长的字节数组，由应用程序决定如何打包信息和如何进行编码解码。</p><p>尽管如此，KV接口还是存在一些限制，比如构建并发控制等还是有需要考虑的东西。RocksDB已经意识到了数据版本控制的重要性，也规划支持合适的功能，如MVCC和point in time read等。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文很像是一个综述，主要是把RocksDB相关的一些问题和开发设计思路进行了总结，包括写放大、空间放大，CPU利用率提高、remote storage等，同时也提到了开发过程中的一些反思，包括数据格式的前后兼容，如何支持数据库的备份和复制，简单化配置系统等。总而言之，这是一篇总结类的工业论文，非常适合进一步学习RocksDB。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Evolution-of-Development-Priorities-in-Key-value-Stores-Serving-Large-scale-Applications-The-RocksDB-Experience&quot;&gt;&lt;a href=&quot;#Evolution</summary>
      
    
    
    
    
    <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>The Dataflow Model</title>
    <link href="http://yoursite.com/2022/03/05/The-Dataflow-Model/"/>
    <id>http://yoursite.com/2022/03/05/The-Dataflow-Model/</id>
    <published>2022-03-05T09:41:19.000Z</published>
    <updated>2022-03-05T09:41:59.459Z</updated>
    
    <content type="html"><![CDATA[<h1 id="The-Dataflow-Model-A-Practical-Approach-to-Balancing-Correctness-Latency-and-Cost-in-Massive-Scale-Unbounded-Out-of-Order-Data-Processing"><a href="#The-Dataflow-Model-A-Practical-Approach-to-Balancing-Correctness-Latency-and-Cost-in-Massive-Scale-Unbounded-Out-of-Order-Data-Processing" class="headerlink" title="The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing"></a>The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing</h1><blockquote><p>无界、无序、大规模的数据集在日常业务中越来越普遍，并且，这些数据的消费端也发展出更复杂的需求，例如事件时间排序和数据本身特征的窗口化等，以及对消费速度有了更高的要求。本文介绍了Google在数据流模型的核心设计原则</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>现代的数据处理是一个非常复杂且发展蓬勃的领域，从MapReduce到SQL社区内大量关于流的工作如窗口、查询系统等，再到Spark Streaming、Storm等低延迟领域的发展。然而，现有的模型和系统在许多常见场景中仍然存在不足。批处理系统会遇到导入系统带来的延迟问题，而对于流处理系统，要么缺乏大规模的容错机制，要么缺乏提供exactly-once语义的能力影响数据准确性，又或者缺少窗口所必需的时间原语等等。Lambda架构可以满足很多需求，但由于必须和构建两套系统就会带来简单性的不足。论文提出的观点是，不再关注执行引擎决定系统语义的主流思维，而是通过考虑批处理，微批处理和流传输系统之间潜在的差异（即延迟和资源成本）来选择执行引擎。</p><p>本文提出了一个简单统一的模型概念：</p><ul><li>允许计算event-time的有序结果，并根据数据本身的特征在无边界，无序的数据源上进行窗口聚合处理，在准确性，延迟和成本三者之间平衡；</li><li>拆分四个跨维度相关的管道实现：<ul><li>what：正在计算什么结果；</li><li>where：事件发生时在哪里计算；</li><li>when：在哪个处理时间内进行物化；</li><li>how：前期结果如何与后续改进相关联；</li></ul></li><li>将数据处理的逻辑概念与底层的物理实现分开，允许基于准确性，延迟和成本的考虑来选择批处理，微批处理或流引擎；</li></ul><p>具体而言可以分为以下几个部分：</p><ul><li><strong>A windowing model</strong>：支持未对齐的event-time窗口；</li><li><strong>A triggering model</strong>：将事件处理的运行时特征与输出次数坐绑定；</li><li><strong>incremental processing model</strong>：将数据更新整合到前面的window model和trigger model中；</li><li><strong>Scalable implementations</strong>：基于MillWheel流式引擎和Flume批处理引擎实现了Google cloud Dataflow的SDK；</li><li><strong>core principles</strong>：模型设计的核心原则；</li></ul><h3 id="Unbounded-x2F-Bounded-vs-Streaming-x2F-Batch"><a href="#Unbounded-x2F-Bounded-vs-Streaming-x2F-Batch" class="headerlink" title="Unbounded&#x2F;Bounded vs Streaming&#x2F;Batch"></a>Unbounded&#x2F;Bounded vs Streaming&#x2F;Batch</h3><p>Dataflow统一用Bounded&#x2F;Unbounded Dataset来描述有限&#x2F;无限数据集，而Streaming&#x2F;Batch则用来特指某些执行引擎。</p><h3 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h3><p>Window即窗口化，是指将无限的数据集切分为有限的数据片以便进行聚合处理。对于无边的数据集，有些操作如<strong>aggregation</strong>，<strong>outer join</strong>，<strong>time-bounded</strong>都需要窗口。窗口一般是基于时间的，但也有些系统支持基于记录数的窗口，这可以理解为是逻辑时间，其中的元素按顺序依次增加逻辑时间戳。</p><p>窗口模型主要由三种主要分为以下三种：</p><p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f01d.png"></p><ul><li>Fixed Window：这是按固定窗口的大小定义的，比如说小时窗口或天窗口，通常是对齐窗口，每个窗口都包含了对应时间段范围内的所有数据，可以看到的是每个窗口之间没有重叠；</li><li>Sliding Window：这是根据窗口大小和滑动周期大小来定义的，比如说小时窗口，每一分钟滑动一次，通常情况滑动周期会比窗口更小，滑动窗口一般也是对齐的，如上图的五个滑动窗口实际上都包含了对三个键的处理；Fixed Window可以认为是窗口大小等于滑动周期大小的Sliding Window；</li><li>Session Window：这种类型的窗口会在数据的子集上捕捉一段时间内的活动，属于非对齐窗口，比如上图的窗口2只包含key 1，窗口3则只包含key2；</li></ul><h3 id="Time-Domain"><a href="#Time-Domain" class="headerlink" title="Time Domain"></a>Time Domain</h3><p>在流式处理中有两个关于时间的概念需要重点关注：</p><ul><li>Event Time：事件本身实际发生的时间，系统时钟时间在事件发生时的记录；</li><li>Processing Time：事件在系统中被处理的时间；</li></ul><p>在数据处理过程中，由于系统自身收到的影响如通信延迟，调度算法，处理时长，管道中间数据序列化等，会导致上述两个值之间存在一定的差值，诸如punctuations或watermarks之类的全局进度指标都提供了一种可视化这种差值的好方法，本文则是使用了一种类似MillWheel的水位标记，这是一个时间戳，用来表示小于这个时间戳的数据已经完全被系统处理了。理想情况下，这两个时间的差值应该为0，即事件一旦发生则马上做处理，如下图所示。但实际上，由于前面提到的原因，水位标记会偏离真实时间，这是非常正常的现象。</p><p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f020.png"></p><h2 id="DATAFLOW-MODEL"><a href="#DATAFLOW-MODEL" class="headerlink" title="DATAFLOW MODEL"></a>DATAFLOW MODEL</h2><p>接下来将讨论Dataflow的正式模型。</p><h3 id="Core-Primitives"><a href="#Core-Primitives" class="headerlink" title="Core Primitives"></a>Core Primitives</h3><p>首先从经典的批处理模型开始，Dataflow把所有的数据都抽象成键值对，并提出了两个核心的数据转换操作：</p><ul><li>ParDo：对每个输入元素都用一个用户自定义函数进行处理，生成零个活多个的输出元素，如下图所示：</li></ul><p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f026.png"></p><ul><li>GroupByKey：根据键值将元素重新分组，作为一个聚合操作，由于需要收集到所有需要的数据，需要结合窗口化一起使用；</li></ul><p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f02c.png"></p><h3 id="Windowing"><a href="#Windowing" class="headerlink" title="Windowing"></a>Windowing</h3><p>支持GroupByKey的系统通常会将其重新定义为GroupByKeyAndWindow，Dataflow在这里的主要贡献是支持未对齐窗口，其底层的优化则是通过下面两部来实现：</p><ul><li>**Set<Window> AssignWindows(T datum)**：将元素复制给若干个窗口；</li><li>**Set<Window> MergeWindows(Set<Window> windows)**：窗口合并；</li></ul><p>为了在本地支持事件时间的窗口，这里不再是传递简单的键值对，而是传递(key, value, eventtime, window)4元组。元素进入系统时会带有事件时间的时间戳，并且在最初会分配一个磨人的全局窗口。</p><h4 id="Window-Assignment"><a href="#Window-Assignment" class="headerlink" title="Window Assignment"></a>Window Assignment</h4><p>窗口赋值就是指将数据拷贝到对应的窗口。下图就是一个窗口大小为2分，滑动窗口间隔为1分钟的例子。</p><p><img src="https://pic.imgdb.cn/item/62232fcf5baa1a80ab64f033.png"></p><h4 id="Window-Merge"><a href="#Window-Merge" class="headerlink" title="Window Merge"></a>Window Merge</h4><p>窗口合并是GroupByKeyAndWindow操作的一部分，具体来说这是一个由五部分组成的复合操作：DropTimestamps、GroupByKey、MergeWindows、GroupAlsoByWindow和ExpandToElements，其具体的含义可以参考下图：</p><p><img src="https://pic.imgdb.cn/item/62232ff85baa1a80ab65130d.png"></p><h3 id="Triggers-amp-Incremental-Processing"><a href="#Triggers-amp-Incremental-Processing" class="headerlink" title="Triggers &amp; Incremental Processing"></a>Triggers &amp; Incremental Processing</h3><p>能够构建未对齐的事件时间窗口是一种进步，但仍面临两个问题：</p><ul><li>为了与其他流式系统保持兼容，需要提供基于processing time和基于tuple的窗口；</li><li>由于事件发生时间是无序的，数据可能会慢一步到来，我们需要何时才能将窗口的结果数据发往下游；</li></ul><p>论文这里主要讨论第二种情况，就是如何保证窗口数据的完整性。最原始的想法是使用某种全局事件时间戳，比如watermark来处理，这里可以立即为一个阈值，但watermark设计的过长过短对数据处理的准确性会有一定的影响，过短会导致水位标记到达后仍有记录到达，过长则可能会使得迟到的数据影响到整个数据处理管道的watermark。</p><p>Dataflow的处理方法是用一种叫Trigger的机制，这种机制是受信号激励从而触发GroupByKeyAndWindow执行并输出结果的机制，相对于窗口是决定哪些event time数据被分到一组进行聚合操作，Trigger更多是决定在什么处理时间窗口的结果会被输出。Trigger提供了三种不同的模式来控制不同计算结果之间是如何关联的：</p><ul><li>Discarding：窗口数据的Trigger之后直接丢弃；</li><li>Accumulating：窗口的结果数据在Trigger之后持久化下来，用以支持后面的数据更新；</li><li>Accumulating &amp; Retracting：在第二种基础上增加了回撤结果，即窗口再次Trigger时会将上次的结果做回撤，然后将新的结果作为正常数据下发。</li></ul><h2 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h2><p>无边界的数据时数据处理的未来，有边界的数据本身也是由无边界的对应部分所包含的，并且处理数据的消费者进化的越来越快，因此需要更强大的架构支持例如事件时间顺序和未对齐的窗口等。Dataflow模型把数据处理的逻辑划分了以下几个部分：计算什么、在哪个event time范围内计算、在什么处理时间点触发计算，如果用新的结果修正之前的处理结果，这使得整个数据处理逻辑变得更加透明清晰。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;The-Dataflow-Model-A-Practical-Approach-to-Balancing-Correctness-Latency-and-Cost-in-Massive-Scale-Unbounded-Out-of-Order-Data-Proce</summary>
      
    
    
    
    
    <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing</title>
    <link href="http://yoursite.com/2022/01/14/Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing/"/>
    <id>http://yoursite.com/2022/01/14/Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing/</id>
    <published>2022-01-14T15:44:53.000Z</published>
    <updated>2022-01-14T15:46:07.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing"><a href="#Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing" class="headerlink" title="Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing"></a>Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing</h1><blockquote><p>Mesa是一个可扩展的分析型数据仓库，可用于存储Google广告业务相关的数据，能满足多数据中心、高可用、近实时等特性。</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>Mesa满足了以下的要求：</p><ul><li>原子更新；</li><li>一致性和正确性；</li><li>高可用性，能承受整个数据中心或者园区挂掉；</li><li>近实时更新的吞吐，支持全量和增量的更新；</li><li>查询性能，以低延时满足高吞吐；</li><li>可扩展性，数据读写性能都能随着集群规模实现线性增长；</li><li>支持热更新表的schema；</li></ul><p>Mesa充分利用了Google内部的基础设施，包括Colossus、BigTable和BigTable等。Mesa主要是分shard存储，批量更新，每个更新分配一个版本号来满足MVCC，同时基于Paxos算法做元数据的一致性管理。</p><h2 id="MESA-STORAGE-SUBSYSTEM"><a href="#MESA-STORAGE-SUBSYSTEM" class="headerlink" title="MESA STORAGE SUBSYSTEM"></a>MESA STORAGE SUBSYSTEM</h2><p>Mesa存储的数据是多维度的，其中维度属性称为keys，指标属性则是values。</p><h3 id="The-Data-Model"><a href="#The-Data-Model" class="headerlink" title="The Data Model"></a>The Data Model</h3><p>Mesa的数据是以表的概念来维护的，每个表都会有一个schema。通常来说，表会有key空间和对应的value空间，分别就是前面提到的维度和对应的指标。同时values列需要定一个聚合函数，例如SUM、MIN、MAX等。聚合函数必须满足结合律，可选择性满足交换律。Mesa往往会存储上千张表，每个表有几百个列。如下图就是三张经典的Mesa表，其中表C是表B的物化视图：</p><p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a812f9.png"></p><h3 id="Updates-and-Queries"><a href="#Updates-and-Queries" class="headerlink" title="Updates and Queries"></a>Updates and Queries</h3><p>为了实现高吞吐，Mesa是通过批量的方式进行更新，上游数据源以分钟级的频率批量更新。每一个更新都有对应的一个递增版本号，更新以串行的方式进行。因此对于Mesa的查询需要提供版本号和Predicate，这样就可以在[0, n]的版本key集合里做filter。</p><p>如下图所示，表A和B经历了两个更小的Batch，其中表C是B的物化视图，B的更新，对应的物化视图都能保持和原表的一致性原子更新。</p><p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a812fd.png"></p><p>另外对于一些数据会滚的需求，Mesa支持negative facts，则对一些指标列做减法，以最终一致的实现来实现会滚。</p><h3 id="Versioned-Data-Management"><a href="#Versioned-Data-Management" class="headerlink" title="Versioned Data Management"></a>Versioned Data Management</h3><p>数据版本在Mesa的读写过程中扮演着非常重要的角色，但也存在一些问题，一是存储成本会变高，二是无论查询还是更新，聚合所有版本的代价也会比较高。</p><p>Mesa的做法就是提出Delta的概念，每一个Delta包含的是不重复key的数据，并使用[V1, V2]表示版本号，V1小于或等于V2，其中的数据就是在版本号V1和V2之间更新的key，value则是这些更新操作聚合后的。另外由于每个delta内部数据都是有序的，因此合并可以以线性的时间完成。</p><p>Mesa对于delta的结构分成了三层：每次批量写入都会当作是一个单例delta合并到Mesa，单例delta的V1等于V2。因为Mesa对于指标列都会有相关的聚合函数，因此delta[V1, V2]和delta[V2, V3]可以通过合并key、聚合value的方式合并成delta[V1, V3]，这些就是cumulative delta。另外还存在一个base delta，设它的版本号为[0, B], 其中B大于或等于0，生成base delta，后续任意一个[V1, V2]只要满足0 &lt;&#x3D; V1 &lt;&#x3D; V2 &lt;&#x3D; B就可以被删除（唯一的删除条件），这就是异步的base compaction。这也是为什么Mesa仅仅支持一段时间以内的所有版本，比如24小时内的，因为更早的版本已经可能被聚合到base delta里。</p><p>如下图，对于版本n的查询，就可以通过聚合这三层的delta来返回值。任何时刻都存在一个基本delta[0, B], 一系列的累积delta[B+1, B+10], [B+1, B+20],[B+1, B+30],…以及B以后的所有单例delta。这样的好处就是，对于某个版本n的查询，可以方便通过查询cumulative delta来减少IO开销。举个例子，如果查询版本91的数据，在没有cumulative delta的情况下，就需要查询61-91这32次的delta；如果存在cumulative delta，则只需要一次base的查询，61-90的cumulative，外加91这一个单例delta。</p><p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a81301.png"></p><p>Mesa的解决思路总结起来就是：及时删除过期数据，merge小文件。</p><h3 id="Physical-Data-and-Index-Formats"><a href="#Physical-Data-and-Index-Formats" class="headerlink" title="Physical Data and Index Formats"></a>Physical Data and Index Formats</h3><p>Mesa的delta，无论哪些类型其存储格式都是一样的，并且都是immutable。因此Mesa关于物理数据的存储主要关注空间成本以及查询性能。关于存储，论文没介绍太多细节，主要是分成index files和data files。Mesa将delta的行按顺序存储在大小受限的data files中，若干行的数据会组织成一个row blocks，每个row blocks则是按照column进行存储（提高压缩率，并且因为查询性能的问题优先考虑解压效率高的）。Index files存储的则是row blocks第一个key的固定长度前缀以及对应row blocks在data files中的偏移量，然后就可以将index files加载进内存通过二分查找去读数据。</p><h2 id="MESA-SYSTEM-ARCHITECTURE"><a href="#MESA-SYSTEM-ARCHITECTURE" class="headerlink" title="MESA SYSTEM ARCHITECTURE"></a>MESA SYSTEM ARCHITECTURE</h2><h3 id="Single-Datacenter-Instance"><a href="#Single-Datacenter-Instance" class="headerlink" title="Single Datacenter Instance"></a>Single Datacenter Instance</h3><p>每一个Mesa实例都包含了两个子系统：update&#x2F;maintenance系统和querying系统，这些子系统可以独立扩展。元数据信息存在BigTable里，数据文件则是存在Google的Colossus。</p><h4 id="Update-x2F-Maintenance-Subsystem"><a href="#Update-x2F-Maintenance-Subsystem" class="headerlink" title="Update&#x2F;Maintenance Subsystem"></a>Update&#x2F;Maintenance Subsystem</h4><p>update and maintenance子系统主要负责以下的操作：加载更新数据、执行表压缩，在线进行Schama修改，检查表的checksum等。这些操作都是由下图的controller&#x2F;worker framework完成的。</p><p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a81307.png"></p><p>controller可以看作是表元数据的cache，同时负责worker的调度，worker队列的管理。controller不做任何数据相关的工作，只负责调度和元数据管理。元数据存在BigTable上，controller会去订阅表的更新，同时也是元数据唯一的修改方。</p><p>worker组件则是负责在每个Mesa实例中的具体数据操作工作，不同的worker是隔离的，有自己独立的职责，有一组独立的worker池。空闲的worker会定期轮询controller，请求对应类型的任务，收到工作任务后，会去验证并处理，最后则在任务完成后通知controller。图中还有一个Garbage Collector，主要是负责清理因为worker执行失败而留下的中间状态数据。worker与controller之间通过租约的方式，防止挂掉的worker一直霸占着任务，同时controller也只接受分配的worker的任务结果，确保执行安全。</p><h4 id="Query-Subsystem"><a href="#Query-Subsystem" class="headerlink" title="Query Subsystem"></a>Query Subsystem</h4><p>Mesa的query subsystem由下图的查询服务器组成，这些服务器接收用户查询、从元数据和数据集中查找对应内容、执行相关聚合、并在返回client前将数据转换到client协议格式。</p><p><img src="https://pic.imgdb.cn/item/61e197c22ab3f51d91a81315.png"></p><p>Mesa的客户端对不同的请求有不同要求，有些要求低延时、有些要求高吞吐。因此Mesa会通过标记工作负载和隔离、优先级等机制来满足不同的延迟和吞吐量要求。</p><p>出于性能的考虑，相似的数据查询往往会路由到某个查询服务器的子集（比如同一张表的查询都由某一批查询服务器负责），这样做的好处就是，查询服务器可以通过预取和缓存的方式来提供低延时保证。在启动时，每个查询服务器都会向Global Locator Service注册所主动缓存的表列表，client可以通过这个列表来决定如何路由。</p><h3 id="Multi-Datacenter-Deployment"><a href="#Multi-Datacenter-Deployment" class="headerlink" title="Multi-Datacenter Deployment"></a>Multi-Datacenter Deployment</h3><p>Mesa可以多中心部署，每个数据中心是相互隔离的的，有一份独立的数据。</p><h4 id="Consistent-Update-Mechanism"><a href="#Consistent-Update-Mechanism" class="headerlink" title="Consistent Update Mechanism"></a>Consistent Update Mechanism</h4><p>Mesa中的表都是多版本的，上游系统每几分钟生成一批更新数据以供Mesa合并，如下图，Mesa入了committer组件引入了committer组件。committer为每个更新批次分配一个新版本号，并将与更新相关的所有元数据发布到版本数据库（a globally replicated and consistent data store build on top of the Paxos consensus algorithm），应该就是spanner或者F1。committer是无状态的，可以多中心部署。</p><p><img src="https://pic.imgdb.cn/item/61e198002ab3f51d91a84866.png"></p><p>Mesa的controller会监听版本数据库，以检测新更新，然后将相应的工作分配给Update workers，并将更新结果报告回版本数据库。然后committer会检查verion提交的一致性条件是否满足（例如Mesa表的物化视图是否已经更新完成）。当满足提交标准时，committer将在版本数据库里更新版本号。</p><p>Mesa的更新机制对性能非常友好：MVCC使得Mesa不需要在查询和更新之间无锁；所有更新数据都由各Mesa实例异步合并，元数据则基于Paxos协议同步更新。</p><h4 id="New-Mesa-Instances"><a href="#New-Mesa-Instances" class="headerlink" title="New Mesa Instances"></a>New Mesa Instances</h4><p>Mesa会使用P2P的方法，通过一个load worker去加载新的Mesa实例，可以将表从另一个Mesa实例复制到当前的表。另外这一机制也支持从损坏的表中恢复。</p><h2 id="ENHANCEMENTS"><a href="#ENHANCEMENTS" class="headerlink" title="ENHANCEMENTS"></a>ENHANCEMENTS</h2><h3 id="Query-Server-Performance-Optimizations"><a href="#Query-Server-Performance-Optimizations" class="headerlink" title="Query Server Performance Optimizations"></a>Query Server Performance Optimizations</h3><p>论文中心还提到了Mesa关于查询性能的优化：</p><ul><li>delta pruning：查询服务器会检查描述每个delta包含的键范围的元数据，避免读取不必要的delta；</li><li>scan-to-seek：对于非第一个key有filter的查询，可以用scan-to-seek来优化索引，避免读取不必要的数据；</li><li>resume key：Mesa 通常以流方式将数据返回给客户端，每一次返回一个block，对于每一个block，Mesa都会附加一个resume key。如果查询超时，受影响的Mesa客户端可以透明地切换到另一个查询服务器，从resume key的地方继续查询，而不是重新执行整个查询；</li></ul><h3 id="Parallelizing-Worker-Operation"><a href="#Parallelizing-Worker-Operation" class="headerlink" title="Parallelizing Worker Operation"></a>Parallelizing Worker Operation</h3><p>Mesa利用MapReduce框架来并行化处理worker任务。</p><h3 id="Schema-Changes-in-Mesa"><a href="#Schema-Changes-in-Mesa" class="headerlink" title="Schema Changes in Mesa"></a>Schema Changes in Mesa</h3><p>Mesa用户经常需要修改schemas，一些常见的更改比如添加或者删除列、添加或删除索引等。Mesa使用两种技术来执行在线的schemas变更：</p><ul><li>拷贝固定版本的表数据并按照新的schema进行存储；</li><li>回放并更新当前版本和之前固定版本的数据；</li><li>更新元数据的schema；</li><li>直到旧schema没有查询，则删除旧的数据；</li></ul><p>但这种方法成本很高，需要临时存有两份存储资源，也需要删掉历史数据。另一种技术则是linked schema change，不再是重新灌一遍历史数据，而是对增量数据以新的schema处理。如果是新加的列，对于历史数据则以数据类型的默认值填充。但这种方法无法处理所有情况：比如删除修改列等。</p><h3 id="Mitigating-Data-Corruption-Problems"><a href="#Mitigating-Data-Corruption-Problems" class="headerlink" title="Mitigating Data Corruption Problems"></a>Mitigating Data Corruption Problems</h3><p>这一章主要是讲Mesa在容错方面的努力，通过在线写入的检查和定期离线的全量检查来确认数据不会有损坏。当出现数据损坏时，Mesa实例会自动从另一个实例重新加载该表的正常副本。如果都损坏了，则从备份中恢复旧版本的表并重放更新。</p><h2 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h2><p>本文介绍了Google近实时、可扩展的数据仓库的设计与实现——Mesa系统。Mesa支持在线查询和批量更新，同时提供强大的一致性和事务正确性保证，并且在数据模型上有非常创新的理论设计。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing&quot;&gt;&lt;a href=&quot;#Mesa-Geo-Replicated-Near-Real-Time-Scalable-Data-Warehousing</summary>
      
    
    
    
    
    <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Napa: Powering Scalable Data Warehousing with Robust Query Performance at Google</title>
    <link href="http://yoursite.com/2022/01/14/Napa-Powering-Scalable-Data-Warehousing-with-Robust-Query-Performance-at-Google/"/>
    <id>http://yoursite.com/2022/01/14/Napa-Powering-Scalable-Data-Warehousing-with-Robust-Query-Performance-at-Google/</id>
    <published>2022-01-14T15:43:55.000Z</published>
    <updated>2022-01-14T15:46:33.576Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Napa-Powering-Scalable-Data-Warehousing-with-Robust-ery-Performance-at-Google"><a href="#Napa-Powering-Scalable-Data-Warehousing-with-Robust-ery-Performance-at-Google" class="headerlink" title="Napa: Powering Scalable Data Warehousing with Robust !ery Performance at Google"></a>Napa: Powering Scalable Data Warehousing with Robust !ery Performance at Google</h1><blockquote><p>Google产生的大量应用数据，需要有一个可扩展性强、响应时间短、可用性高和强一致性的存储提供服务。Napa就是Google用来满足这些要求的系统，其核心的数据就是使用一致的物化视图。</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>Napa其实是用来替代Google另一款OLAP产品Mesa的，并且从Mesa迁移了PB级的历史数据，与Mesa相比，Napa的设计需求更为广泛：一是更稳定的查询性能，期望毫秒级的低查询延迟和无论集群负载如何延迟相对稳定；二是更灵活，支持用户根据需求在查询性能、数据新鲜度等方面进行取舍；三是数据写入高吞吐，在海量更新负载下，Napa实现了一个LSM范式的分布式表和视图维护框架。</p><h2 id="NAPA’S-DESIGN-CONSTRAINTS"><a href="#NAPA’S-DESIGN-CONSTRAINTS" class="headerlink" title="NAPA’S DESIGN CONSTRAINTS"></a>NAPA’S DESIGN CONSTRAINTS</h2><p>这一章主要是将Napa的设计过程中考虑了哪些关键目标，最理想的情况当然是以尽可能低的成本实现最高的查询性能和最高的数据新鲜度。数据新鲜度是通过将数据添加到表的时间点到可用于查询的时间点之间的维度来衡量的，而成本则主要是数据处理和存储所需要的机器资源成本。</p><h3 id="Clients-Need-Flexibility"><a href="#Clients-Need-Flexibility" class="headerlink" title="Clients Need Flexibility"></a>Clients Need Flexibility</h3><p>这一个前面也说了，Napa需要支持client在下面三个因素（数据新鲜度、资源成本和查询性能）之间进行权衡选择。</p><p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcad.png"></p><p>一个重要的考虑点在于数据写入和当前存储是否耦合，如果将新的数据和当前存储结合起来，这意味着数据的写入只有只有在应用到表及其所有视图后才会提交，这是Mesa的设计，但有一个比较大的缺点，即额外添加视图会导致数据写入变慢，这种设计虽然提高了查询速度，但牺牲了一定的数据新鲜度。另一种就是视图的生成可以作为查询的一部分选择性地完成，维护视图的异步模型会在一定程度上影响表及视图之间的一致性。</p><p>因此Napa需要提供一个灵活的client，以便随时调整系统对于这些目标的需求。</p><h2 id="DESIGN-CHOICES-MADE-BY-NAPA"><a href="#DESIGN-CHOICES-MADE-BY-NAPA" class="headerlink" title="DESIGN CHOICES MADE BY NAPA"></a>DESIGN CHOICES MADE BY NAPA</h2><p>Napa中的一个关键设计选择是依靠物化视图来实现更高的查询性能。Napa的主要架构由下图三个组件组成：</p><p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcb1.png"></p><ul><li>Ingestion framework：负责将更新数据提交到表中，这些更新在Napa称为deltas。</li><li>Storage framework：将更新应用于表及其视图，Napa以LSM的结构维护deltas，每个表都以一个deltas的集合方式表示。Delta不断合并以形成更大的Delta，即Compaction。视图维护层通过应用相应的SQL转换将表Delta转换为视图Delta，存储层还负责定期压缩表和视图。</li><li>Query serving：负责应对客户端的查询，系统在查询时执行表（或视图）的必要Delta合并。存储子系统处理更新的速度越快，查询时需要合并的增量就越少。</li></ul><p>Napa将数据写入、视图维护与查询处理进行解耦，允许client根据自己的需要在数据新鲜度、性能和成本之间进行权衡。</p><h3 id="Providing-Flexibility-to-Clients"><a href="#Providing-Flexibility-to-Clients" class="headerlink" title="Providing Flexibility to Clients"></a>Providing Flexibility to Clients</h3><p>Napa会将client在数据新鲜度、查询性能和成本方面的要求转换为对应的数据库配置、比如视图数量、写入任务的quota限制、查询期间的最大打开delta数量等等，这是一个动态但易于理解的数据库状态指示器。</p><p>为此Napa引入了称为可查询时间戳 (QT) 的概念。QT是数据新鲜度的直接指标，因为[Now() - QT]表示数据延迟，客户端可以查询到 QT 时间戳之前的所有数据。下文就介绍了三类Napa client以及系统如何使用QT来调整Napa。</p><ul><li>Tradeoff freshness：牺牲数据新鲜度即意味着Napa的QT推进会取决于查询执行时保持适度数量的视图和更少的deltas来合并。同时为了保持低资源成本，Napa的执行框架会使用更少的worker和资源进行视图维护。</li><li>Tradeoff query performance：牺牲查询性能即意味着Napa的QT推进会取决于更少的视图，但在查询执行时需要合并的Deltas可能相对较多。由于每个表和视图的Deltas较多，查询性能较低。简单来说就是将视图维护和压缩的成本转移到查询；</li><li>Tradeoff costs：以更高的成本提供良好的查询性能和数据新鲜度。Napa的QT推进取决于多个视图，并且合并时的Deltas数量非常少，从而确保更短的查询执行时间。并且花费更多的资源来增加worker来满足要求。</li></ul><h3 id="Data-Availability"><a href="#Data-Availability" class="headerlink" title="Data Availability"></a>Data Availability</h3><p>关于数据可用性，Napa的做法是将数据和元数据操作解耦，在数据中心的每个副本上异步执行数据操作，并定期在Spanner使用元数据操作以确保副本彼此保持同步，确保副本之间的一致性，将同步和异步的模式进行组合。</p><h2 id="SYSTEM-ARCHITECTURE"><a href="#SYSTEM-ARCHITECTURE" class="headerlink" title="SYSTEM ARCHITECTURE"></a>SYSTEM ARCHITECTURE</h2><p>如下图，Napa的架构由数据平面和控制平面组成，数据平面则是由上述的数据写入、存储和查询服务组成的。控制平面则负责协调各个子系统之间的工作，多个数据中心同步和元数据事务。</p><p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcbb.png"></p><p>Napa大量依赖了Google现有的基础设施，比如Napa的表数据就是存储在Colossus文件系统上、Spanner负责元数据管理和系统状态存储、F1 Query则用来做查询服务和大规模的数据处理。</p><p>Napa客户端使用ETL管道将数据写入表中，数据摄取框架可以承受高达数十GB&#x2F;s压缩数据的负载。同时，Napa的存储组件通过压缩表和视图的delta，创建更大的delta，从而减少在线查询期间的合并操作。查询服务则在运行时进行必须要缓存、预取和合并Delta的操作，提供低延迟和稳定的查询，前者是通过查询定向到预先计算的物化视图实现的，后者则是通过控制compaction和一些系列IO优化技术来减少长尾。</p><p>Napa依赖视图作为获得良好查询性能的主要机制，包含物化视图的Napa表按其主键进行排序、索引和Range分区。与现有数据库更多倾向于使用Scan的查询处理不同，Napa考虑到负载和查询性能等要求，最终选择了按索引key查找，当然这可能会带来热点和负载均衡等问题，但论文中并没有详细讲述如何应对的。</p><p>Napa的控制平面则调度compaction和视图的更新任务，以控制表的deltas保持在一个配置值。如前所述，QT构成了数据新鲜度的基础，查询系统使用它来提供稳定的查询性能。如果数据相对落后了（写入慢了），Napa会通过牺牲查询性能来将写入速度拉回到一个合理的配置值。</p><h2 id="INGESTING-TRILLIONS-OF-ROWS"><a href="#INGESTING-TRILLIONS-OF-ROWS" class="headerlink" title="INGESTING TRILLIONS OF ROWS"></a>INGESTING TRILLIONS OF ROWS</h2><p>数据写入框架的目标是允许写入管道在没有显着开销的情况下将大量数据写到Napa中。该子系统的目标是接受数据、执行最少的处理并使其持久化，而不考虑后续视图维护的速度。如下图所示，所有写入的行都会被分配一个元数据时间戳用于排序，然后在满足其他持久性条件（例如复制）后标记为已提交。其次，该子系统允许通过配置增加或减少批处理、聚合复制等worker数量的方式来控制机器成本。</p><p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcc5.png"></p><p>客户端将要写入的数据发送到所有的Napa副本，该框架会产生写入优化的delta，但不能立即用于查询。这些就是unqueryable delta，需要在可查询前将它们进行压缩。</p><h2 id="QUERYABLE-TIMESTAMP"><a href="#QUERYABLE-TIMESTAMP" class="headerlink" title="QUERYABLE TIMESTAMP"></a>QUERYABLE TIMESTAMP</h2><p>表的可查询时间戳（QT）是一个时间戳，它表示可以查询的数据的新鲜度。假设QT(table) &#x3D; X，则client可以查询到时间 X之前的所有数据，并且时间X之后摄入的数据属于不可查询数据的一部分，即一个表的新鲜度是[Now() - QT]。一旦在(Y-X)时间范围内写入的数据经过优化后满足了查询性能要求，QT的值将从X变成Y。因此client可以使用Napa的配置选项和这个指标来调整新鲜度、查询性能和成本。如果client想要高查询性能和低成本，但可以牺牲数据新鲜度，则系统优先使用较少的机器资源进行视图维护以降低成本，QT的推进则变得缓慢。</p><p>确保更高查询性能的一个重要方法是优化读取的基础数据并确保视图可用以加快查询速度。Napa中的表是其所有delta files的集合，每个delta对应于在一个时间窗口内为该表接收的更新，如下图所示，不可查询的delta对应于最近写入的数据。每个delta都按key排序、范围分区，并具有类似本地B树的索引，在查询时合并需要读取的delta。</p><p><img src="https://pic.imgdb.cn/item/61e199b32ab3f51d91a9bcd1.png"></p><p>一般client查询都会有严格的延迟限制，通常会在查询期间对打开和合并读的最多delta做限制，通常会限制不超过数十个delta，并且会根据对数据的查询性能做自动设置。通过保持给定数据库的增量数量接近恒定，Napa能够提供强大且稳定的查询性能。</p><p>QT本质上会依赖于后台操作如compaction和视图维护的进度。数据库的QT是数据库中所有表的QT的最小值。 另外，QT还用于为client提供跨所有Napa副本的一致数据视图。每个副本都有一个局部QT，它基于本地副本中数据的新鲜程度的得出的。 QT的全局值则是根据查询服务可用性要求从本地QT值计算出来的，实际就是一个基于Quorum机制的选择。</p><h2 id="MAINTAINING-VIEWS-AT-SCALE"><a href="#MAINTAINING-VIEWS-AT-SCALE" class="headerlink" title="MAINTAINING VIEWS AT SCALE"></a>MAINTAINING VIEWS AT SCALE</h2><p>Napa的存储子系统负责维护视图和delta的compaction，其目标是能有效地管理数千个表和视图，并且数据量通常是PB级的。但视图维护遇到的一个困难是，基表的更新转换为视图更新的过程中，由于key空间的映射带来的数据倾斜问题。另外就是由于数据库的QT往往会受到长尾视图的影响，因此Napa利用了下面的技术来解决视图维护的问题：</p><ul><li>Use of F1 Query as a “data pump”：使用Google的F1 Query作为data pump来压缩表和维护视图，视图维护使用查询优化器，它可以在备选计划中做出很好的选择。</li><li>Replanning to avoid data skews：如果检测到数据倾斜，系统可以即时做新的维护计划，提高视图的维护速度。</li><li>Intelligence in the loop：需要具备能力，根据历史负载选择数据中心执行任务，根据进度主动终止任务，并发任务执行以限制长尾。</li></ul><h3 id="Query-optimizations-challenges-in-View-Maintenance"><a href="#Query-optimizations-challenges-in-View-Maintenance" class="headerlink" title="Query optimizations challenges in View Maintenance"></a>Query optimizations challenges in View Maintenance</h3><p>Napa的视图维护过程有效地利用了输入中的数据属性，由于处理的数据量和特定数据属性使大规模查询处理变得更复杂，视图更新必须解决独特的优化挑战。数据属性的一个例子就是要更新的视图相对于基表的排序顺序。如果基于视图排序顺序对视图key重新排序，这可能会是一个非常昂贵的方案，而尽可能维护输入顺序反而是低成本。因此如下图所示，根据维护它们的成本，存在三类视图：</p><p><img src="https://pic.imgdb.cn/item/61e199e72ab3f51d91a9e423.png"></p><ul><li>与基表共享前缀的视图：例如基表具有键(A, B, C)，而视图位于(A, B)上，框架只需要对公共key前缀（A、B）对输入进行聚类则可以完全避免重新排序；</li><li>仅有部分公共前缀的视图：例如基表具有key(A, B, C, D)，而视图位于(A, B, D)。即使在这种情况下，我们也可以通过在(A,B)上对输入基表进行聚类，然后在D上对每个唯一的(A,B)组进行排序来部分利用输入顺序；</li><li>不共享任意前缀的视图：这一类的优化机会就很少了；</li></ul><h3 id="Mechanics-of-Compaction"><a href="#Mechanics-of-Compaction" class="headerlink" title="Mechanics of Compaction"></a>Mechanics of Compaction</h3><p>压缩将多个输入delta组合成一个输出delta，异步压缩能有效减少查询时的合并负载，但对于高频写入的表来说，压缩是非常高的代价，会使得数据新鲜度变低。由于delta files是单独排序的，因此压缩本质上是归并排序。合并过程会在各种输入之间分配固定的内存预算，因此输入越多，每个输入流的内存越小，并且当其中一个输入被消耗完时，归并过程将停止。因此大型的归并会使得发送归并终止的可能性越高。</p><h2 id="ROBUST-QUERY-SERVING-PERFORMANCE"><a href="#ROBUST-QUERY-SERVING-PERFORMANCE" class="headerlink" title="ROBUST QUERY SERVING PERFORMANCE"></a>ROBUST QUERY SERVING PERFORMANCE</h2><p>维护较低的查询性能是业务使用的关键，本节主要讲Napa如果使用QT、视图等一系列技术实现强大的查询性能。</p><p>这里主要讲了几点：</p><ol><li>尽量使用视图来响应查询，filter和聚合下推，尽可能减少读取的数据量。并且依赖并发加快查询。</li><li>通过分布式的cache层和数据预取来进一步减少IO。</li><li>并行化的IO调用往往会受到长尾延迟的影响，为了对抗这种延迟，Napa会尽可能合并小IO，并基于下面两种技术去实现：Lazy merging across deltas，当有N个子查询和M个delta时，尽可能避免delta server中的交叉delta合并，每个server只处理一个delta，将NxM的并行IO组合减少到N个冰箱IO；Size-based disk layout，根据delta大小选择不同的磁盘文件布局，PAX适用于小delta，将所有列访问组合到一个IO，大delta则使用按列layout，提高扫描查询的IO效率。</li><li>由于Napa建立在多种Google基础设施上，对于这种复杂且相互依赖的系统所有可变性来源，无法完全消除长尾延迟。Napa的做法是采用对冲机制，则记录延迟的状态，尽可能在状态不稳定时将请求发送到不同的服务器&#x2F;数据中心，以容忍一定的长尾延迟。</li></ol><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>Napa是Google新一代的OLAP系统，具备高可用性，并且与其他数据库依赖列存储、并行、压缩等方式不同，其强依赖物化视图提供了更好的查询能力，同时允许client自行在数据新鲜度、查询延迟和成本之间进行权衡选择，总体而言是一篇非常具备启发性的OLAP论文。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Napa-Powering-Scalable-Data-Warehousing-with-Robust-ery-Performance-at-Google&quot;&gt;&lt;a href=&quot;#Napa-Powering-Scalable-Data-Warehousing-wit</summary>
      
    
    
    
    
    <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum：A Hybrid Database for Transactional and Analytical Workloads</title>
    <link href="http://yoursite.com/2021/12/05/Greenplum%EF%BC%9AA-Hybrid-Database-for-Transactional-and-Analytical-Workloads/"/>
    <id>http://yoursite.com/2021/12/05/Greenplum%EF%BC%9AA-Hybrid-Database-for-Transactional-and-Analytical-Workloads/</id>
    <published>2021-12-05T08:11:28.000Z</published>
    <updated>2021-12-05T08:12:04.307Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Greenplum：A-Hybrid-Database-for-Transactional-and-Analytical-Workloads"><a href="#Greenplum：A-Hybrid-Database-for-Transactional-and-Analytical-Workloads" class="headerlink" title="Greenplum：A Hybrid Database for Transactional and Analytical Workloads"></a>Greenplum：A Hybrid Database for Transactional and Analytical Workloads</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Greenplum是一个老牌的、基于MPP架构的数据仓库系统，主打的OLAP功能，采用了share nothing和sharding的架构，能够处理PB级别的数据。Greenplum存在一个固定的coordinator节点，负责与client交互，查询计划的生成与分布式执行、事务的管理等，是一个比较重要的节点。其他segment节点（单机Postgres）则存储数据与本地查询和事务。为了增强Greenplum的OLTP能力，往HTAP的方向发展，论文中提到了Greenplum对以下几个方面进行了增强：</p><ul><li>事务能力的增强：加入了全局的死锁检查器，避免了过去由于严格的锁机制导致并发能力不够的问题；对于单个segment server上的事务，由2PC转变为1PC；</li><li>提高点查询的性能；</li><li>引入了资源管理组，避免OLAP与OLTP两种查询负载相互影响；</li></ul><p>从某种角度来说，Greenplum演变成HTAP数据库的路径与大多数数据库不一样，它是在传统的OLAP数据库上加强了OLTP功能的支持。</p><h2 id="GreenPlum’s-MPP-Architecture"><a href="#GreenPlum’s-MPP-Architecture" class="headerlink" title="GreenPlum’s MPP Architecture"></a>GreenPlum’s MPP Architecture</h2><p>GreenPlum是一个典型的MPP架构，集群由多个worker segments组成，每个segment都是一个增强版的PostgreSQL。下图就是GreenPlum的架构。</p><p><img src="https://pic.imgdb.cn/item/61ac73a32ab3f51d91ec3a9e.png"></p><h3 id="Roles-and-Responsibility-of-Segments"><a href="#Roles-and-Responsibility-of-Segments" class="headerlink" title="Roles and Responsibility of Segments"></a>Roles and Responsibility of Segments</h3><p>一个GreenPlum集群由许多个跨主机的segments组成，其中会有一个segment是coordinator，其他的统称为segment server。coordinator是一个比较重的节点，负责接收client请求，生成分布式查询计划，根据计划生成分布式进程，将计划分配到每个进程，收集结果，返回到client。</p><p>segment server则是存储数据，从coordinator接收查询计划。为了提高可用性，segment也可以配置镜像节点，不参与计算，但会接收WAL并回放日志。</p><p>作为一个share nothing的架构，GreenPlum中每个segment都会有自己的共享内存和数据目录。</p><h3 id="Distributed-Plan-and-Distributed-Executor"><a href="#Distributed-Plan-and-Distributed-Executor" class="headerlink" title="Distributed Plan and Distributed Executor"></a>Distributed Plan and Distributed Executor</h3><p>由于是share nothing的架构，当两个表需要进行join时，通常需要检查不同的segment server的元组是否满足条件，免不了需要在segment server移动数据。GreenPlum引入了一种叫Motion的算子来实现移动。Motion算子会通过网络来接发数据，Motion算子将查询计划切成不同的slice，在slice之间会做数据的分发，每个slice的执行都由一组特定的worker负责，这组进程就是gang。coordinator将查询计划分配个跨集群的进程组，不同的segment server生成不同的进程，都有相关的上下文信息。</p><p>如下图，顶部是一个join的分布式计划，下方则是在两个segment server集群的执行过程。在segment server上有两个slice，一个slice会扫描class表并通过redistributed Motion发送元组，两个slice则是从Motion节点接收元组，并扫描Student表执行hash join，将结果发送至顶部的coordinator。</p><p><img src="https://pic.imgdb.cn/item/61ac73a42ab3f51d91ec3ab9.png"></p><h3 id="Distributed-Transaction-Management"><a href="#Distributed-Transaction-Management" class="headerlink" title="Distributed Transaction Management"></a>Distributed Transaction Management</h3><p>Greenplum通过分布式快照和2PC来确保ACID属性，在单个segment节点上，则是Postgres原生的事务机制。</p><h3 id="Hybrid-Storage-and-Optimizer"><a href="#Hybrid-Storage-and-Optimizer" class="headerlink" title="Hybrid Storage and Optimizer"></a>Hybrid Storage and Optimizer</h3><p>Greenplum支持三种表类型：PostgreSQL原生的heap表，行存储；还有就是两种新加入的，Append Optimized的行存储和列存储。AO表更有利于批量IO而不是heap表的随机访问模式，因此更适合AP的工作负载。特别是AO column表，可以用不同的压缩算法对不同的列进行压缩。Greenplum的查询引擎不敢直表的存储类型，同一个查询可以join不同的表类型。</p><p>表可以按用户指定的key和分区策略（list、Range）进行分区，其中每个分区可以是heap、AO-row、AO-column、甚至是外部表（比如AWS的S3）。以下图的销售表为例，每个分区由日期范围定义，从老到新分别是外部表、heap表和AO-column表。</p><p><img src="https://pic.imgdb.cn/item/61ac73a42ab3f51d91ec3ac2.png"></p><p>至于优化器、Greenplum也提供两种选择（不是自适应的），分别是适合执行时间长的Orca和适合短查询Postgres原生的优化器。</p><h2 id="OBJECT-LOCK-OPTIMIZATION"><a href="#OBJECT-LOCK-OPTIMIZATION" class="headerlink" title="OBJECT LOCK OPTIMIZATION"></a>OBJECT LOCK OPTIMIZATION</h2><p>这一节主要讲的是锁优化，这是Greenplum增强OLTP性能的关键，着眼于解决分布式系统的全局死锁。</p><h3 id="Locks-in-Greenplum"><a href="#Locks-in-Greenplum" class="headerlink" title="Locks in Greenplum"></a>Locks in Greenplum</h3><p>Greenplum有三种锁：spin锁、LW锁和对象锁。前两种用于保护读写共享内存的临界区，并遵循某些规则来避免死锁。这里主要关注的是操作表、元组或事务等数据库对象时的对象锁。</p><p>其锁级别如下，level越高，并发控制粒度更严格。</p><p><img src="https://pic.imgdb.cn/item/61ac73a42ab3f51d91ec3ad4.png"></p><h3 id="Global-Deadlock-Issue"><a href="#Global-Deadlock-Issue" class="headerlink" title="Global Deadlock Issue"></a>Global Deadlock Issue</h3><p>在Greenplum中处理全局死锁时，DML语句的锁定级别非常重要：在分析阶段，事务会对表上锁；在执行阶段，则是用tuplelock。由于Greenplum会夸与多个segment server执行锁，很难避免全局死锁。如下图，在segment server0上，事务B等待事务A，而在segment server1上，事务A等待事务B。但本地的PostgreSQL却没有发现本地死锁。</p><p><img src="https://pic.imgdb.cn/item/61ac73a42ab3f51d91ec3acc.png"></p><p>更复杂的例子如下，包括协调者在内的所有segment server都导致了全局死锁。</p><p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec643e.png"></p><p>在旧版本的Greenplum中，会在coordinator分析阶段，用X模式锁定目标表。因此对于执行写操作的事务来说，它们是以串行的方式运行，而且即便是更小不同元组也会串行运行，降低了OLTP的性能。</p><h3 id="Global-Deadlock-Detection-Algorithm"><a href="#Global-Deadlock-Detection-Algorithm" class="headerlink" title="Global Deadlock Detection Algorithm"></a>Global Deadlock Detection Algorithm</h3><p>新版的Greenplum的全局死锁检查方法（GDD）如下：会在coordinator起一个守护进程，然后该进程会定期收集每个segment server上的Wait-for图，并检查是否发送全局死锁，然后用预定的策略终止全局的死锁。（比如终止最新的事务）</p><p>对于全局Wait-for图来说，事务就是顶点，其中输出边是顶点的出度，输入边数量则是入度。顶点的局部度是在某单个segment server的wait-for图中计算的值； 顶点的全局度则是在所有segment server的所有局部度的总和。另外考虑收集Wait-for图的过程是异步的，因此在下检测结论的时候，需要判断下涉及的事务是否还存在。如果有事务结束了，则放弃本轮检测，等待下一个周期。</p><p>对于Wait-for图有两种类型的边，实边是指等待的锁只有在事务结束的时候才能释放，pg中大多数对象都是这个类型。如果等待的锁无需事务结束则可以释放，比如tuple lock，则对应虚边。</p><p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec6443.png"></p><p>至于具体的检测方法如上图，是一种贪婪的算法，在每一轮的循环中：</p><ul><li>首先会将全局出度为0的顶点对应的输入边删除掉，出度为0的事务没有等待任何锁，本身可以正常结束，对它的等待也不会导致死锁，这一步可以持续直到没有这种顶点；</li><li>接着关注局部graph，接着删除局部出度为0的点所对应的输入虚边删除掉。虚边本身依赖的是tuple lock，但因为没有局部出度，因此该依赖关系可以在事务执行完之前就结束了，可以直接删掉。</li></ul><p>如果仍然存在无法消除的边，则认为死锁存在，此时再确认下之前的事务是否还存在。</p><p>下面就是一个具体的例子，上面一个图是全局和局部的Wait-for图，下图则是GDD算法执行过程。由于事务C没有全局出度，因此删除它和关联的边，变为(b)图。再看局部图，s1中A到B是一个虚边，并且B的局部出度为0，这条边也可以去掉，变成(c)图。再看全局图中B -&gt; A的边，A没有全局出度了，可以继续消除而变为(d)。</p><p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec644d.png"></p><h2 id="DISTRIBUTED-TRANSACTION-MANAGEMENT"><a href="#DISTRIBUTED-TRANSACTION-MANAGEMENT" class="headerlink" title="DISTRIBUTED TRANSACTION MANAGEMENT"></a>DISTRIBUTED TRANSACTION MANAGEMENT</h2><p>Greenplum的事务是由coordinator创建的，并将其分发到各个segment server中。coordinator为每个事务分配了一个单调递增的整数，作为分布式事务id。在每个局部segment上，根据分布式事务id也会利用原生的PG事务机制来生成本地事务标id。</p><h3 id="Distributed-Transaction-Isolation"><a href="#Distributed-Transaction-Isolation" class="headerlink" title="Distributed Transaction Isolation"></a>Distributed Transaction Isolation</h3><p>Greenplum利用了Postgres原生的snapshot机制来构建全局的分布式snapshot，可以应付分布式环境下的事务隔离。元组的可见性是由本地事务id和分布式事务id共同决定的。对于一个给定的事务，在修改一个元组时会给该元组创建一个新版本并打上局部事务ID，维护局部事务到分布式事务ID的映射。</p><p>考虑到维护本地事务ID映射分布式事务ID的开销较大，因此仅维护一个最大的分布式事务ID，和周期性地截断映射关系的元数据。</p><h3 id="One-Phase-Commit-Protocol"><a href="#One-Phase-Commit-Protocol" class="headerlink" title="One-Phase Commit Protocol"></a>One-Phase Commit Protocol</h3><p>一般来说，coordinator会使用2pc来保证事务在所有segment server上要么abort要么commit。至于这里说的一阶段优化，则是指如果事务只会修改单个segment上的数据，则可以省掉不必要的PREPARE过程。如下图，coordinator将跳过PREPARE阶段，直接把Commit命令分发至参与segment server上，节省掉一个网络来回的PREPARE消息：</p><p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec6457.png"></p><p>还有进一步的优化，最后一个Query可以和Prepared&#x2F;Commit消息合并，多节省一轮roundtrip。</p><p><img src="https://pic.imgdb.cn/item/61ac73e92ab3f51d91ec6460.png"></p><h2 id="RESOURCE-ISOLATION"><a href="#RESOURCE-ISOLATION" class="headerlink" title="RESOURCE ISOLATION"></a>RESOURCE ISOLATION</h2><p>Greenplum还引入资源组的概念，考虑到TP和AP同时运行时，AP的工作负载会对TP产生极大的影响，通常前者会消耗大量的CPU、内存和IO带宽，并对后者的查询性能产生影响。目前Greenplum主要是实现了对CPU和memory的限制。</p><p>CPU的使用隔离是利用cgroup实现的，可以通过cpu.shares来控制着CPU的使用百分比或者优先级，也可以通过cpuset.cpus来指定资源组的cpu的核数。</p><p>内存的使用隔离则是基于内存管理模块Vmemtracker实现的，但由于想要显式控制内存的使用不是那么容易，引入了三个层次来管理内存使用情况：</p><ul><li>slot memory：单个查询的内存使用情况，通过资源组的非共享内存除以并发数得出；</li><li>shared memory：在同一资源组中的查询超过slot memory时使用该层；</li><li>global shared memory：最后一层，前面的都限制不了时会使用这个配额；</li></ul><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>论文主要讲了主要面向OLAP的数据库如何转换成一个HTAP系统，考虑到OLAP的工作负载会极大影响OLTP的性能，论文提出的方法如全局死锁检测器和1PC的提交协议会显著提高OLTP性能。另外就是通过资源组的使用，限制CPU和内存，保证了在单个系统中同时运行OLTP和OLAP工作负载不会有太大的影响。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Greenplum：A-Hybrid-Database-for-Transactional-and-Analytical-Workloads&quot;&gt;&lt;a href=&quot;#Greenplum：A-Hybrid-Database-for-Transactional-and-</summary>
      
    
    
    
    
    <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>F1 Lightning HTAP as a Service</title>
    <link href="http://yoursite.com/2021/12/01/F1-Lightning-HTAP-as-a-Service/"/>
    <id>http://yoursite.com/2021/12/01/F1-Lightning-HTAP-as-a-Service/</id>
    <published>2021-11-30T16:56:22.000Z</published>
    <updated>2021-11-30T16:57:29.645Z</updated>
    
    <content type="html"><![CDATA[<h1 id="F1-Lightning-HTAP-as-a-Service"><a href="#F1-Lightning-HTAP-as-a-Service" class="headerlink" title="F1 Lightning: HTAP as a Service"></a>F1 Lightning: HTAP as a Service</h1><blockquote><p>本文介绍了F1 Lightning的设计与经验，该系统不是从头设计一个HTAP系统，而是在已有的若干个事务系统中存在着大量数据的情况，如何由一个独立的联合引擎实现对原事务系统的快速组合查询和事务操作。</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>HTAP系统的研究表明，数据所有者强烈希望能处理对同一数据集的同时处理查询和事务。已经有大量与 HTAP 系统相关的研究和开发（甚至在“HTAP”一词出现之前很久就开始了）。其中大部分工作都是在考虑：理想的 HTAP 系统应该是什么样子，以及需要哪些技术进步才能获得良好的性能。本文则考虑了另一种方法：一个松散耦合的 HTAP 架构，支持 在不同约束条件下的HTAP工作负载。</p><p>这样考虑主要是因为在Google中存在着多个事务数据存储系统来处理不同的工作负载，和与这些系统松耦合的联合查询引擎。为了避免高昂的迁移代价和提高事务存储系统的灵活性，需要一个单一的HTAP解决方案，可以跨事务存储来进行启用。</p><p>本文提出的Lightning是一种松耦合的HTAP解决方案，即HTAP as-a-service。只需将事务存储中架构中的某些表标记为“Lightning表”，Lightning即可透明地提供应用程序需要的HTAP功能。创建读取优化数据副本的所有工作，都由Lightning 及其集成处理使用的联合查询引擎来提供的，业务使用上甚至不需要知道Lightning的存在。</p><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><p>现有的HTAP系统一般分为<strong>同时承载OLTP和OLAP的单一系统</strong>和<strong>单独的OLTP和OLAP系统</strong>。再进一步的，后者又分为用于OLTP和OLAP的共享存储和解耦存储，共享存储需要对OLTP系统进行修改，以利用现有的分析查询引擎来实现OLAP。又或者是维护一个单独的、离线的ETL过程，使用松解耦的存储来实现HTAP架构，但这容易带来高延迟的问题。F1 Lightning通过与变更数据捕获 (CDC) 机制的集成以及结合内存驻留和磁盘驻留存储层次结构的使用来提高数据新鲜度。</p><p>文中介绍了几个类似系统的对比：SAP HANA异步并行表的复制机制；为行存储TiDb添加了一个列式存储和矢量化处理层的TiFlash。Oracle Database In-Memory则是结合OLTP 和 OLAP的单一系统代表，通过为活跃数据维护一个内存中列存储来加速HTAP工作负载，该列存储在事务上与持久行存储保持一致。LinkedIn Databus则是一个数据源不可知的分布式变更数据捕获系统，能将来自真实源系统的变更提供给下游应用程序，以构建用于不同目的的专用存储和索引。</p><h2 id="SYSTEM-OVERVIEW"><a href="#SYSTEM-OVERVIEW" class="headerlink" title="SYSTEM OVERVIEW"></a>SYSTEM OVERVIEW</h2><p>该HTAP系统由三个部分组成：一个OLTP，作为数据来源并公开数据变更捕获接口；F1 Query，分布式SQL查询引擎和Lightning，维护和服务读优化的副本。</p><p>在Google有两个主要的OLTP数据库：F1 DB和Spanner，前者是在后者基础上实现的。F1 Query则是一个联合查询引擎，它的查询执行依赖的是内部称为GoogleSQL（开源为 ZetaSQL）的SQL语言。F1 Query是一个联合引擎，支持许多不同的内部数据源，包括F1 DB、Spanner、Mesa、ColumnIO , 和BigTable等，用户能够编写无缝连接这些系统的查询。一般来说，F1 DB和Spanner能通过高效的面向行的存储和索引来优化OLTP工作负载，如写入和点查找查询，并且用户也会设计特定的模式以最大化写入吞吐量。虽然F1 Query能够通过多worker的方式来运行分布式分析查询，提高查询性能，但会导致大量的计算资源成本。</p><p>针对这些问题，一些团队设置了pipeline，用于将F1 DB的表复制到ColumnIO文件或其他文件格式以供进一步分析。但这种实现也有几个问题，一是导致重复存储资源，多个团队各自保留自己的相同数据副本。二是ColumnIO文件不支持就地更新，副本必须作为一个整体定期全量更新，另外就是数据新鲜度比较差。三是需要显式更改查询模式，因为两个数据源来自不同的系统，具有不同的语义。最后就是权限问题，如何保持权限上的同步。</p><p>为了解决这些问题，论文提到了一个新的HTAP系统Lightning，可将OLTP数据库中的数据复制为针对分析查询优化的格式。既可以为单个表启用Lightning，也可以为整个数据库启用Lightning。对于每个启用的表，Lightning的一个组件 Changepump会使用F1 DB或者Spanner公开的数据变更捕获机制来检测新的数据更改。Changepump会将这些变更转发到由单个Lightning服务器管理的分区，每个服务器会维护由分布式文件系统支持的LSM树。当Lightning摄取到这些变更时，它会将相关行数据转换为针对分析优化的列存储格式。</p><p><img src="https://pic.imgdb.cn/item/61a657142ab3f51d91364af2.png"></p><p>Lightning与对应OLTP数据库保持快照一致的方式来读取数据，包括F1 DB和Spanner在内的数据库都支持使用时间戳的多版本并发控制，因此提交到Lightning的每个更改也都保留其原始的提交时间戳。Lightning保证在特定时间戳下的读取将产生与在同一时间戳读取OLTP数据库相同的结果，这就为F1 Query通过重写符合条件的查询以提高性能，也就是某个查询可能会分别从Lightning和对应的OLTP数据库读取数据。</p><p>将Lightning添加到查询系统中，具备以下的有点：</p><ul><li>提高分析查询的资源效率和降低延迟；</li><li>配置简单，Lightning支持标准的流程从而更改启用HTAP；</li><li>透明的用户体验：用户无需更改SQL文本，甚至无需感知Lightning的存在；</li><li>数据一致性和数据新鲜度；</li><li>数据安全：F1 Query在重写查询以使用Lightning 之前，会以原始OLTP数据库的访问权限为准；</li><li>Lightning是一个独立的系统，无需维护其他OLTP数据库；</li><li>可扩展性，原则上，Lightning可以扩展到任何提供数据变更捕获机制的OLTP数据库上运行；</li></ul><h2 id="LIGHTNING-ARCHITECTURE"><a href="#LIGHTNING-ARCHITECTURE" class="headerlink" title="LIGHTNING ARCHITECTURE"></a>LIGHTNING ARCHITECTURE</h2><p>Lightning由以下组件组成：</p><ul><li>Data storage：数据存储层负责将更改应用到Lightning副本。它会在分布式文件系统中的创建相关的读取优化文件，提并供一个 API，允许查询引擎以与OLTP数据库相同的语义读取存储的数据，并处理后台维护相关操作，如数据压缩。</li><li>Change replication：Change replication负责跟踪OLTP数据库提供的事务日志，并对变更进行分区以分发到相关数据存储服务器。</li><li>Metadata database：相关元数据，比如数据存储状态和Change replication组件的状态会放在该数据库中。</li><li>Lightning masters：负责协调Lightning服务器之间的状态。</li></ul><h3 id="Read-semantics"><a href="#Read-semantics" class="headerlink" title="Read semantics"></a>Read semantics</h3><p>Lightning支持具有快照隔离的MVCC。所有针对Lightning特定表的查询都指定了一个读取时间戳，并且Lightning返回与该时间戳的OLTP数据库一致的数据。这一点主要是与Google内部的OLTP数据库保持一致。</p><p>由于Lightning会异步应用来自OLTP数据库的更改日志，因此在OLTP数据库中所做的更改对Lightning上的查询可见之前会存在延迟。此外，Lightning支持控制单个查询的读取时间上限。这个上限可以是无限的（即Lightning可以存储所有的更改），但实际上大多数查询都集中在最近的数据上，限制Lightning中的数据量可以节省成本。</p><p>Lightning可查询的时间戳称为安全时间戳。最大安全时间戳表示Lightning已经获取到该时间戳之前的所有更改，最小安全时间戳即表示可以查询到的最旧版本时间戳。</p><h3 id="Tables-and-deltas"><a href="#Tables-and-deltas" class="headerlink" title="Tables and deltas"></a>Tables and deltas</h3><p>Lightning将数据以表的形式组织，数据库表、索引和视图在Lightning中都被视为物理表。每个Lightning表都按range partitioning划分为一组分区。每个分区都存储在多组件的LSM树中。LSM树中的每个组件称为delta。</p><p>Delta包含其相应Lightning表的部分行版本数据，每个行版本由相应行的主键和该版本在OLTP数据库中提交时的时间戳标识。Lightning存储三种类型的版本，对应于对源数据所做的更改：</p><ul><li>Inserts：插入包含所有列的值，每行的第一个版本是一个插入。</li><li>Updates：更新至少包含一个非键列的值，并省略未修改列的值。</li><li>Deletes：删除不包含非键列的任何值，仅做墓碑。</li></ul><p>单个Delta可能包含同一键的多个版本，并且同一分区的不同Delta之间可能存在重复版本。在Delta内，部分行是由  (hkey,timestamp)唯一标识，并且为了支持对特定时间戳的快速查找，Delta按键升序、时间戳降序排序。</p><h3 id="Memory-resident-deltas"><a href="#Memory-resident-deltas" class="headerlink" title="Memory-resident deltas"></a>Memory-resident deltas</h3><p>当Lightning接收更改时，生成的部分行数据首先写入内存驻留的、按行构造的B树，类似于C-Store的写优化存储。</p><p>一个Memory-resident Delta最多有两个活跃的Writer，以及许多读取者。另外还有后台线程应用来自OLTP事务日志的新更改和定期运行垃圾收集过程。</p><p>一旦数据写入Memory-resident Delta，它就可以立即用于查询，这取决于Changepump提供的一致性协议。Memory-resident Delta并不持久，在系统故障的情况下，存储在内存中的更改可能会丢失。Lightning会通过OLTP重放的方式恢复，同时为了提高恢复速度，会将B树按原样定期checkpoint到磁盘。</p><p>当Delta变得太大时，当到达每个Delta的大小限制或者服务器内存限制时，Lightning都会将它们写入磁盘，并且会转换成为读取优化的列格式。</p><h3 id="Disk-resident-deltas"><a href="#Disk-resident-deltas" class="headerlink" title="Disk-resident deltas"></a>Disk-resident deltas</h3><p>含有Lighting数据的Disk-resident deltas被存储在读取优化的文件中，Lighting通过构建了一个具有通用接口的抽象层，允许使用许多不同的文件格式来存储Delta。文中只介绍了一种文件格式，每个增量文件存储两部分：数据部分和索引部分。数据部分以PAX风格的行列混合。索引部分包含主键上的稀疏B树索引，其中叶节点跟踪每个row bundles的键范围。索引比较小，通常在缓存中。</p><h3 id="Delta-merging"><a href="#Delta-merging" class="headerlink" title="Delta merging"></a>Delta merging</h3><p>Delta合并包含两个逻辑操作：merging和collapsing。merging对源Delta中的更改进行重复数据删除，还可能执行Schama变更。collapsing将同一key的多个版本合并为一个版本。</p><p>此过程使用的是LSM逻辑的矢量化执行，先枚举需要参与合并的Deltas，然后从每个输入Delta读取一个Block，进行多路归并，但这里需要确认在这一轮中可以collapsing的key范围。</p><p><img src="https://pic.imgdb.cn/item/61a657142ab3f51d91364af8.png"></p><p>比如这两个Deltas的合并，当前轮Lightning只能collapse小于K2的数据。K2的数据可能需要等到下一轮collapse，读入下一个Block再说。</p><h3 id="Schema-management"><a href="#Schema-management" class="headerlink" title="Schema management"></a>Schema management</h3><p>由于Lightning是复制OLTP数据库并透明地提供查询服务，因此它必须处理与OLTP数据具有相同语义的Schema演变。Lightning监控源数据库schema的更改并自动应用该更改。</p><p>为了实现这一点，Lightning使用了两级schema抽象。第一级是逻辑架构，它将OLTP的schema映射到Lightning表schema。逻辑schema包含复杂类型、比如Protocol Buffer，还有一些简单类型。对于特定的逻辑schema，Lightning会生成一个或多个物理schema。物理schema只包含基本类型，例如整数、浮点数和字符串。Lightning的文件格式接口仅在物理schema级别运行，降低工程实现难度。</p><p>如下图，逻辑schema和物理schema通过逻辑映射连接。映射指定如何将逻辑行转换为物理行，反之亦然，数据在读取期间从逻辑行转换为物理行。</p><p><img src="https://pic.imgdb.cn/item/61a657142ab3f51d91364b02.png"></p><p>映射的一个好处就是能为相同的逻辑数据实现使用不同的存储布局。例如，在存储protocol buffer时，有两种选择，一是序列化后存储，另一种则是将各字段单独存储，甚至可以同时存储，以提供极致的性能。映射还有助于仅元数据schema的更改。 Lightning可以适应许多常见的schema更改，而无需明确重写磁盘上的数据，比如增删一列。</p><p>因此，每当发生schema更改时，Lightning都会创建一个新的逻辑schema，schema更改后创建的增量会使用新的物理schema进行写入。当schema映射关系太多时，Lighting也会在compaction时将数据转换为新schema，从而减少数据转换的开销。</p><h3 id="Delta-compaction"><a href="#Delta-compaction" class="headerlink" title="Delta compaction"></a>Delta compaction</h3><p>Lightning支持四种不同的压缩方式：active compaction, minor compaction, major compaction和base compaction。</p><ul><li>active compaction：在Lighting服务器上进行，将内存delta持久化到磁盘上；</li><li>minor compaction：压缩小的和新的磁盘deltas；</li><li>major compaction：压缩大的和旧的磁盘deltas；</li><li>base compaction：将最小可查询时间戳之前的数据生成新的数据snapshot；</li></ul><p>其他三个任务都不在Lighting服务器上进行，由Lighting服务器调度，但在专门的任务worker上执行，</p><h3 id="Change-replication"><a href="#Change-replication" class="headerlink" title="Change replication"></a>Change replication</h3><p>Changepump提供跨不同OLTP源的统一接口，将各个OLTP的CDC接口细节从主要的Lightning数据存储层抽象出来，并提供了一种可扩展且有效的方式将事务变更呈现给对应的变更订阅者。作用包括了：隐藏了各个OLTP数据库的详细信息；面向事务的变更日志适应为面向分区的变更日志，一个事务可能对应不同的Lighting分区；维护事务一致性，跟踪已应用于Lightning服务器的所有更改时间戳，并发出检查点，以提高每个分区的最大安全时间戳。</p><h4 id="Subscriptions"><a href="#Subscriptions" class="headerlink" title="Subscriptions"></a>Subscriptions</h4><p>对于每个partitions，Lightning都会对Changepump做一个订阅。订阅会指定partitions的表和key范围，Changepump则负责将这些更改传送到Lightning服务器。订阅会有一个开始时间戳，Changepump只会返回在该时间戳之后提交的更改。</p><h4 id="Change-data"><a href="#Change-data" class="headerlink" title="Change data"></a>Change data</h4><p>Changepump订阅会返回两种数据：change updates和checkpoint timestamp updates。前者就是数据本身的修改，同一个key的修改按时间戳升序排序，跨行则没有严格顺序保证。后者则是用来表明该时间戳之前的修改都已经传递完成，方便Lighting服务器提交其最大的安全时间戳。</p><h4 id="Schema-changes"><a href="#Schema-changes" class="headerlink" title="Schema changes"></a>Schema changes</h4><p>Lightning使用两种机制来检测Schema的变化：lazy detection和eager detection。前者则是在收到OLTP数据库传来的修改时，如果发现它引用了没见过的架构，则会停止对该partition的更改处理，直到加载并分析了新schema。这种机制会增加处理延时。后者则是用后台线程轮询OLTP数据库以查看是否发生了任何新的schema更改。</p><h4 id="Sharding"><a href="#Sharding" class="headerlink" title="Sharding"></a>Sharding</h4><p>Changepump本身也是一个sharding的服务，单个订阅可以在内部连接到多个Changepump服务器，Changepump客户端会将多个这样的连接合并成一个单一的变更流中。与Lightning服务器不同，Changepump是根据数据量划分sharding的，后者是按全量数据量。</p><h4 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h4><p>Changepump会对增量的修改记录做缓存，好处是方便一个partition的不同副本能够共享这些数据，另外也可以加速partition的failover。</p><h4 id="Secondary-indexes-and-views"><a href="#Secondary-indexes-and-views" class="headerlink" title="Secondary indexes and views"></a>Secondary indexes and views</h4><p>Lighting还会维护二级索引和物化视图，这是与正常表同等看待的，但生成方式不一样，不是通过Changepump订阅产生，而是Lightning需要根据基础表的变更日志计算派生数据的修改。由于派生表需要按key排序，但基表可能是由不同的Lighting服务器维护的，因此Lighting的做法是将其写进BigTable。目前Lightning只能支持有限的几种物化视图。</p><h4 id="Online-repartitioning"><a href="#Online-repartitioning" class="headerlink" title="Online repartitioning"></a>Online repartitioning</h4><p>Lightning支持在线重分区，以期达到负载均衡，这里的重分区方案基本是元数据操作。Lighting的分区在达到数据大小阈值或者流量瓶颈的时候，就会进行分区分裂。在分裂分区时，Lightning就会新分区标记为非活跃。这是一个meta only的操作——新分区会共享老分区的所有delta，之后新的分区会从Changepump订阅数据并应用为新的delta，直到追上所有的新数据后才会被标记为活跃。至于老的分区，则会被标记为非活跃，然后等待所有请求都服务完后再被清理掉。由于新分区是继承自老分区的，因此读取时需要一个过滤器处理超过新分区边界范围的数据。</p><p>分区也会被触发合并，操作过程类似。</p><h3 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault tolerance"></a>Fault tolerance</h3><h4 id="Coping-with-query-failures"><a href="#Coping-with-query-failures" class="headerlink" title="Coping with query failures"></a>Coping with query failures</h4><p>对于查询失败，Lighting使用集群内和跨集群复制来处理这些故障。在一个数据中心内，Lightning为每个partition分配给多个Lightning服务器。这些Lightning服务器都从Changepump订阅相同的更改，并且它们独立维护其内存delta，但它们共享一组相同的磁盘delta和内存delta checkpoint。每个partition都只有一个主副本可以执行compaction来写入delta。当一个副本完成写入时，它会通知其他副本更新它们的LSM树。</p><p>这些副本中任一个都可以为查询提供服务，查询请求可以在这些副本之间维持负载均衡，Lighting可以为流量大的partition增加更多的副本。在服务器更新，每个partition最多同时重新启动一个副本，以保持数据的高可用性，并且重启时会尝试从其他副本加载修改。</p><p>Lighting也可以部署在多个数据中心。单个数据中心拥有自己的一套Changepump服务器、Lightning服务器等等。这些服务器独立于其他数据中心运行，在每个数据中心维护数据的完整副本，所有数据中心共享同一个元数据数据库（假设meta DB永远可用）。如果出现数据中心级别的故障，也可以将请求转给其他数据中心的相同partition。</p><h4 id="Coping-with-ingestion-failures"><a href="#Coping-with-ingestion-failures" class="headerlink" title="Coping with ingestion failures"></a>Coping with ingestion failures</h4><p>数据摄取也会出现相关的故障，比如OLTP系统的CDC出现问题或者Changepump服务器崩溃。对于前者，Changepump会自动连接另一个datacenter的OLTP系统（Google的OLTP系统本身也是跨数据中心部署的）。对于后者，Changepump是无状态的，可以连接另一个服务器继续服务。</p><p>Lightning master还会监控每个分区上所有数据中心的Changepump延迟，如果延迟过大，会重启这个数据中心的分区，使得可以从其他数据中心搬运数据过来。</p><h4 id="Table-level-failover"><a href="#Table-level-failover" class="headerlink" title="Table-level failover"></a>Table-level failover</h4><p>Lighting也会配备表级别的failover，当某张表本身有问题时（比如到达资源瓶颈，或者数据坏了），系统可以自动将查询路由回 OLTP 系统。但用户可以选择是否进行这样的容错，以防AP的查询压力突然打垮TP系统。</p><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>本文提到的Lighting是HTAP系统的另一种实现方式，与同时承载OLTP和OLAP的单一系统不同，它是直接基于多个OLTP系统透明地提高HTAP性能的能力，而无需修改OLTP系统，也无需用户数据迁移到新系统。通过与F1 Query的结合，能够无感知地重写query plan（甚至在在逻辑计划阶段对查询也是生成一样的计划，到了物理计划阶段才会决定访问OLTP和OLAP系统），向量化列式处理查询请求，下推subplan。Google大规模部署Lighting后，在不影响查询语义的情况下，计算资源和查询延迟方面都实现了数量级的收益。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;F1-Lightning-HTAP-as-a-Service&quot;&gt;&lt;a href=&quot;#F1-Lightning-HTAP-as-a-Service&quot; class=&quot;headerlink&quot; title=&quot;F1 Lightning: HTAP as a Service&quot;</summary>
      
    
    
    
    
    <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Beyond malloc efficiency to fleet efficiency</title>
    <link href="http://yoursite.com/2021/10/10/Beyond-malloc-efficiency-to-fleet-efficiency/"/>
    <id>http://yoursite.com/2021/10/10/Beyond-malloc-efficiency-to-fleet-efficiency/</id>
    <published>2021-10-10T14:58:29.000Z</published>
    <updated>2021-10-10T14:59:03.477Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Beyond-malloc-efficiency-to-fleet-efficiency"><a href="#Beyond-malloc-efficiency-to-fleet-efficiency" class="headerlink" title="Beyond malloc efficiency to fleet efficiency"></a>Beyond malloc efficiency to fleet efficiency</h1><blockquote><p>内存分配的优化可以带来巨大的成本效益。一般有两种做法，一是提高分配器的效率，减少分配器代码中的周期；一种是通过数据放置的策略来提高应用的整体性能。这篇文章主要关注的是hugepage，提出了一个叫TEMERAIRE的hugepage机制，以最大化hugapage的覆盖率和最小化碎片开销。</p></blockquote><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文主要关注的是通过提高内存分配器的大页面覆盖率来提升应用性能。Cache miss和TLB miss是现代系统中最主要的性能开销，Hugepages的出现可以显著减少TLB未命中的数量，增加大页的大小使得相同数量的TLB条目能够映射更大范围的内存，另外大页还能减少miss+填充的时间，因为页表遍历更快了。论文提出了一种TEMERAIRE 的设计，作为TCMALLOC的一部分减少应用代码的CPU开销，最大化大页覆盖、减少内存碎片。</p><h2 id="The-challenges-of-coordinating-Hugepages"><a href="#The-challenges-of-coordinating-Hugepages" class="headerlink" title="The challenges of coordinating Hugepages"></a>The challenges of coordinating Hugepages</h2><p>虚拟内存是通过TLB来将用户空间地址转换为物理地址的，TLB条目有限，如果使用默认页面大小，整个TLB覆盖的内存范围很小。现代的处理器通过在TLB中支持hugepage来加大覆盖范围，完整的大页（比如X86时2MB）仅仅占用一个条目。</p><p><img src="https://pic.imgdb.cn/item/6162ff542ab3f51d9128f6c5.png"></p><p>传统的分配器以页面大小的块来管理内存，Transparent Huge Pages机制为内核使用大页来覆盖连续页面提供了可能性。但内存的释放面临着更大的挑战，对于非大页区域来说，内存的释放要求内核使用较小的页面来表示剩余的内存。又或者大页的返回需要整个页面都变成空闲状态，这就带来内存碎片的问题。因此对于大页的设计需要在内存碎片和TLB使用率之间权衡。</p><h2 id="Overview-of-TCMALLOC"><a href="#Overview-of-TCMALLOC" class="headerlink" title="Overview of TCMALLOC"></a>Overview of TCMALLOC</h2><p>下图展示了TCMALLOC的内存组织，TCMALLOC将内存按spans分区，并且与页面大小对齐。</p><p>足够大的分配请求由仅仅包含分配对象的spans实现，至于其他的span则会包含多个相同大小的小对象，小对象的边界是256KB，小于这个的请求会四舍五入到100个大小类别中去。TCMALLOC将对象存储在一系列缓存中，如下图所示。span是从一个简单的pageheap分配的，它跟踪所有未使用的页面并进行best fit分配。pageheap还会负责定期释放内存回操作系统，减少过多的系统内存分配。</p><p>TCMALLOC会首先从local cache中分配，这里用的是per-hyperthread local cache，本地缓存会存着不同大小的空闲对象列表。如果请求不能满足要求，会路由到对应大小类别的central cache。这里有两个组件，一个是小的、快速的、互斥保护的transfer cache，另一个则是大的、互斥保护的central列表，包含了该大小对应的每一个span。当一个span的所有对象都返回到central list的一个span时，该span就会返回到pageheap。</p><p>TCMALLOC的pageheap有一些简单的内存接口：New(N)分配一个N页的span；Delete(S)向分配器返回一个新的span；Release(N)将pageheap缓存的&gt;&#x3D;N个未使用的页返回给操作系统；</p><h2 id="TEMERAIRE’s-approach"><a href="#TEMERAIRE’s-approach" class="headerlink" title="TEMERAIRE’s approach"></a>TEMERAIRE’s approach</h2><p>TEMERAIRE就是基于TCMALLOC提出的大页优化，将分配请求尽可能打包到频繁使用的大页上，同时形成完全未使用的大页面以便遍返回给操作系统。并且根据malloc的使用情况和TCMALLOC的结构制定了一些TEMERAIRE的选择原则：</p><ul><li>总内存的需求随时发生变化，并且是不可预测的；</li><li>将不是几乎为空的大页面返回给操作系统的成本是比较高的，因此其设计必须能够将分配密集地打包到高度使用的区域中；另外虽然我们的目标是专门使用大页大小的二进制，但malloc也必须支持大于单个大页的分配大小；</li><li>当一个大页完全为空时，可以选择是保留它以供将来分配内存，也可以将其返回给操作系统。适应性地做出这个决策非常重要；</li><li>很少有分配请求会直接接触pageheap，但所有分配都通过pageheap支持；</li></ul><p>TCMALLOC分配器通过委托给几个子组件来实现它的接口，如下图所示，每个组件都是根据上述原则构建的，都对最适合它处理的分配类型进行了设计。虽然TEMERAIRE的特定实现与TCMALLOC内部结构相关联，但大多数现代分配器都有类似的大页面分配支持。</p><h3 id="The-overall-algorithm"><a href="#The-overall-algorithm" class="headerlink" title="The overall algorithm"></a>The overall algorithm</h3><p>这一章主要描述各个组件，其主要目标是最小化或重用生成的slack（slack就是大页之间的多余空间区域）。所有组件的背后是HugeAllocator组件，它负责处理虚拟内存和操作系统之间的关系，为其他组件提供了可备份可传递的内存。HugeCache则是一个完全为空的大页缓存。HugeFiller则是一个存着部分填充空间的大页列表。HugeRegion则是用来应对大页边界的分配请求的。TEMERAIRE使用下图算法根据请求大小将分配决策定向到其子组件。</p><p><img src="https://pic.imgdb.cn/item/6162ff542ab3f51d9128f6d4.png"></p><p>为大页面大小的精确倍数，或那些足够大以至于slack无关紧要的分配请求由HugeCache负责；中等大小的分配由HugeCache负责（1MiB到1GiB）；例如，来自HugeCache的4.5MiB分配会产生1.5MiB的slack，这里的开销是比较高的。TEMERAIRE通过假装请求的最后一个大页面有一个单一的“前导”分配，将这个slack交由HugeFiller负责。如下图：</p><p><img src="https://pic.imgdb.cn/item/6162ff542ab3f51d9128f6dd.png"></p><p>对于某些中等分配的请求来说，其往往会产生更多的slack。如下图，例如，许多1.1MiB的分配将产生0.9MiB的每个大页的slack。当检测到这种模式时，HugeRegion分配器会跨越大页边界进行分配，以最大限度地减少这种开销。</p><p><img src="https://pic.imgdb.cn/item/6162ff542ab3f51d9128f6eb.png"></p><p>小请求(&lt;&#x3D; 1MB的)始终由HugeFiller提供服务。对于1MB和大页之间的分配，TEMERAIRE会评估了几个选项：</p><ul><li>如果有足够的可用空间，就会尽量使用HugeFiller；</li><li>如果HugeFiller不能满足请求，接下来会考虑HugeRegion；如果已分配的HugeRegion能满足要求，TEMERAIRE就会使用它。如果不存在满足要求区域，就会考虑分配一个区域；</li><li>否则就从HugeCache中分配一个完整的大页面，当然这样会产生slack，但预期其会在未来被填平；</li></ul><p>对于TEMERAIRE来说，1个1GB的空闲范围内存和512 个不连续的free大页是被同等对待。</p><h3 id="HugeAllocator"><a href="#HugeAllocator" class="headerlink" title="HugeAllocator"></a>HugeAllocator</h3><p>HugeAllocator会跟踪记录所映射的虚拟内存，所有操作系统的映射都在此进行。</p><h3 id="HugeCache"><a href="#HugeCache" class="headerlink" title="HugeCache"></a>HugeCache</h3><p>HugeCache以完整的大页粒度来追踪返还的内存范围，HugeFiller填充和释放大页后，需要决定何时将大页返回给操作系统，需要权衡后续是否需要使用来做决定。HugeCache的做法是在 2 秒的滑动窗口内跟踪请求的周期性，并计算记录最大值和最小值，每当内存返回到HugeCache时，如果cache此时大于Demand_max-Demand_min，则将大页返回给操作系统。</p><h3 id="HugeFiller"><a href="#HugeFiller" class="headerlink" title="HugeFiller"></a>HugeFiller</h3><p>HugeFiller用来满足较小的分配请求，每个分配请求都尽量在单个大页完成。HugeFiller满足了大部分的分配请求，是真哥哥系统中最重要的组件。对于给定的大页，使用best-fit的算法来进行分配。</p><p>HugeFiller的两个目标，一是使得一部分大页尽可能地满，另一部分大页面尽可能为空；第二个目标是最小化每个大页内的碎片，使得新分配请求尽可能得到满足。因为几乎为空的大页非常宝贵，通过尽可能保留具备最长空闲内存范围的大页来满足上面的目标，将相应的大页组织到一个排序列表中，充分利用每个大页的统计数据。</p><p>在每个的大页中，HugeFiller会记录使用的页面位图。为了填充某个大页的请求，HugeFiller会从该位图进行best-fit的搜索。并且还记录了以下几个数据：尚未分配的连续页数，最长的空闲内存范围L；分配总数A；已使用的页面总数U。</p><p>通过上述三个统计信息来确定分配大页的优先级顺序——选择具有最小的合适L和最大A的大页。这个的决策选择是根据大量实验做出来的。</p><h3 id="HugeRegion"><a href="#HugeRegion" class="headerlink" title="HugeRegion"></a>HugeRegion</h3><p>HugeCache与HugeAllocator足以满足大内存分配，HugeFiller适用于可以打包成单个大页面的小内存分配，HugeRegion则是用来处理两者不好应付的场景。</p><p>考虑对1.1MiB内存的分配请求，这可以通过HugeFiller分配，对于2MiB的大页会留下0.9 MiB未使用的内存：这里会预期slack会被小于1.1MB的分配请求填充。但极限情况下，很可能会有一个二进制只请求1.1MB的请求。</p><p>HugeRegion是一个固定大小的分配（当前为1GB），与HugeFiller使用位图类似，以小页粒度进行追踪。对于内存请求，一样是采用best-fit策略来应对。出于与HugeFiller相同的原因，其保留了这些区域的列表，按最长空闲内存范围进行排序。</p><p>大多数分配不需要HugeRegion，只有积累了大量比程序小分配数量更多的slack，才会分配HugeRegion。对于键值存储来说，它会将一些大块数据加载到内存，并为服务请求进行一些短期分配。如果没有HugeRegion，请求相关的分配很可能产生大量的空缺。</p><h3 id="Memory-Release"><a href="#Memory-Release" class="headerlink" title="Memory Release"></a>Memory Release</h3><p>如上所述，Release(N)由后台线程定期调用。为了实现接口的Release(N)方法，TEMERAIRE通常只是从HugeCache中释放大页的内存范围。释放的页数量超过提示也不会有问题，后台线程会以实际释放的数量作为反馈，并调整未来的调用以达到合适的总体数量。如果HugeCache不能释放N页内存，HugeFiller将会释放最空的大页上的空闲小页。</p><p>从部分填充的大页面中释放小页是减少内存占用的最后手段，因为该过程在很大程度上是不可逆的。通过在大页上返回部分填充的小页，使操作系统用剩余页面的小条目替换跨越整个页面的单页表条目，这会增加TLB的miss概率，减慢对剩余内存的访问，即便后续重新使用前面释放的内存，Linux内核仍然只使用小的页表项。</p><p>HugeFiller会对部分释放的大页有单独的处理，除非没有其他大页可用，否则不会从它们进行分配，直到这些大页完全为空。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>TEMERAIRE通过更改内存分配器的使用方式来优化TLB的查找性能，从而极大提高了应用程序的性能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Beyond-malloc-efficiency-to-fleet-efficiency&quot;&gt;&lt;a href=&quot;#Beyond-malloc-efficiency-to-fleet-efficiency&quot; class=&quot;headerlink&quot; title=&quot;Beyo</summary>
      
    
    
    
    
    <category term="OS" scheme="http://yoursite.com/tags/OS/"/>
    
  </entry>
  
  <entry>
    <title>CockroachDB</title>
    <link href="http://yoursite.com/2021/10/10/CockroachDB/"/>
    <id>http://yoursite.com/2021/10/10/CockroachDB/</id>
    <published>2021-10-10T14:52:37.000Z</published>
    <updated>2021-10-10T15:02:16.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CockroachDB-The-Resilient-Geo-Distributed-SQL-Database"><a href="#CockroachDB-The-Resilient-Geo-Distributed-SQL-Database" class="headerlink" title="CockroachDB: The Resilient Geo-Distributed SQL Database"></a>CockroachDB: The Resilient Geo-Distributed SQL Database</h1><blockquote><p>本文介绍了一个叫CockroachDB的数据库系统。</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>对于现代数据库来说，OLTP的工作负载越来越与地理分布有关。一些应用可能针对不同的地区分布有着不同的数据库需求，比如某些地区的数据需要有更严格的权限控制满足法律法规，有些地区则是处于快速增长的阶段，需要考虑成本、延时、性能等等的情况。</p><p>CockroachDB作为一款商业DBMS，满足了全球化公司关于数据库系统的种种需求：</p><ul><li><strong>容错和高可用性</strong>：在不同的地区为每个分区至少维护三个副本。节点故障，能自动恢复；</li><li><strong>地理分布和副本放置</strong>：CockroachDB支持水平伸缩，添加节点时自动增加容量并迁移数据。并根据需求选择最优的数据放置方法，同时支持用户自定义选择；</li><li><strong>高性能的事务</strong>：CockroachDB的事务协议支持跨多个分区的分布式事务，而不需要特定的硬件；</li></ul><p>除此之外，CockroachDB还实现了最新的查询优化器和分布式SQL执行引擎来支持更全面的SQL标准。</p><h2 id="SYSTEM-OVERVIEW"><a href="#SYSTEM-OVERVIEW" class="headerlink" title="SYSTEM OVERVIEW"></a>SYSTEM OVERVIEW</h2><h3 id="Architecture-of-CockroachDB"><a href="#Architecture-of-CockroachDB" class="headerlink" title="Architecture of CockroachDB"></a>Architecture of CockroachDB</h3><p>CockroachDB是典型的shared-nothing架构，其所有节点都参与计算和存储。这些节点可由同个数据中心或者多个数据中心组成，client可以连接到任意的节点。在单个节点内部具备以下的分层架构：</p><h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><p>SQL层是最上层，负责与client进行交互，包括了解析器、优化器和SQL执行引擎，其将SQL语句转化成对KV存储的读写请求。SQL层并不清楚数据的具体分区情况。</p><h4 id="Transactional-KV"><a href="#Transactional-KV" class="headerlink" title="Transactional KV"></a>Transactional KV</h4><p>SQL层的请求会来到事务KV层，负责确保跨KV对的更新原子性。</p><h4 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h4><p>该层根据key排序呈现了一个key空间的抽象，包括系统元数据和用户数据都可以在该key空间内查找。CockroachDB使用范围分区的方法将数据划分为大小约为64MiB的连续有序块，称之为Range。Range之间的顺序在一组系统Range内的两级索引结构（前面说过用于内部数据结构和元数据的系统数据也会组成Range）中维护。Range会被缓存起来用于快速查找，同时该层也主要负责对上层查询做路由。</p><p>Range大小为64MB，并根据需要对Range进行合并和拆分（数据太多就拆分，太少就合并，还有根据一些热点策略做Range划分）。</p><h4 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h4><p>默认情况下，每个Range都是三个副本，每个副本存储在不同的节点上。</p><h4 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h4><p>最底下的就是存储层，CockroachDB主要依赖了RocksDB。</p><h3 id="Fault-Tolerance-and-High-Availability"><a href="#Fault-Tolerance-and-High-Availability" class="headerlink" title="Fault Tolerance and High Availability"></a>Fault Tolerance and High Availability</h3><h4 id="Replication-using-Raft"><a href="#Replication-using-Raft" class="headerlink" title="Replication using Raft"></a>Replication using Raft</h4><p>CockroachDB使用Raft算法来进行一致性的复制，其中Range的副本会组成一个Raft组，每个副本要么是leader，要么是follower。CockroachDB的复制单元是Command，它表示的是对存储引擎进行的一系列底层修改，当Raft commit日志的时候，每个副本会将Command应用到存储引擎上。</p><p>CockroachDB使用Range级别的租约，其中一般是Raft组的leader来充当租约持有者，它是唯一可以提供最新读取信息或者发起提议的副本，所有的写入也是通过租约持有者进行。对于用户Range，节点会每隔4.5秒在系统范围内发送一套特殊记录的心跳；对于系统Range，则是每9秒更新一次租约。</p><p>同时为了保证一致性，一次只有一个副本存留，尝试获取租约的副本会通过提交特殊的租约获取日志来达成目的。</p><h4 id="Membership-changes-and-automatic-load-re-balancing"><a href="#Membership-changes-and-automatic-load-re-balancing" class="headerlink" title="Membership changes and automatic load (re)balancing"></a>Membership changes and automatic load (re)balancing</h4><p>集群支持节点的添加和删除，节点的变更会导致负载在集群活动节点之间重新分布。</p><p>对于短暂的故障，只要大多数副本可用，Raft算法能保证CockroachDB正常运行。如果leader失败，则依赖Raft重新选举leader。对于故障节点重新加入集群，则可以通过以下方式追上更新：发送完整Range数据的快照；发送一组要Apply的Raft缺失日志。二选一。</p><p>对于长期故障，CockroachDB则是会根据不受影响的副本，创建一个新的副本，将其放到合适的位置。</p><h4 id="Replica-placement"><a href="#Replica-placement" class="headerlink" title="Replica placement"></a>Replica placement</h4><p>CockroachDB支持手动和自动来控制副本的放置。自动放置副本的机制会根据一定的约束条件进行，并且会尽量平衡负载。</p><h3 id="Data-Placement-Policies"><a href="#Data-Placement-Policies" class="headerlink" title="Data Placement Policies"></a>Data Placement Policies</h3><p>CockroachDB的副本和租约holder的放置策略可以由用户根据性能和容错能力决定：</p><ul><li>副本按地理位置分区；</li><li>租约holder按地理位置分区；</li><li>重复索引：通过基于表来复制索引，并将每个索引的lease holder固定到特定地区，CockroachDB就可以通过较快的本地读取功能，同时提高容错性，但可能会带来写放大和跨区写延时的提高；</li></ul><h2 id="TRANSACTIONS"><a href="#TRANSACTIONS" class="headerlink" title="TRANSACTIONS"></a>TRANSACTIONS</h2><p>CockroachDB的事务可以跨越整个key空间，能在保证ACID的前提下访问到跨越整个集群的数据。CockroachDB是使用MVCC的变形来支持串行化隔离的。</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>SQL事务从SQl连接的节点开始，该节点负责扮演事务协调者的角色。</p><h4 id="Execution-at-the-transaction-coordinator"><a href="#Execution-at-the-transaction-coordinator" class="headerlink" title="Execution at the transaction coordinator"></a>Execution at the transaction coordinator</h4><p>下面的算法给出了基于协调者角度的事务处理逻辑，在执行事务过程中，协调者接受一些列来自SQL层的KV操作。</p><p><img src="https://pic.imgdb.cn/item/6162fdf52ab3f51d91263fb5.png"></p><p>从途中可以看出，在下一个操作发出前，需要对当前操作进行应答，为了提高性能，协调者做了两个优化：写流水线和并行提交。因此，协调者需要跟踪还没完全复制成功的操作和维护了事务时间戳（第一行）。</p><p>写流水线：如果某个操作未尝试提交，并且该操作与先前任何操作都没有重叠，则可能立即执行该操作（第七行）。这就是为什么写流水线可以对不同key的进行多个操作。如果该操作依赖于先前的操作完成复制，则需要pipeline等待。inflightOps就是对运行中的状态进行追踪。第九行就是协调者将操作发送给租约持有者并等待回应。如果返回的时间戳更待，意味着存在了另一个事务影响了租约持有者的时间戳。协调者此时会调整事务时间戳，并通过一轮RPC去验证新时间戳返回的值是否相同，如果不同则事务失败。（第十到十四行）</p><p>本质就是事务内语句流水线并行执行，如果两条语句有操作重叠，则通过事务上下文追踪执行过的key，如果重叠则需要等待其执行成功。</p><p>并行提交：一个标准的2PC，往往需要两轮consensus，一轮完成Prepare，一轮将事务设置为Committed。并行提交的做法是使用了staging来表示一个事务操作的所有写入是否已经都复制完成，持久化staging状态和Prepare写数据同时进行（第五行），假设两者都成功，则协调者可以立即确认事务已经提交（第十五行），并且在终止前会异步将事务状态记录为commit。（十六和十七行）</p><h4 id="Execution-at-the-leaseholder"><a href="#Execution-at-the-leaseholder" class="headerlink" title="Execution at the leaseholder"></a>Execution at the leaseholder</h4><p>如下，当lease holder接收到来自协调者的操作请求时，会按下图执行。第二行险检查租约有效性，第三行会去获取操作依赖的所有key的latch。第四行会校验相关依赖的操作完成。第五和第六行则是在执行写操作时，保证时间戳在任何冲突读取之后，并且根据需要增加时间戳。</p><p><img src="https://pic.imgdb.cn/item/6162fdf52ab3f51d91263fd2.png"></p><p>接下来则是评估操作需要转换为对存储引擎的哪些操作。如果不是事务则无需要等待复制，lease holder可以直接响应协调者。如果是写操作则会被复制，达成共识后会应用到本地引擎。最后释放锁，响应协调者。</p><h3 id="Atomicity-Guarantees"><a href="#Atomicity-Guarantees" class="headerlink" title="Atomicity Guarantees"></a>Atomicity Guarantees</h3><p>CRDB通过观察所有事务的write intents来实现原子提交。write intents就是一个常规的MVCC kv对，所有写入都先写到临时的write intents，其带有的一个元数据指示是intents。该元数据会指向一个事务记录，事务记录就是一个特殊的key，保存了事务当前的状态：pending, staging, committed和aborted。通过write intents，一个Reader会读取其事务记录，如果事务记录已提交，则Reader会将intents视为常规值，并执行清除操作。如果事务是pending的，则会阻塞直至完成。如果是事务是Staging，则可能是commit，也可能是Abort，Reader尝试中止该事务。（如果所有写操作都已经完成复制，实际上就是commit，并对其进行更新）。</p><h3 id="Concurrency-Control"><a href="#Concurrency-Control" class="headerlink" title="Concurrency Control"></a>Concurrency Control</h3><p>如前所述，CRDB对每个事务都以commit时间戳来执行写入和读取。这样可以实现串行化的执行，但同时也会因为事物之间的冲突需要调整时间戳。</p><h4 id="Write-read-conflicts"><a href="#Write-read-conflicts" class="headerlink" title="Write-read conflicts"></a>Write-read conflicts</h4><p>读请求遇到未commit的intent，如果其时间戳更小，则需要等待事务结束。否则可以忽略，并不需要等待。</p><h4 id="Read-write-conflicts"><a href="#Read-write-conflicts" class="headerlink" title="Read-write conflicts"></a>Read-write conflicts</h4><p>以时间戳tb写一个key的时候，如果存在对该key有更大时间戳tb的读请求，则无法直接写入。CRDB强制将写入事务的commit时间戳增加到tb以后。</p><h4 id="Write-write-conflicts"><a href="#Write-write-conflicts" class="headerlink" title="Write-write conflicts"></a>Write-write conflicts</h4><p>写请求如果遇到一个时间戳更小的未commit intent，则等到较早的事务完成。相反，遇到较大时间戳的committed key，则会推进该时间戳。写写冲突可能导致冲突，CRDB会采用分布式死锁检测算法来中止循环等待中的一个事务。</p><h3 id="Read-Refreshes"><a href="#Read-Refreshes" class="headerlink" title="Read Refreshes"></a>Read Refreshes</h3><p>前面提到的冲突会导致事务的commit timestamp推进。如果可以证明事务在ta读取的数据于(ta, tb]时间间隔内没有更小，可以将事务的read timestamp推进到tb &gt; ta。如果发生了数据变更，则会导致事务重启。</p><p>为了确定时间戳是否可以推进，CRDB会在事务的读取集中维护键的集合，并会发出一个Read Refreshes请求来确认key在某个时间间隔内有没有被更新。</p><h3 id="Follower-Reads"><a href="#Follower-Reads" class="headerlink" title="Follower Reads"></a>Follower Reads</h3><p>CRDB允许非租约持有者的副本通过查询修饰符“AS OF SYSTEM TIME”来为带有时间戳的只读查询提供服务。为了提供该功能，其要求在给定时间戳T上执行读取操作的非租约持有者副本确认：将来不存在任何写操作使得读操作无效，并且具备读取所需要的所有数据。即要求Follower提供在时间戳T上的读取内容，并且租约持有者不再接受时间戳T’&lt;&#x3D;T的写操作，Follower还要追上影响到MVCC快找的Raft前缀日志。</p><p>因此，租约持有者需要记录所有传入请求的时间戳，并定期在节点级别生成closed timestamp，在该时间戳以前，将不接受进一步的写操作。closed timestamp与当时的Raft日志索引一起在副本之间定期交换，这样Follower副本就可以使用该状态来决定它们是否剧本在特定时间戳下提供一致性读取所需的所有数据。</p><h2 id="CLOCK-SYNCHRONIZATION"><a href="#CLOCK-SYNCHRONIZATION" class="headerlink" title="CLOCK SYNCHRONIZATION"></a>CLOCK SYNCHRONIZATION</h2><p>CRDB不依赖特定的硬件来进行时钟同步，通过NTP或者Amazon Time Sync Service在公有云或者私有云的服务器即可。</p><h3 id="Hybrid-Logical-Clocks"><a href="#Hybrid-Logical-Clocks" class="headerlink" title="Hybrid-Logical Clocks"></a>Hybrid-Logical Clocks</h3><p>CRDB在集群的每个节点里都维护一个混合逻辑时钟HLC，该时钟提供了物理时间和逻辑时间组合的时间戳。物理时间基于节点的粗同步系统时钟，洛基适中则是基于Lamport时钟。</p><p>HLC提供了一些重要的属性：</p><ol><li>HLC在每次节点交换时钟时通过其逻辑组建提供了因果追踪。节点在发送消息的时候会附加上HLC时间戳，并使用接收到的信息里的时间戳来更新本地时钟。</li><li>HLC在单个节点上的重启内或者重启之间都提供了严格的单调性。</li><li>在瞬态时钟的偏斜波动情况下，HLC能提供自我稳定的功能。</li></ol><h3 id="Uncertainty-Intervals"><a href="#Uncertainty-Intervals" class="headerlink" title="Uncertainty Intervals"></a>Uncertainty Intervals</h3><p>CRDB不支持严格的可串行化，而是通过追踪每个事务的不确定间隔来满足单key线性化属性，事务协调者的本地HLC会分配一个commit_ts，不确定性间隔为[commit_ts, commit_ts + max_offset]。</p><p>当事务遇到在高于commit_ts且在不确定间隔内遇到key时，它会执行不确定性重启，将commit_ts移动到高于不确定值的位置，并保持不确定间隔的上限不变。</p><h3 id="Behavior-under-Clock-Skew"><a href="#Behavior-under-Clock-Skew" class="headerlink" title="Behavior under Clock Skew"></a>Behavior under Clock Skew</h3><p>这里主要是考虑时钟偏离范围的系统行为。本身在单个Range内，通过Raft日志的读写是能够保持任意时钟偏斜下的一致性。但因为引入了Range的租约，如果存在较大的时钟偏移，多个节点可能会发生脑裂，都认为自己拥有租约。CRDB采用两种保护措施来做保护：</p><ol><li>Range租约包含了开始和结束时间戳。租约持有者不能为超出租约间隔的读写提供服务；</li><li>每次写入Range的Raft日志时，都会包含建议使用的Range租约序列号。由于Range本身的租约变更也会被写入Range的Raft日志，因此某个时刻内只有一个租约持有者能够对Range进行变更；</li></ol><h2 id="SQL-1"><a href="#SQL-1" class="headerlink" title="SQL"></a>SQL</h2><h3 id="SQL-Data-Model"><a href="#SQL-Data-Model" class="headerlink" title="SQL Data Model"></a>SQL Data Model</h3><p>每个SQL表或者索引都是存储在若干个Ranges里。所有的用户数据则存储在若干个索引中，其中一个是primary index，primary index在主key上，其他列存储在value里。Secondary Index则是在索引key上，并且还会存储主key以及所以模式所示定的任意数量列。</p><h3 id="Query-Optimizer"><a href="#Query-Optimizer" class="headerlink" title="Query Optimizer"></a>Query Optimizer</h3><p>CRDB中的转换规则时通过Optgen的DSL编写的，提供了用于定义、匹配等简洁语法。Optgen编译为Go，然后与其余CRDB代码库无缝衔接。另外，考虑到CRDB的某些规则涉及到地理分布和分区性质，因此优化器会将数据分布考虑到cost model内，会复制辅助索引以使每个地区都有其自己的副本，从而提高性能，减少跨地区的数据通信。</p><h3 id="Query-Planning-and-Execution"><a href="#Query-Planning-and-Execution" class="headerlink" title="Query Planning and Execution"></a>Query Planning and Execution</h3><p>CRDB的SQL查询执行存在两种模式： gateway-only mode和distributed mode。</p><p>由于分布层提供了一个全局key空间的抽象视图，SQL层可以对任何节点上的Ranges执行读写操作。CRDB根据需要的网络带宽来决定采用哪种模型。对于distributed mode，CRDB通过物理计划阶段，将查询优化器的计划转换为物理SQL运算符的有向无环图。物理计划将逻辑扫描操作分为多个TableReader操作符，每个节点都会包含一个扫描需要读取的Range。扫描被分割后，剩余的逻辑运算符会被安排到与TableReader相同的节点上，从而将filters, joins, and aggregations这些推到尽可能接近物理数据的位置。</p><p><img src="https://pic.imgdb.cn/item/6162fdf52ab3f51d91263fe2.png"></p><p>在数据流内部，CRDB根据输入基数和计划复杂性采用两种不同的执行引擎： Row-at-a-time execution engine和Vectorized execution engine。前者主要是基于Volcano迭代器模型，一次一行。后者则是采用受MonetDB&#x2F;X100启发的向量化执行引擎，主要面向的是列数据，而不是行，从CRDB的KV层读取时，会将磁盘数据从行格式转换为列格式，才发送会有用户之前再次将列格式转换为行格式。</p><h3 id="Schema-Changes"><a href="#Schema-Changes" class="headerlink" title="Schema Changes"></a>Schema Changes</h3><p>CRDB使用一种协议来执行Schema Changes，例如添加列或二级索引，该协议允许在Schema Changes期间保持表能提供在线的读写服务，允许不同的节点在不同时间内异步过渡到新表。具体的实现是将每个Schema Changes分解为一系列增量变更的协议来实现。</p><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>CockroachDB是一个开源的SQL DMBS，支持全球性分布的OLTP业务，并提供了灵活的SQL使用，保证了更好的伸缩性和高可靠性、高性能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CockroachDB-The-Resilient-Geo-Distributed-SQL-Database&quot;&gt;&lt;a href=&quot;#CockroachDB-The-Resilient-Geo-Distributed-SQL-Database&quot; class=&quot;hea</summary>
      
    
    
    
    
    <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>RocksDB源码学习一</title>
    <link href="http://yoursite.com/2021/08/01/Rocksdb%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%E4%B8%80/"/>
    <id>http://yoursite.com/2021/08/01/Rocksdb%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%E4%B8%80/</id>
    <published>2021-08-01T15:59:54.000Z</published>
    <updated>2021-08-01T16:00:48.243Z</updated>
    
    <content type="html"><![CDATA[<h2 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h2><p>第一章先来看一下Rocksdb的编码情况，具体的实现在：<code>util/coding.cc, util/coding.h, util/coding_lean.h</code>。Rocksdb的编码实现与leveldb基本一致的，由于需要将Key、Value等数据按序写入到内存中，并最终flush到磁盘上，因此需要一个高效的编解码实现。这里的编解码很多会用在key size、value size的整型编码上面。</p><p>Rocksdb的整型编码方式很简单，主要支持两种方案：定长编码和变长编码。</p><ul><li>定长编码：定长编码的实现比较简单，比如可以直接将4字节&#x2F;8字节的整型直接按序写入到指定的位置；</li><li>变长编码：节省空间，如果用定长编码的方式，实际上对于小数来说，很多位其实是没必要存储。结合ASCII码的特点，所有字符ASCII码的最高位都是0，可以利用最高位去做一些特别的标记；</li></ul><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="字节端序"><a href="#字节端序" class="headerlink" title="字节端序"></a>字节端序</h4><p>关于编码，首先需要了解计算机在存储字节时分为大端字节序和小端字节序两种，Rocksdb是用小端序存储（低位放在较小的地址处，高位放在较大的地址处）的，因此需要提供根据不同平台对内存存储模型进行转换的选项。</p><p>在port&#x2F;port.h文件内包含了一些平台相关的的头文件：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> defined(ROCKSDB_PLATFORM_POSIX)</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;port/port_posix.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> defined(OS_WIN)</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;port/win/port_win.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>以<code>port_posix.h</code>为例，这里包含了posix平台关于大小端的定义：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">undef</span> PLATFORM_IS_LITTLE_ENDIAN</span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> defined(OS_MACOSX)</span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;machine/endian.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">if</span> defined(__DARWIN_LITTLE_ENDIAN) &amp;&amp; defined(__DARWIN_BYTE_ORDER)</span></span><br><span class="line">    <span class="meta">#<span class="keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN \</span></span><br><span class="line"><span class="meta">        (__DARWIN_BYTE_ORDER == __DARWIN_LITTLE_ENDIAN)</span></span><br><span class="line">  <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> defined(OS_SOLARIS)</span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/isa_defs.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">ifdef</span> _LITTLE_ENDIAN</span></span><br><span class="line">    <span class="meta">#<span class="keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN true</span></span><br><span class="line">  <span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="meta">#<span class="keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN false</span></span><br><span class="line">  <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;alloca.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> defined(OS_AIX)</span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;arpa/nameser_compat.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN (BYTE_ORDER == LITTLE_ENDIAN)</span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;alloca.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> defined(OS_FREEBSD) || defined(OS_OPENBSD) || defined(OS_NETBSD) || \</span></span><br><span class="line"><span class="meta">    defined(OS_DRAGONFLYBSD) || defined(OS_ANDROID)</span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/endian.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">define</span> PLATFORM_IS_LITTLE_ENDIAN (_BYTE_ORDER == _LITTLE_ENDIAN)</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;endian.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">bool</span> kLittleEndian = PLATFORM_IS_LITTLE_ENDIAN;</span><br><span class="line"><span class="meta">#<span class="keyword">undef</span> PLATFORM_IS_LITTLE_ENDIAN</span></span><br></pre></td></tr></table></figure><h4 id="定长编码"><a href="#定长编码" class="headerlink" title="定长编码"></a>定长编码</h4><p>定长编码比较简单，首先判断大小端序，对于小端，value本身就是按小端排放的，可以直接拷贝；对于大端，则是将value的最低位放置在内存的最低地址端。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">EncodeFixed32</span><span class="params">(<span class="type">char</span>* buf, <span class="type">uint32_t</span> value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (port::kLittleEndian) &#123;</span><br><span class="line">    <span class="built_in">memcpy</span>(buf, &amp;value, <span class="built_in">sizeof</span>(value));</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    buf[<span class="number">0</span>] = value &amp; <span class="number">0xff</span>;</span><br><span class="line">    buf[<span class="number">1</span>] = (value &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xff</span>;</span><br><span class="line">    buf[<span class="number">2</span>] = (value &gt;&gt; <span class="number">16</span>) &amp; <span class="number">0xff</span>;</span><br><span class="line">    buf[<span class="number">3</span>] = (value &gt;&gt; <span class="number">24</span>) &amp; <span class="number">0xff</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解码也一样，Rocksdb提供了三种整型编解码：<code>uint16_t,uint32_t,uint64_t</code>。解码时，gcc会优化里面的memcpy的操作，变成inline copy loops，提高效率。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">uint32_t</span> <span class="title">DecodeFixed32</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* ptr)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (port::kLittleEndian) &#123;</span><br><span class="line">    <span class="comment">// Load the raw bytes</span></span><br><span class="line">    <span class="type">uint32_t</span> result;</span><br><span class="line">    <span class="built_in">memcpy</span>(&amp;result, ptr, <span class="built_in">sizeof</span>(result));  <span class="comment">// gcc optimizes this to a plain load</span></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">return</span> ((<span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(<span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">char</span>&gt;(ptr[<span class="number">0</span>]))) |</span><br><span class="line">            (<span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(<span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">char</span>&gt;(ptr[<span class="number">1</span>])) &lt;&lt; <span class="number">8</span>) |</span><br><span class="line">            (<span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(<span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">char</span>&gt;(ptr[<span class="number">2</span>])) &lt;&lt; <span class="number">16</span>) |</span><br><span class="line">            (<span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(<span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">char</span>&gt;(ptr[<span class="number">3</span>])) &lt;&lt; <span class="number">24</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="变长编码"><a href="#变长编码" class="headerlink" title="变长编码"></a>变长编码</h4><p>前面提到的，为了节省空间，Rocksdb使用变长的编码方式<em>varint</em>。Rocksdb使用最高位来表示编码是否结束，而低7bit则存储实际的数据。根据统计来说，小整型出现的概率更高，这样就能节省更多的空间，比如0-127的整数都可以只用一个字节来表示，而uint32较大的数字则需要5个字节。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">0001 0001 ====&gt;&gt; 表示33</span><br><span class="line">^                           A: 最高位为0，表示结束；</span><br><span class="line">A</span><br><span class="line">=======================================================</span><br><span class="line">1000 0011 0110 1111 ====&gt;&gt; 表示1007</span><br><span class="line">^         ^                 A: 最高位为1，表示未结束，实际值是000 0011；</span><br><span class="line">A         B                 B: 最高位为0，表示结束，实际值是110 1111；</span><br></pre></td></tr></table></figure><p>具体的编码以<code>uint32_t</code>为例，将<code>uint32_t</code>编码成变长字符：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">char</span>* <span class="title">EncodeVarint32</span><span class="params">(<span class="type">char</span>* dst, <span class="type">uint32_t</span> v)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Operate on characters as unsigneds</span></span><br><span class="line">  <span class="type">unsigned</span> <span class="type">char</span>* ptr = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">unsigned</span> <span class="type">char</span>*&gt;(dst);</span><br><span class="line">  <span class="type">static</span> <span class="type">const</span> <span class="type">int</span> B = <span class="number">128</span>;</span><br><span class="line">  <span class="keyword">if</span> (v &lt; (<span class="number">1</span> &lt;&lt; <span class="number">7</span>)) &#123;</span><br><span class="line">    *(ptr++) = v;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span> &lt;&lt; <span class="number">14</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = v &gt;&gt; <span class="number">7</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span> &lt;&lt; <span class="number">21</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = v &gt;&gt; <span class="number">14</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span> &lt;&lt; <span class="number">28</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">14</span>) | B;</span><br><span class="line">    *(ptr++) = v &gt;&gt; <span class="number">21</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">14</span>) | B;</span><br><span class="line">    *(ptr++) = (v &gt;&gt; <span class="number">21</span>) | B;</span><br><span class="line">    *(ptr++) = v &gt;&gt; <span class="number">28</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">reinterpret_cast</span>&lt;<span class="type">char</span>*&gt;(ptr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>uint64_t</code>的变长编码实现，作者用了循环来编码，每7bit一组，并在最低位判断是否需要置位1。因此对于64位整型，最多需要10个字节：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">char</span>* <span class="title">EncodeVarint64</span><span class="params">(<span class="type">char</span>* dst, <span class="type">uint64_t</span> v)</span> </span>&#123;</span><br><span class="line">  <span class="type">static</span> <span class="type">const</span> <span class="type">unsigned</span> <span class="type">int</span> B = <span class="number">128</span>;</span><br><span class="line">  <span class="type">unsigned</span> <span class="type">char</span>* ptr = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">unsigned</span> <span class="type">char</span>*&gt;(dst);</span><br><span class="line">  <span class="keyword">while</span> (v &gt;= B) &#123;</span><br><span class="line">    *(ptr++) = (v &amp; (B - <span class="number">1</span>)) | B; <span class="comment">// leveldb的实现是v | B; 不明白区别在哪</span></span><br><span class="line">    v &gt;&gt;= <span class="number">7</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  *(ptr++) = <span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">char</span>&gt;(v);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">reinterpret_cast</span>&lt;<span class="type">char</span>*&gt;(ptr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解码的实现也有一些优化，主要是利用内联函数提高效率，当数字小于等于127时，直接返回结果：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">const</span> <span class="type">char</span>* <span class="title">GetVarint32Ptr</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* p,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="type">const</span> <span class="type">char</span>* limit,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="type">uint32_t</span>* value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (p &lt; limit) &#123;</span><br><span class="line">    <span class="type">uint32_t</span> result = *(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">unsigned</span> <span class="type">char</span>*&gt;(p));</span><br><span class="line">    <span class="keyword">if</span> ((result &amp; <span class="number">128</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">      *value = result;</span><br><span class="line">      <span class="keyword">return</span> p + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">GetVarint32PtrFallback</span>(p, limit, value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span>* <span class="title">GetVarint32PtrFallback</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* p, <span class="type">const</span> <span class="type">char</span>* limit,</span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">uint32_t</span>* value)</span> </span>&#123;</span><br><span class="line">  <span class="type">uint32_t</span> result = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">uint32_t</span> shift = <span class="number">0</span>; shift &lt;= <span class="number">28</span> &amp;&amp; p &lt; limit; shift += <span class="number">7</span>) &#123;</span><br><span class="line">    <span class="type">uint32_t</span> byte = *(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">unsigned</span> <span class="type">char</span>*&gt;(p));</span><br><span class="line">    p++;</span><br><span class="line">    <span class="keyword">if</span> (byte &amp; <span class="number">128</span>) &#123;</span><br><span class="line">      <span class="comment">// More bytes are present</span></span><br><span class="line">      result |= ((byte &amp; <span class="number">127</span>) &lt;&lt; shift);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      result |= (byte &lt;&lt; shift);</span><br><span class="line">      *value = result;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">char</span>*&gt;(p);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>编解码这里还是比较简单和高效的，比较有意思的是实现了一个variant编码，源码值得一看。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;编码&quot;&gt;&lt;a href=&quot;#编码&quot; class=&quot;headerlink&quot; title=&quot;编码&quot;&gt;&lt;/a&gt;编码&lt;/h2&gt;&lt;p&gt;第一章先来看一下Rocksdb的编码情况，具体的实现在：&lt;code&gt;util/coding.cc, util/coding.h, util/</summary>
      
    
    
    
    
    <category term="RocksDB" scheme="http://yoursite.com/tags/RocksDB/"/>
    
  </entry>
  
  <entry>
    <title>Dynamo: Amazon’s Highly Available Key-value Store</title>
    <link href="http://yoursite.com/2021/07/26/Dynamo-Amazon%E2%80%99s-Highly-Available-Key-value-Store/"/>
    <id>http://yoursite.com/2021/07/26/Dynamo-Amazon%E2%80%99s-Highly-Available-Key-value-Store/</id>
    <published>2021-07-25T16:10:16.000Z</published>
    <updated>2021-07-25T16:10:53.243Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dynamo"><a href="#Dynamo" class="headerlink" title="Dynamo"></a>Dynamo</h1><blockquote><p>原文是2007年SOSP上Amazon发布的分布式存储经典论文<a href="https://sites.cs.ucsb.edu/~agrawal/fall2009/dynamo.pdf">《<strong>Dynamo: Amazon’s Highly Available Key-value Store</strong>》</a>。这是一个高可用的分布式KV存储——Dynamo，Amazon的一些核心服务就是依赖Dynamo提供持续可用的服务，为了达到这种可用级别，Dynamo牺牲了几种特定场景下的一致性。并且，Dynamo大量地使用了对象版本化和应用层面的冲突解决机制。</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>支撑着Amazon电商发展的是建立在分布于全球数据中心成千上万的服务器基础上的，因此对性能、可靠性和效率都有很高的要求。同时为了支撑业务的持续增长和避免因故障导致的经济损失，平台需要有足够好的可扩展性、可靠性。</p><p>Amazon使用的是去中心化的、松耦合的、面向服务的架构，这种服务架构对持续可用的存储技术有着强烈的诉求。例如，即便是磁盘故障、路由抖动、数据中心被摧毁，用户仍然能够向购物车添加和查看商品。因此Amazon推出了一款高可用的kv存储组件——Dynamo。Dynamo用于管理对可靠性要求非常高的服务，这些服务还要求对一致性、成本效率和性能有很强的控制能力。</p><p>Dynamo使用了一些常见的技术来实现了可扩展性和高可用性：</p><ul><li>数据通过一致性哈希来分区和复制；</li><li>通过对象版本化来实现一致性；</li><li>副本之间的一致性使用了类似quorum的技术和一个去中心化的副本同步协议；</li><li>gossip-based分布式故障检测和成员检测协议；</li></ul><p>Dynamo是一个极少需要人工管理的、完全去中心化的系统，向Dynamo添加或者删除节点不需要人工调整哈希节点和重分布节点间数据。</p><h2 id="BACKGROUND"><a href="#BACKGROUND" class="headerlink" title="BACKGROUND"></a>BACKGROUND</h2><p>传统上生产系统会使用关系型数据库来存储状态，但这并不是一种理想的方式，大多数服务并不需要RDBMS提供的复杂查询和管理功能，这些额外的支持带来的硬件成本并不经济，并且这类数据库的复制功能有局限，往往是通过牺牲可用性来换取一致性。</p><h3 id="System-Assumptions-and-Requirements"><a href="#System-Assumptions-and-Requirements" class="headerlink" title="System Assumptions and Requirements"></a>System Assumptions and Requirements</h3><p>Dynamo对于使用它的服务有几点假设：</p><ul><li>Query Model：通过唯一的key对数据进行读写，存储状态是binary objects。没有relational schema需求，无跨data items的操作。存储对象较小，往往小于1MB；</li><li>ACID Properties：Dynamo的设计目标是使用部分一致性来换取更高的可用性；</li><li>Efficiency：存储系统必须满足那些严格的SLA；</li><li>Other Assumptions：内部使用，假设环境足够安全；</li></ul><h3 id="Service-Level-Agreements-SLA"><a href="#Service-Level-Agreements-SLA" class="headerlink" title="Service Level Agreements (SLA)"></a>Service Level Agreements (SLA)</h3><p>在Amazon去中心化的基础设施中，SLA会扮演着重要的角色，客户端和服务端会定义一个 SLA协议。Amazon不是使用传统的平均值、中位数和方差来描述面向性能的SLA，而是更多使用了P99.9分布，来确定性能的长尾结果。</p><h3 id="Design-Considerations"><a href="#Design-Considerations" class="headerlink" title="Design Considerations"></a>Design Considerations</h3><p>前面提过，很多系统中数据复制算法一般是同步的，为了提供一个强一致性的数据访问结果，往往会牺牲掉某些场景下的可用性。考虑到这一点，Dynamo最终被设计为最终一致的数据存储系统。</p><p>在分布式系统中，需要关注的是机器或者网络故障时可能会导致数据冲突，需要检测和解决冲突。一些传统的数据库可能会在写的时候解决冲突，牺牲一点可用性。但Dynamo的目标是提供一个持续可写的存储，因此将解决冲突的逻辑放到了读操作，从而避免写操作被拒绝。同时Dynamo可以配置是存储系统来解决冲突还是应用选择自行实现冲突解决操作。</p><h2 id="SYSTEM-ARCHITECTURE"><a href="#SYSTEM-ARCHITECTURE" class="headerlink" title="SYSTEM ARCHITECTURE"></a>SYSTEM ARCHITECTURE</h2><p>本文主要介绍了Dynamo用到的部分分布式系统技术：包括partitioning, replication, versioning, membership, failure handling 和 scaling。</p><h3 id="System-Interface"><a href="#System-Interface" class="headerlink" title="System Interface"></a>System Interface</h3><p>Dynamo的存储接口非常简单，只有两个：</p><ul><li>get()：会返回存储key对应的所有对象副本，以及一个context；</li><li>put()：确定对象的存储位置，写入到相应的磁盘。</li></ul><p>Dynamo将调用方提供的key和对象都视作是opaque array of bytes，其对key应用MD5哈希得到一个128bit的ID，并根据ID计算应该存储到哪个存储节点。</p><h3 id="Partitioning-Algorithm"><a href="#Partitioning-Algorithm" class="headerlink" title="Partitioning Algorithm"></a>Partitioning Algorithm</h3><p>Dynamo的设计有一个核心诉求：支持增量扩展。这就要求有一种机制能够将数据分散到系统的不同节点中，Dynamo的方案是基于一致性哈希，其哈希函数的输出是一个固定范围，作为一个循环空间环，每个节点会随机分配一个循环空间内的值，代表着节点在环上的节点。</p><p>Dynamo寻找item对应节点的方法：</p><ul><li>首先对key做哈希得到哈希值；</li><li>然后在环上沿着顺时针方向找到第一个多带值被这个哈希值更大的节点；</li></ul><p>这种方法有一个缺陷，就是每个节点随机分配的位置可能会导致数据不均匀分布负载，也没有考虑到节点的异构因素。为了解决这些问题，Dynamo做了优化，每个节点不是映射到环上的一个点，而是多个点。Dynamo使用了虚拟节点的概念，一个新节点加入到系统后，会在环上分配多个位置（对应多个token）。</p><p>虚拟节点的好处就是：</p><ul><li>当一个节点不可用离开时，会将该节点管理的虚拟节点平均分配给其他真实节点均衡管理；</li><li>同理，新节点加入时，会从现有虚拟节点中拿出虚拟节点分配给新节点；</li><li>一个节点负责的虚拟节点数量可以根据节点容量来决定，充分利用机器的异构性信息；</li></ul><h3 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h3><p>为了实现高可用性和持久性，Dynamo会将数据复制到N台机器上，N可配置。</p><p>具体的做法是，每个Key都会分配一个coordinator节点，coordinator负责落到它管理范围内的复制，除了自己存储一份之外，还会沿着顺时针方向的其他N-1个节点存储一份副本。</p><p>如下图，B除了自己存储一份之外，还会将数据存储到C和D节点。D实际存储的数据，其key范围包括了<code>(A, B]</code>、<code>(B, C]</code> 和 <code>(C, D]</code>。</p><p><img src="https://pic.imgdb.cn/item/60fd8cb75132923bf82b2916.png"></p><p>存储某个特定key的所有节点会组成一个<strong>preference list</strong>，为了防止节点的failure，整这个preference list可能多于N个节点，另外由于引入了虚拟节点机制，preference list会保证N个节点不落在相同的物理机上。</p><h3 id="Data-Versioning"><a href="#Data-Versioning" class="headerlink" title="Data Versioning"></a>Data Versioning</h3><p>Dynamo提供最终一致性，所有更新操作会异步地传递给所有的副本。put()操作返回时，更新可能还没有应用到所有的副本，后续的get操作可能去不到最新数据。Amazon有些应用是可以容忍这种不一致性的，应用在这种情况下能继续运行。以操作购物车威力，如果购物车的最新状态不可用，用户对一个老版本的购物车状态做了修改，这种修改是需要保留的，由后续的步骤来解决冲突。</p><p>为了解决冲突，Dynamo将每次修改结果都作为一个全新的版本，允许系统多个版本共存。使得冲突一致化的两种方式：syntactic reconciliation和semantic reconciliation。在大多数情况下，新版本都包含了老版本的数据，而且系统可以判断哪个是权威版本，这就是syntactic reconciliation。</p><p>但是在发生故障并且并发更新的情况下，版本可能发生分叉，系统无法处理这种情况，需要客户端介入，从而将多个版本分支合并成一个，这就是semantic reconciliation。这种操作的好处是写操作永远可用，但会导致业务应用上一些奇怪现象，比如已经删除的商品偶尔又在购物车中冒出来。</p><p>Dynamo使用<strong>向量时钟（vector clock）</strong>来追踪同一个对象不同版本之间的因果关系，一个向量时钟就是一个 (node, counter) 列表。一个向量时钟关联了一个对象的所有版本，可以用来判断对象两个版本是并行分支还是具备因果关系。如果对象的第一个时钟上的所有 counter 都小于它的第二个时钟上的 counter，那第一 时钟就是第二个的祖先，可以安全的删除。否则需要进行reconciliation。</p><p>在Dynamo中，客户端更新一个对象需要指明基于哪个版本进行更新。流程是先读拿到context，context带有vector clock，写的时候把context带下去。在读的时候如果发现了多个版本，并且系统无法reconcile这些版本，就会返回所有的版本，待解决了冲突将多个版本分支合并起来。</p><p>以下图为例</p><p><img src="https://pic.imgdb.cn/item/60fd8cb75132923bf82b2923.png"></p><ul><li>客户端写入一个对象。处理这个key的写请求节点Sx增加key的counter，系统有了一个对象D1和它的时钟[(Sx, 1)]；</li><li>客户端更新这个对象。假设还是Sx处理这个请求。此时，系统有了对象D2和它的时钟 [(Sx, 2)]，但可能D1在其他节点的副本还没有看到这个更新；</li><li>假设这个客户端，再次更新了对象，并且这次是由另外的一个节点Sy处理 请求。此时，系统有了D3和它的时钟[(Sx, 2), (Sy, 1)]。假设另一个客户端读取D2，并尝试更新它，写请求由另一个节点Sz处理。 现在，系统有D4（D2的后代），版本clock是[(Sx, 2), (Sz, 1)]。</li><li>此时，D3和D4各自的改动都没有反映在对方之中。因此这两个版本都应当被保留，然后交给客户端，由客户端在下次读取的时候执行semantic reconciliation；</li><li>假设某个客户端读到了D3和D4，即[(Sx, 2), (Sy, 1), (Sz, 1)]。如果客户端执行 reconciliation，并且节点Sx执行协调写，Sx会更新自己在clock中的序列号。最终新生成的数据D5的clock格式如下：[(Sx, 3), (Sy, 1), (Sz, 1)]。</li></ul><p>vector clock的一个潜在问题是，如果有多个节点先后协调同一个对象的写操作，那这个对象的clock vector会变得很长。这种情况发生的可能性不大，只有在网络分裂或多台服务器挂掉的情况下，写操作才可能由非preference list前N个节点来执行，导致vector clock变长。</p><p>为了避免这个问题，Dynamo采用的方法是clock truncation scheme，另外保存一个和<code>(node, counter)</code> 对应的时间戳，记录最后一次更新该记录的时间，当vector clock达到阈值时就删掉最老的一个。这种方案可能会导致无法精确判断部分后代的因果关系，但论文说生产环境没遇到过这个问题。</p><h3 id="Execution-of-get-and-put-operations"><a href="#Execution-of-get-and-put-operations" class="headerlink" title="Execution of get () and put () operations"></a>Execution of get () and put () operations</h3><p>Dynamo中所有存储节点都可以接受key的读写操作。</p><p>读写操作由Amazon基础设施相关的请求处理框架发起HTTP请求。客户端有两种选择：</p><ul><li>将请求路由到负载均衡器，由后者根据负载信息选择后端节点；</li><li>使用能感知partition的客户端，直接路由到coordinator节点；</li></ul><p>前者是负载均衡器转发到环上任意一个节点，如果收到请求的节点不是preference list前N个节点中的一个，那它就不会处理这个请求，而是转发到preference list第一个节点。</p><p>读写操作需要preference list中有不可用节点，就跳过。优先访问preference list中编号较小的节点。</p><p>为了保证副本的一致性，Dynamo使用了一种类似quorum的一致性协议。这个协议有两个配置参数：<code>R</code> 和 <code>W</code>：</p><ul><li>R：允许执行一次读操作所需的最少节点数；</li><li>W：允许执行一次写操作所需的最少节点数；</li></ul><p>设置<code>R + W &gt; N</code>，就得到了一个quorum系统。在这种模型下，读写请求由R&#x2F;W副本中最慢的一个决定。</p><p>当收到写请求后，coordinator 会为新版本生成 vector clock，并将其保存到节点本地。然后将新版本（和对应的vector clock）发送给N个排在最前面的、可用的节点，只要有至少W-1个节点返回成功，就认为写操作成功。</p><p>读操作类似，如果coordinator收集到多个版本，它会将所有系统判断没有因果关系的版本返 回给客户端。客户端需要对版本进行reconcile，合并成一个最新版本，然后将结果写回 Dynamo。</p><h3 id="Handling-Failures-Hinted-Handoff"><a href="#Handling-Failures-Hinted-Handoff" class="headerlink" title="Handling Failures: Hinted Handoff"></a>Handling Failures: Hinted Handoff</h3><p>如果使用传统的quorum算法，Dynamo无法在节点不可用时保持可用。Dynamo采用了一种更为宽松quorum：所有读和写操作在preference list的前N个健康节点上执行，遇到不可用节点，会沿着哈希环的顺时针方向顺延。</p><p>以下图为例，如果A临时不可用，正常情况下发到A的请求会发送到D，发到D副本的元数据中会提示这个副本数据应该发到A，然后这个数据会被D保存到本地的一个独立数据库中，并且有一个定期任务不断扫描，一旦A可用了，就将这个数据发送回 A，然后D就可以从本地数据库中将其删除了。</p><h3 id="Handling-permanent-failures-Replica-synchronization"><a href="#Handling-permanent-failures-Replica-synchronization" class="headerlink" title="Handling permanent failures: Replica synchronization"></a>Handling permanent failures: Replica synchronization</h3><p>如果出现了在hinted副本移交给原副本节点之前就变的不可用，就会威胁到持久性。Dynamo基于Merkle trees实现了一种逆熵（副本同步）协议来保证副本是同步的。</p><h3 id="Membership-and-Failure-Detection"><a href="#Membership-and-Failure-Detection" class="headerlink" title="Membership and Failure Detection"></a>Membership and Failure Detection</h3><h4 id="Ring-Membership"><a href="#Ring-Membership" class="headerlink" title="Ring Membership"></a>Ring Membership</h4><p>Amazon使用显式机制来向Dynamo环增删节点，管理员通过命令行或web方式连接到 Dynamo node，然后下发一个成员变更命令增删节点。负责处理这个请求的 node 将成员变动信息和对应的时间写入持久存储。成员变动会形成历史记录。Dynamo使用一个gossip-based的算法传播成员变更信息。</p><h4 id="External-Discovery"><a href="#External-Discovery" class="headerlink" title="External Discovery"></a>External Discovery</h4><p>上面的逻辑会有个问题，假设管理员同时添加两个节点，那么它们不会立即感知到对方，导致临时的逻辑分裂。为了避免这个问题，论文将部分Dynamo节点作为种子节点，所有节点都知道种子节点的存在，因为所有节点最终都会和种子节点reconcile成员信息，所以逻辑分裂就几乎不可能发生了。种子是从静态配置文件或者配置中心获取的。</p><h4 id="Failure-Detection"><a href="#Failure-Detection" class="headerlink" title="Failure Detection"></a>Failure Detection</h4><p>故障检测在Dynamo中的读写操作或者partition和hinted replica时移跳过不可用的节点。其做法是，节点B只要没有应答节点A的消息，A就认为B不可达。在有持续的client请求时，Dynamo Ring上的节点会有持续的交互，能定期检查及诶但是否恢复。使用简单的gossip协议就可以感知到节点的增删。</p><h3 id="Adding-x2F-Removing-Storage-Nodes"><a href="#Adding-x2F-Removing-Storage-Nodes" class="headerlink" title="Adding&#x2F;Removing Storage Nodes"></a>Adding&#x2F;Removing Storage Nodes</h3><p>当新节点加入系统后，会获得一些随机分散到Ring上的token，此时原本负责某些key range的节点会将此时负责的key转移到新节点。</p><h2 id="IMPLEMENTATION"><a href="#IMPLEMENTATION" class="headerlink" title="IMPLEMENTATION"></a>IMPLEMENTATION</h2><p>Dynamo支持选择不同的本地存储组件来作为存储引擎，其实以插件的方式引入的，包括了BDB、Mysql、in-memory buffer with persistent backing store等。应用能够为不同的访问类型选择最合适的存储引擎。</p><p>coordinator会代替客户端执行读写请求，每个客户端请求都会在收到这个请求的节点上创建一个状态机，包括了所有相关的逻辑。</p><p>对于写请求，前面提到过由preference list前N个节点中任一个coordinate，总是由第一个来coordinate的好处是使得在同一个地方完成写操作的顺序化，但可能会导致复杂不均匀。为了解决这个问题，preference list内的所有N个节点都可以coordinate写操作，另外由于写操作前面都带有读操作，写操作的coordinator会选择前一次读操作返回最快的节点（这个信息会被存在返回的上下文中）。由于这项优化使得前一次读操作选中的存储节点更容易被选中，提高了read-your-writes的概率。</p><h2 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h2><p>本文介绍了Dynamo作为一个高可用、高可扩展性的数据存储系统，在Amazon的诸多核心系统中都有应用。Dynamo提供了期望的可用性与性能，能够很好处理节点不可用、网络分裂的情况。Dynamo最大的意义是证明了：一些去中心化的技术结合起来能够提供一个高可用的系统，并且在很多应用环境中投产了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Dynamo&quot;&gt;&lt;a href=&quot;#Dynamo&quot; class=&quot;headerlink&quot; title=&quot;Dynamo&quot;&gt;&lt;/a&gt;Dynamo&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;原文是2007年SOSP上Amazon发布的分布式存储经典论文&lt;a href=&quot;h</summary>
      
    
    
    
    
    <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>TiDB: A Raft-based HTAP Database</title>
    <link href="http://yoursite.com/2021/07/26/TiDB-A-Raft-based-HTAP-Database/"/>
    <id>http://yoursite.com/2021/07/26/TiDB-A-Raft-based-HTAP-Database/</id>
    <published>2021-07-25T16:03:14.000Z</published>
    <updated>2021-07-25T16:06:13.186Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TIDB"><a href="#TIDB" class="headerlink" title="TIDB"></a>TIDB</h1><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>Hybrid Transactional and Analytical Processing (HTAP，混合事务和分析处理)数据库要求独立地处理事务和分析查询，避免相互干扰。为了实现这一点，需要为两类查询维护不同的数据副本。然而，为存储系统中的分布式副本提供一致性视图并不容易。在该系统中，分析请求可以大规模地、高可靠性地从事务工作负载中读取一致的实时数据。</p><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>关系型数据库（RDBMS）一直因其关系模型、强力的事务保证和SQL接口而广受好评，但它不具备高可扩展性和高可用性。因此NoSQL就应运而生，像Google bigtable和DynamoDB之类的放宽了一致性要求，提供了更高的可扩展性。但由于业务又需要事务处理能力、数据一致性和SQL接口等，就慢慢出现了像CockroachDB和Spanner之类的NewSQL。此外，像许多架构在Hadoop之上的SQL系统一样，在线分析处理系统（OLAP）也在迅速发展。</p><p>以前一般认为针对OLAP和OLTP应该采用的不同的数据模型和技术方案，但维护多个系统的成本很高。业界开始探索OLTP和OLAP的混合系统HTAP。HTAP系统需要满足几个关键点：一是数据新鲜度，即OLAP需要拿到最新的数据；二是隔离，即为单独的OLTP或者OLAP查询提供不同的硬件资源处理，避免性能相互影响。</p><p>本文就是基于上面的考虑，提出了一个以Raft为共识算法的HTAP数据库——TiDB，在Raft中引入了一个专门的节点Learner，Learner会异步地复制Leader节点的事务日志，并为OLAP查询构造新副本，并将日志中的行格式元组转换为列格式，便于查询。</p><h2 id="RAFT-BASED-HTAP"><a href="#RAFT-BASED-HTAP" class="headerlink" title="RAFT-BASED HTAP"></a>RAFT-BASED HTAP</h2><p>使用共识算法如Raft、Paxos等可以基于复制状态机在服务器之间实时可靠地复制数据，通过调整，该论文的做法可以针对不同的 HTAP 工作负载将数据复制到不同的服务器，并且保持资源隔离和数据新鲜度。</p><p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf82635ad.png"></p><p>如上图所示，TiDB将数据按行格式存储在多个Raft Group里，每个Raft Group由一个Leader和多个Follower组成，另外为每个组增加一个Learner角色，可以异步复制Leader的数据，并将行格式转换为列格式。另外，扩展的查询优化器用来构建访问列存和行存的物理计划。</p><p>在该实现中，Leader不参与Leader选举，也不计入日志复制的个数，不参与Quorum。读数据时，需要保证Leader和Learner之间的强一致性和日志复制的低延迟。行列格式的转换也有优点，行格式可以利用索引来高效进行事务查询，列格式可以有效利用数据压缩和矢量化处理。由于Learner部署在单独的物理资源中，OLAP和OLTP可以在独立的资源中得到处理。</p><p>总的来说，这个设计克服了几个困难：一是基于Raft的系统如何应对高并发读写；二是资源独立如何保证数据新鲜度；另外就是如何构建查询计划在行式存储和列式存储中选择最优的。</p><h2 id="ARCHITECTURE"><a href="#ARCHITECTURE" class="headerlink" title="ARCHITECTURE"></a>ARCHITECTURE</h2><p>如下图所示，这是TIDB的架构，兼容MySQL协议，由三个核心组件组成：分布式存储层、Placement Driver布局驱动器和计算引擎层。</p><p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf82635cf.png"></p><ul><li>分布式存储层</li></ul><p>由行式存储（TiKV）和列式存储（TiFlash）组成，存储在TiKV的数据是有序的key-value对，key由两个整数table id和row id组成，其中row id就是主键列，value就是真实的行数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: &#123;table&#123;tableID&#125; record&#123;rowID&#125;&#125;</span><br><span class="line">Value: &#123;col0, col1, col2, col3&#125;</span><br></pre></td></tr></table></figure><p>为了保证可扩展性，TiDB采用了range partition的策略，将kv对映射分割成若干个连续的范围，每个范围称为一个region，每个region都有多个副本来保证可用性，Raft就是用于维护每个region中若干个副本的一致性的。</p><p>PD负责管理region，包括提供每个key对应的region和其物理位置，以及自动转移region以平衡负载。同时PD也是时间戳分配器，提供了严格递增全剧唯一的时间戳。PD不具备持久状态。</p><p>计算引擎层是无状态、可扩展的，其SQL引擎由cost-based query optimizer和distributed query executor组成。另外TiDB基于Percolator实现了2PC协议。</p><p>除此之外，TiDB还与Spark集成，方便集成TiDB和HDFS中存储的数据。</p><h2 id="MULTI-RAFT-STORAGE"><a href="#MULTI-RAFT-STORAGE" class="headerlink" title="MULTI-RAFT STORAGE"></a>MULTI-RAFT STORAGE</h2><p>下图展示了分布式存储层的架构，其由TiKV和TiFlash组成。每个Region的副本之间都使用Raft来维护数据一致性。当数据复制到TiFlash的时候，为了方便扫描，多个Regions会被合并成一个Partition。</p><p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf82635e8.png"></p><h3 id="Row-based-Storage"><a href="#Row-based-Storage" class="headerlink" title="Row-based Storage"></a>Row-based Storage</h3><p>TiKV是由多个TiKV服务器组成的，每个TiKV服务器都可以是不同Region的Raft Leader或者Follower，另外在TiKV服务器上，数据和原数据都会保存在RocksDB上。</p><p>基于Raft算法响应读写请求的过程如下：</p><ol><li>Region的Leader从SQL引擎层接受请求；</li><li>Leader将请求Append到日志中；</li><li>Leader向Follower发送新的日志条目，Follower会将接收到的日志Append到自己的日志中；</li><li>Leader等待Follower回应，满足指定数量的节点响应成功后，Leader会在本地commit并Apply；</li><li>Leader将结果发送给客户端；</li></ol><h4 id="Optimization-between-Leaders-and-Followers"><a href="#Optimization-between-Leaders-and-Followers" class="headerlink" title="Optimization between Leaders and Followers"></a>Optimization between Leaders and Followers</h4><p>为了提高吞吐，可以在Leaders和Followers之间的操作做一些优化。首先是，上述过程的第二步和第三步可以并行进行，即便第二步失败了，只要第三步成功了仍然可以提交日志。另外就是，第三步中Leader可以缓冲这些日志条目，并批量发送。并且发送日志后也不需要等待Follower响应，可以假设发送成功，并利用预测的日志索引发送后来的日志。即便出现错误，Leader可以调整日志索引进行重发。还有就是，第四步中，Leader应用日志可以交给其他线程去做。整体流程就变成：</p><ol><li>Region的Leader从SQL引擎层接受请求；</li><li>Leader并行地向Follower发送日志，并同时Append本地日志；</li><li>Leader继续接收请求，重复2；</li><li>Leader commit日志，并将应用逻辑交给另外的线程去做；</li><li>应用日志后，Leader返回结果；</li></ol><h4 id="Accelerating-Read-Requests-from-Clients"><a href="#Accelerating-Read-Requests-from-Clients" class="headerlink" title="Accelerating Read Requests from Clients"></a>Accelerating Read Requests from Clients</h4><p>从Leader读取数据具有线性化的语义，但通过常规的Raft流程来保证会导致很高的网络IO开销。为了提高性能，可以避免日志同步阶段。</p><p>Raft保证：一旦Leader成功写入数据，就可以响应任何读取请求，而无需同步日志。但Leader是可能改变的。为了实现从Leader读取，TiKV做了以下优化：</p><ul><li>read index：Leader响应请求时，会将当前的提交索引记录为本地read index，然后向Follower发送heartbeat确认Leader地位。如果确实是Leader，并且应用的索引大于或等于read index，就可以返回值。</li><li>lease read：为了减少heartbeat开销，Leader和Follower之间维护一个Lease期限，Follower在这期间不发出选举请求，因此Leader在此期间也无需与Follower进行heartbeat交流。</li></ul><p>Follower也可以响应client的读请求，但需要向Leader询问read index，如果本地应用索引等于或大于read index，则Follower可以将值返回给客户端。</p><h4 id="Managing-Massive-Regions"><a href="#Managing-Massive-Regions" class="headerlink" title="Managing Massive Regions"></a>Managing Massive Regions</h4><p>为了实现跨机器平衡Region，Plancement Driver会对Region副本数量和位置施予限制。一个就是必须要在不同的TiKV实例上放置至少三个Region副本。PD通过Heartbeat从服务器收集信息、监控服务器负载，并将热Region进行转移。</p><p>另一方面，维护大量Region涉及Heartbeat信息和元数据管理导致的大量的网络和存储开销，会被PD根据负载情况调整心跳频率。</p><h4 id="Dynamic-Region-Split-and-Merge"><a href="#Dynamic-Region-Split-and-Merge" class="headerlink" title="Dynamic Region Split and Merge"></a>Dynamic Region Split and Merge</h4><p>这里主要设计Region的拆分和合并。热点Region或者大型Region会被拆成小Region，小或者访问少的Region，会被合并成大Region，以减少由于维护小Region心跳和元数据带来的网络和CPU开销。</p><p>PD操作Region，是通过向TiKV发送拆分和合并指令，然后以Raft流程来完成更新请求。Region拆分比较简单，只需要更改元数据。合并的话，PD会移动两个Region的副本，放到单独的服务器上。然后通过两阶段操作，在每台服务器上本地合并两个Region的并置副本：即停止其一Region的服务并将其与另一Region合并。</p><h3 id="Column-based-Storage-TiFlash"><a href="#Column-based-Storage-TiFlash" class="headerlink" title="Column-based Storage (TiFlash)"></a>Column-based Storage (TiFlash)</h3><p>考虑到TiKV中的行存数据并不适合OLAP，因此将列存储TiFlash合并到TiDB中。TiFlash由Learner节点组成，仅从Raft组接收Raft日志，并将行格式的元祖转换为列存数据。</p><p>用户可以通过SQL语句为表设置列格式副本，<code>ALTER TABLE x SET TiFLASH REPLICA n;</code>其中x是表名，n是副本数量。在TiFlash中，每个表会按partitions进行划分，每个partitions包括几个连续Regions，更大的Regions便于范围扫描。</p><p>当初始化一个TiFlash实例时，相关Region的Leader开始讲数据复制到新的Learner。一旦初始化完成后，TiFlash开始监听Raft组的更新。Learner收到日志后，会将日志应用到本子状态机，包括日志重放、转换数据格式和更新本地存储中的引用值。</p><h4 id="Log-Replayer"><a href="#Log-Replayer" class="headerlink" title="Log Replayer"></a>Log Replayer</h4><p>由于在Raft中，Learner接收到的日志时可线性化的，因此重放日志也会按照FIFO的策略重放日志，具体步骤包括：</p><ul><li>压缩日志：事务日志分为三种状态：预写、提交或回滚。回滚的日志不需要写盘；</li><li>解码元组：缓冲区中的日志被解码为行格式的元组，去除关于事务的冗余信息；</li><li>转换数据格式：行元组会被转换为列数据；</li></ul><p><img src="https://pic.imgdb.cn/item/60fd8be25132923bf8275779.png"></p><p>更具体的步骤可以参考上图。</p><h4 id="Schema-Synchronization"><a href="#Schema-Synchronization" class="headerlink" title="Schema Synchronization"></a>Schema Synchronization</h4><p>为了实时将元组转换为列格式，Learner需要知道最新的schema，因此TiFlash会通过Schema syncer与TiKV中最新的Schema同步。同时为了减少TiFlash向TiKV的请求次数，每个Learner都会维护一个schema cache。这里采取两阶段策略：</p><ul><li>Regular synchronization：定期同步；</li><li>Compulsive synchronization：synced检测到不匹配的schema，就会主动从TiKV拉去最新的Schema；</li></ul><h4 id="Columnar-Delta-Tree"><a href="#Columnar-Delta-Tree" class="headerlink" title="Columnar Delta Tree"></a>Columnar Delta Tree</h4><p>TiFlash设计了一个新的列存储引擎——Delta Tree，该引擎支持快速追加增量更新，然后将其与每个Partitions的稳定版本合并。如下图所示，在Stable space中，Partitions以chunks的形式存储，每个chunk都覆盖了一个较小的元组范围。TiFlash将元组的列数据及其元数据存储到不同的文件中，以同时更新文件。</p><p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf82635fd.png"></p><p>新进来的增量更新时插入或者删除数据的原子批处理，这些增量会缓存到内存中，并持久化道磁盘。另外TiFlash会定期将小增量压缩成大增量，并持久化。传入增量的内存副本有助于读取最新数据。</p><p>另外，读取的时候由于需要合并增量文件和稳定空间中的数据，而且增量文件本身也可能存在空间放大的问题。TiFlash需要定期将增量合并到稳定空间中，每个增量文件及其相关chunks被读入内存进行合并，合并后的chunks自动替换磁盘中的原始chunks。</p><p>由于相关的键在增量空间中是无序的，合并成本较高，并且也会影响合并读的速度，因此会在增量空间构建B+ Tree索引，每个增量更新项都被插入到 B+ Tree 中，并按其关键字和时间戳进行排序。便于快速响应读请求，和有序数据也更易与稳定块合并。</p><h4 id="Read-Process"><a href="#Read-Process" class="headerlink" title="Read Process"></a>Read Process</h4><p>与Follower read类似，Learner提供snapshot isolation，在接收到读取请求后，Learner向其Leader发送read index请求，以获取涵盖所请求时间戳的最新数据。Learner拿到日志后，回放日志，写入Delta Tree，就可以读到特定数据响应请求了。</p><h2 id="HTAP-ENGINES"><a href="#HTAP-ENGINES" class="headerlink" title="HTAP ENGINES"></a>HTAP ENGINES</h2><p>为了提供OLAP和OLTP查询，TiDB引入了SQL引擎来为了查询计划做决策。SQL引擎使用Percolator模型来实现分布式集群的乐观和悲观锁，并基于优化器、索引和下推算子来支持OLAP查询。</p><h3 id="Transactional-Processing"><a href="#Transactional-Processing" class="headerlink" title="Transactional Processing"></a>Transactional Processing</h3><p>TiDB为ACID事务提供了snapshot isolation语义或者repeatable read语义，前者允许每个请求读取版本一致的数据，后者则是事务中的不同语句可能为同一个key读取不同的值，但重复读取将会读取到相同的值。这是基于MVCC实现。</p><p>如下图，TiDB中的事务由SQL引擎、TiKV和PD三个组件共同完成：</p><p><img src="https://pic.imgdb.cn/item/60fd8ba35132923bf8263619.png"></p><p>TiDB对于悲观锁和乐观锁的实现启发自Percolator模型。</p><ol><li>client接收到Begin命令后，SQL引擎向PD请求一个start_ts时间戳；</li><li>SQL引擎从TiKV读取数据并在本地内存中执行SQL DMLs。TiKV提供在start_ts之前最近的commit_ts数据；</li><li>SQL引擎收到client的commit命令后，启动2PC协议。随机选择一个主键，并行锁定所有的key，并将prewrite发送TiKV节点；</li><li>如果所有的prewrite都成功了，SQL引擎会向PD请求一个commit_ts，并向TiKV发送commit命令。TiKV commit主键，并响应成功回到SQL引擎；</li><li>SQL引擎向Client响应成功；</li><li>SQL引擎向TiKV发送commit命令，异步并行地提交从键和清除锁；</li></ol><p>悲观事务和乐观事务的区别在于获取锁的实际，前者是在第二步执行DMLs的时候获取，后者则是第三步prewrite的时候。</p><h3 id="Analytical-Processing"><a href="#Analytical-Processing" class="headerlink" title="Analytical Processing"></a>Analytical Processing</h3><p>TiDB通过两阶段查询优化来实现优化器：rule-based optimization生成逻辑计划， cost-based optimization将逻辑计划转换为物理计划。由于TiDB有两类存储、因此扫描表往往有三种选项：扫描TiKV的行格式表、扫描TiKV中有索引的表和骚婊TiFlash的列。</p><p>索引可以提高点查询和范围查询的性能，TiDB实现了可扩展性的索引，由于维护索引会消耗大量资源，因此会在后台以非同步的形式构建或删除索引。索引是以与数据相同的方式按Regions分割，并作为键值存储在TiKV 中。唯一键索引上的索引项编码为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: &#123;table&#123;tableID&#125; index&#123;indexID&#125; indexedColValue&#125;</span><br><span class="line">Value: &#123;rowID&#125;</span><br></pre></td></tr></table></figure><p>非唯一索引上的索引项被解码为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: &#123;table&#123;tableID&#125; index&#123;indexID&#125; indexedColValue rowID&#125;</span><br><span class="line">Value: &#123;null&#125;</span><br></pre></td></tr></table></figure><p>物理计划的执行是由SQL引擎层使用pulling iterator model进行的，通过将部分计算下推到存储层，可以进一步优化执行。在存储层，执行计算的组件称为coprocessor，coprocessor在不同的服务器上并行执行substrees of an execution 破烂，这减少了必须从存储层发送到引擎层的元组数量。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要是介绍了一个投入生产环境的HTAP数据库——TiDB。TiDB建立在TiKV上，这是一个基于Raft的分布式行式存储，并引入一个TiFlash组件用于实时分析，作为Raft的learner从TiKV异步复制日志，并将行格式的数据转换为列格式。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;TIDB&quot;&gt;&lt;a href=&quot;#TIDB&quot; class=&quot;headerlink&quot; title=&quot;TIDB&quot;&gt;&lt;/a&gt;TIDB&lt;/h1&gt;&lt;h2 id=&quot;ABSTRACT&quot;&gt;&lt;a href=&quot;#ABSTRACT&quot; class=&quot;headerlink&quot; title=&quot;A</summary>
      
    
    
    
    
    <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>SuRF: Range Query Filter</title>
    <link href="http://yoursite.com/2021/07/18/SuRF-Range-Query-Filter/"/>
    <id>http://yoursite.com/2021/07/18/SuRF-Range-Query-Filter/</id>
    <published>2021-07-18T15:26:19.000Z</published>
    <updated>2021-07-18T15:27:01.007Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SuRF"><a href="#SuRF" class="headerlink" title="SuRF"></a>SuRF</h1><blockquote><p>本文介绍了一种SuRF的数据结构实现，用以替代传统的布隆过滤器，支持单点查询和范围查询使用。</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>LSM树是市面上数据库常用的底层存储引擎，能提供快速写和一定速度的读取。但该设计的一个问题是由于SSTable的多层设计导致大量磁盘IO读取，由此引入了布隆过滤器作为内存数据结构来帮助查询。布隆过滤器对于单点查询很有用，但并不能处理范围查找，尽管也有一些类似的设计（如前缀布隆过滤器）做了优化，但总体不够灵活通用。</p><p>因此本文提出了Succinct Range Filter（下文简称SuRF），这是一种快速紧凑的过滤器，提供了精确匹配过滤、范围过滤和近似范围计数等功能。SuRF是建立在Fast Succinct Trie（FST）这一新型空间高效简洁的数据结构上，FST每个trie节点仅需要10bit。文章使用SuRF替代了RocksDB的布隆过滤器，这将范围查询的速度提高了1.5倍到5倍，但极端情况下会导致单点查询变慢40%。</p><h2 id="FAST-SUCCINCT-TRIES"><a href="#FAST-SUCCINCT-TRIES" class="headerlink" title="FAST SUCCINCT TRIES"></a>FAST SUCCINCT TRIES</h2><p>SuRF的核心数据结构是FST，这是一种空间优化的静态tire，可以做单点查询和范围查询，其设计基于以下的思路：一个tire的上层节点较少，但访问量很大；下层节点虽多，但访问不算频繁。因此FST使用了基于位图的快速编码方案（LOUDS-Dense）来对上层节点编码，下层则用LOUDS-Sparse方案来编码，节省空间。</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>如果一颗树所占用的空间接近于信息论的下限，则该树可以认为是succinct。论文使用的是一种叫Level-Ordered Unary Degree Sequence的技术，LOUDS以广度优先的顺序遍历节点，并使用一元编码Unary coding对每个节点的度进行编码，例如下图节点3有三个字节点，因此被编码成”1110”。</p><p><img src="https://pic.imgdb.cn/item/60f447735132923bf826bdba.png"></p><p>编码完成后，该树会变成一组bit序列，需要访问节点时，则使用rank＆select两种操作：</p><ul><li>rank1(i): 返回[0, i]位置区间中, 1的个数；</li><li>select1(i): 返回第i个1的位置(整个bit序列)；</li><li>对应的则是rank0(i)和select0(i)操作；</li></ul><p>如今关于rank＆select的实现会使用查找表预算存取计算好的结果，保证查询时可以在常数时间里完成。有了rank＆select的支持，LOUDS就可以在常数时间内实现SuRF需要的点查询和范围查询。假设节点代号和bit位置都是从0开始的：</p><ul><li>在bit序列中第i个节点的位置：<code>select0(i)+1</code>；</li><li>在bit序列中从p开始的节点的第k个子节点：<code>select0 (rank1 (p + k)) + 1</code>；</li><li>始于p的节点的父节点位置：<code>select1 (rank0 (p))</code>；</li></ul><p><img src="https://pic.imgdb.cn/item/60f447735132923bf826bdda.png"></p><h3 id="LOUDS-Dense"><a href="#LOUDS-Dense" class="headerlink" title="LOUDS-Dense"></a>LOUDS-Dense</h3><p>LOUDS-Dense使用三个bit map和一个value的字节序列对每个trie节点按层级进行编码：</p><ul><li>D-Labels：记录每个节点的分支标签；上图中根节点具有标记为f，s和t的三个分支。 D-Labels位图设置第102位（f），115位（s）和116位（t）并清除其余位；</li><li>D-HasChild：表示节点是叶子节点还是中间节点，以上图根节点为例，f和t都有子节点，但s没有，所以102和106两个bit会设置为1；</li><li>D-IsPrefixKey：标记当前前缀是否为有效key；以上图根节点为例，f既作为前缀，同时也是trie里的有效key；</li><li>D-Values : 存储的是固定大小的 value，本文中则是三种后缀的指针；</li></ul><p>用rank&amp;select操作trie树，则是：</p><ul><li>第一个孩子节点：<code>D-ChildNodePos(pos)=256 × rank1(D-HasChild, pos)</code>；</li><li>父节点：<code>DParentNodePos(pos)=select1(D-HasChild,[pos / 256])</code>；</li></ul><h3 id="LOUDS-Sparse"><a href="#LOUDS-Sparse" class="headerlink" title="LOUDS-Sparse"></a>LOUDS-Sparse</h3><p>如上图所示，LOUDS-Sparse使用四个字节或者bit序列对trie节点进行编码，然后将编码的节点按层次顺序进行串联。</p><ul><li>S-Labels：记录分支标签，按level order顺序记录所有节点的label，并且使用0xFF($)来标记该key同时也是key节点；</li><li>S-HasChild：使用一个bit记录节点的label是否含有分支子节点；</li><li>S-LOUDS：使用一个bit记录每个label是否为该节点的第一个label；</li><li>S-Values：与上面类似；</li></ul><p>用rank&amp;select操作trie树，则是：</p><ul><li>孩子节点：<code>S-ChildNodePos(pos) = select1(S-LOUDS, rank1(S-HasChild, pos) + 1);</code>；</li><li>父节点：<code>S-ParentNodePos(pos) = select1(S-HasChild, rank1(S-LOUDS, pos) - 1)</code>；</li></ul><h3 id="LOUDS-DS-and-Operations"><a href="#LOUDS-DS-and-Operations" class="headerlink" title="LOUDS-DS and Operations"></a>LOUDS-DS and Operations</h3><p>LOUDS-DS就是一种混合trie，上层用LOUDS-Dense编码，下层使用LOUDS-Sparse编码。其中的分界点则根据性能和空间进行调整，LOUDS-Dense-Size(l)表示从0到l（不包含l）层采用LOUDS-Dense编码，而LOUDS-Sparse-Size(l)则表示从l到H层采用LOUDS-Sparse方式编码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOUDS-Dense-<span class="built_in">Size</span>(l) × R ≤ LOUDS-Sparse-<span class="built_in">Size</span>(l) <span class="comment">// 通常使用R = 64作为默认值，R越小，性能越好但空间使用更多</span></span><br></pre></td></tr></table></figure><p>LOUDS-DS支持三个基本操作：</p><ul><li>ExactKeySearch(key)：如果key存在，则返回key的值（否则返回NULL）；</li><li>LowerBound(key)：返回一个迭代器，该迭代器指向的键-值对(k, v)，其中k是按字典顺序的满足k≥key的最小的键；</li><li>MoveToNext(iter)：将迭代器移至下一个键值；</li></ul><p>对于点查询，则是先在LOUDS-Dense上查询，查不到即到LOUDS-Sparse查询。对于范围查询，则是执行LowerBound，找到最小的满足k≥key的键后，则光标将从当前的叶子标签位置开始并向前移动，就是正常的tree搜索方式。</p><h3 id="Space-and-Performance-Analysis"><a href="#Space-and-Performance-Analysis" class="headerlink" title="Space and Performance Analysis"></a>Space and Performance Analysis</h3><p>考虑到在LOUDS-DS中LOUDS-Sparse的节点更多，假设这是一个具备n个节点的trie，其中会有8n位用于S标签，2n位用于S-HasChild和S-LOUDS，总共10n位。</p><p>对于点查询，在每个LOUDS-Dense级别上进行搜索都需要两次数组查找，以及对位向量D-HasChild的rank操作。因此，主要操作是对所有位向量进行rank和 select，并在LOUDS-Sparse层进行标签搜索。</p><h3 id="Optimizations"><a href="#Optimizations" class="headerlink" title="Optimizations"></a>Optimizations</h3><p>论文针对LOUDS-DS中的三个关键操作：rank、select和label search进行了优化。</p><p><img src="https://pic.imgdb.cn/item/60f447735132923bf826be06.png"></p><ul><li>Rank</li></ul><p>上图展示了一个简洁的Rank结构，将bit vector分割成B bits大小的块，每个块用32bits的字段预先计算好的到这个block的rank值。对于一个pos来说，就有<code>rank1(pos) = LUT[i / B] + popcount[i / B * B, i]</code>。popcount是内置的CPU指令，可以快速计算出某一段区间1的个数。</p><ul><li>Select</li></ul><p>同样是使用LUT的方法，预先计算好值。假设采样周期是3，上图中第三个LUT保存的就是3x2，也就是第6个1的pos值，即8。那就有<code>select1(i) = LUT[i / S] + (selecting (i - i / S * S)th set bit starting from LUT[i / S] + 1) + 1</code>。</p><ul><li>Label Search</li></ul><p>使用SIMD指令在LOUDS-Sparse中执行标签搜索。</p><ul><li>Prefetching</li></ul><p>在切换到LOUDS-DS中的不同位&#x2F;字节序列之前进行预取。</p><h2 id="SUCCINCT-RANGE-FILTERS"><a href="#SUCCINCT-RANGE-FILTERS" class="headerlink" title="SUCCINCT RANGE FILTERS"></a>SUCCINCT RANGE FILTERS</h2><p>基于FST构建SuRF，最重要的是在false positive rate和filter所需要的内存使用之间取得平衡。论文的做法是使用裁剪trie，通过截断低层次的trie，并使用从key获得的后缀位（key本身或者key的哈希值）做替代。论文介绍了4种不同的trie树裁剪方式。</p><p><img src="https://pic.imgdb.cn/item/60f447735132923bf826be2f.png"></p><ol><li>Basic SuRF</li></ol><p>Basic SuRF的基本思路就是只存储key的共有前缀和一个额外的byte，裁剪掉树的部分叶节点。Basic SuRF的FPR与key的分布有关。</p><ol start="2"><li>SuRF with Hashed Key Suffixes</li></ol><p>在Basic SuRF基础上，通过对key进行哈希计算，将hash值的n个bits存储到value中，这种方法可以降低FPR，但对范围查询没有帮助。</p><ol start="3"><li>SuRF with Real Key Suffixes</li></ol><p>SuRF-Real存储n个bits的真实key，这样同时增强了Point query和Range query，但其FPR还是要比SuRF-Hash高。</p><ol start="4"><li>SuRF with Mixed Key Suffixes</li></ol><p>SuRF-Mixed的做法是混合使用2和3两种方式，一部分是real key，另一部分是hashed key。</p><h3 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h3><p>论文总结了如何使用FST实现SuRF的基本操作。</p><ul><li>build(keyList)：根据给定的key构建fitter；</li><li>result &#x3D; lookup(k)：对k执行点查询，返回true意味着k可能存储。以上面SuRF为例，首先搜寻key，直到叶子结点。如果未到叶节点就终止了则返回false；否则计算k的相关bits与叶节点的相关bits做比较；</li><li>iter, fp_flag &#x3D; moveToNext(k)：这个操作实际上是LowerBound执行，返回满足&gt;&#x3D;k的最小key的双向迭代器；</li><li>count, low_fp_flag, high_fp_flag &#x3D; count(lowKey, highKey)：返回在[lowKey, highKey]范围内的key个数；</li></ul><h2 id="EXAMPLE-APPLICATION-ROCKSDB"><a href="#EXAMPLE-APPLICATION-ROCKSDB" class="headerlink" title="EXAMPLE APPLICATION: ROCKSDB"></a>EXAMPLE APPLICATION: ROCKSDB</h2><p>论文将SuRF与RocksDB集成在一起，以替代其Bloom过滤器。下图显示了RocksDB中Get，Seek和Count查询的执行路径。Next的核心算法类似于Seek。过滤器操作在红色框中。如果该框为虚线，则可能由于误报需要检查边界key。</p><p><img src="https://pic.imgdb.cn/item/60f447735132923bf826be57.png"></p><p>对于Get(key)，SuRF的用法与Bloom过滤器完全相同。</p><p>对于Seek(lk, hk)，RocksDB首先通过在块索引中搜索lk来决定所有级别的候选SSTable。在没有SuRF的情况下，RocksDB会检查每个候选SSTable并获取满足&gt;&#x3D;lk的最小的块。 RocksDB然后比较候选key，找到全局最小key。使用SuRF，则无需获取实际的块，RocksDB可以通过在其SuRF上执行moveToNext(lk)查询来避免每个SSTable都进行IO，从而获取每个SSTable的候选key。如果查询成功（例如，是Open Seek或K≤hk），RocksDB将从选定的SSTable中精确地获取一个包含全局最小值K的块。如果查询失败（即，K&gt;hk），则不没有读盘IO。</p><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>本文介绍了SuRF，该filter支持单点查询、范围查询和计数查询，SuRF建立在一个新的succinct数据结构上，即FST。FST的性能极高，并且SuRF本身的内存使用也是较为搞笑的，其空间使用与FPR的权衡可以通过不同数量的后缀位来调整。通过在RocksDB上替换了bloom filter的测试，显著地减少了IO并提高了范围查询的吞吐量。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SuRF&quot;&gt;&lt;a href=&quot;#SuRF&quot; class=&quot;headerlink&quot; title=&quot;SuRF&quot;&gt;&lt;/a&gt;SuRF&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本文介绍了一种SuRF的数据结构实现，用以替代传统的布隆过滤器，支持单点查询和范围查询使用。&lt;/p</summary>
      
    
    
    
    
    <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Percolator</title>
    <link href="http://yoursite.com/2021/06/20/Percolator/"/>
    <id>http://yoursite.com/2021/06/20/Percolator/</id>
    <published>2021-06-19T16:14:20.000Z</published>
    <updated>2021-06-19T16:14:52.598Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Percolator"><a href="#Percolator" class="headerlink" title="Percolator"></a>Percolator</h1><blockquote><p>本文是谷歌的经典论文，介绍了一个对大型数据集做增量处理更新的系统Percolator，谷歌用它来构建索引系统，极大地提高了处理速度。Percolator基于BigTable构建的，由于BigTable不支持跨行事务，更像是给BigTable打补丁。</p></blockquote><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>索引系统是Google Web搜索的核心系统，在应对海量索引数据时，索引创建和索引的实时更新必须要面对的挑战。Google使用Mapreduce解决了高效创建索引的问题，但MR对于实时更新的场景是不合适的，因此他们构建了一个新的增量更新系统Percolator。Percolator主要关注的是跨行事务和Notification，支持在PB级别存储库中进行随机访问，并提供强一致性的保证。</p><h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p>Percolator为了大规模的增量更新提供了两个抽象：</p><ul><li>基于随机访问库的ACID事务；</li><li>observers，一种处理增量计算的方式；</li></ul><p>每个Percolator系统包含三个二进制文件：Percolator的worker、一个BigTable的tablet服务器和一个GFS chunkserver。所有的observer都会连接到Percolator的worker中，该worker会扫描所有在BigTable中发生改变的列，然后调用observer中的回调逻辑。另外Percolator还会依赖两个服务：timestamp oracle和一个轻量级锁服务，前者通过递增的时间戳提供了快照隔离协议，后者则是依赖锁服务来搜索“dirty notification”。</p><h3 id="BigTable"><a href="#BigTable" class="headerlink" title="BigTable"></a>BigTable</h3><p>Percolator是在BigTable基础上构建，数据被组织到BigTable的行列中，元数据则存储在旁边的特殊列中，基于BigTable的接口封装了大量的API，主要目的是提供BigTable缺失的功能：多行事务和observer框架。</p><p>至于BigTable和SSTable所在的GFS的具体实现可以查看对应的论文。</p><h3 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h3><p>Percolator使用ACIS快照隔离来基于BigTable的跨行跨表事务。Percolator使用Bigtable中的timestamp，对每个数据项都存储多版本，以实现快照隔离。在一个事务中，按照某个timestamp读出来的版本数据就是一个快照，然后再用一个往后的timestamp写入新数据。快照隔离可以有效解决write- write冲突，如果事务A和B并行运行，同时往某个cell执行写操作，大部分情况下都能正常提交。任意的timestamp都代表了一个一致快照，读取一个cell仅仅需要用给出的timestamp执行BigTable查询即可。</p><p><img src="https://pic.imgdb.cn/item/60ce1787844ef46bb231edf4.png"></p><p>考虑到Percolator不能直接控制对存储介质的访问，而是需要修改BigTable的状态，所以Percolator需要明确地维护锁，以实现分布式事务。这个锁服务需要具备几个特点：高可用，能够解决锁在2PC阶段消失的情况；高吞吐，上千台机器同时请求锁；低延时，读请求需要读取锁。BigTable作为存储介质，恰好满足这些需求，所以Percolator将数据和锁存储在同一行，特殊的内存列存取锁。访问某一行数据时，Percolator将在一个BigTable行事务中同时对同行的锁进行Read and Modify。</p><p>下图是Percolator在执行事务期间，数据和元数据的布局情况。以银行转账为例，Bob向Joe转7元，该事务从<code>start_ts=7开始，commit_ts=8</code>结束。</p><p><img src="https://pic.imgdb.cn/item/60ce1787844ef46bb231ee35.png"></p><p>下图则展示了Percolator在BigTable中的列所展现的作用，其在BigTable中使用了5个列，其中3个与事务相关：</p><ul><li>c:lock：事务产生的锁，未commit的事务会写该列，映射对是{key,start_ts}&#x3D;&gt;{primary_key}；</li><li>c:write: 已commit的数据信息，映射对是{key,commit_ts}&#x3D;&gt;{start_ts}；</li><li>c:data: 具体存储的数据，映射对是{key,start_ts} &#x3D;&gt; {value}；</li></ul><p><img src="https://pic.imgdb.cn/item/60ce1787844ef46bb231ee6d.png"></p><p>事务的处理流程则是经典的两阶段提交，首先是<strong>Prewrite</strong>：</p><ul><li>client首先从Oracle获取全局唯一的时间戳start_ts；</li><li>client然后从所有key中选出一个primary，其余作为secondaries，并将所有数据写入请求并行发往存储节点；<ul><li>存储节点首先会进行write-write冲突检查，从c:write获取当前key的最新数据，如果该列中的commit_ts&gt;&#x3D;start_ts，则返回写冲突错误；</li><li>然后会检查key是否被锁，如果锁了则返回错误；</li><li>向c:lock写入{key, start_ts} &#x3D;&gt; {primary_key}；</li><li>向c:data写入{key,start_ts} &#x3D;&gt; {value}；</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// prewrite tries to lock cell w, returning false in case of conflict.</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Prewrite</span><span class="params">(Write w, Write primary)</span> </span>&#123;</span><br><span class="line">Column c = w.col;</span><br><span class="line">bigtable::Txn T = bigtable::<span class="built_in">StartRowTransaction</span>(w.row);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (T.<span class="built_in">Read</span>(w.row, c+<span class="string">&quot;write&quot;</span>, [start_ts_, max])) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">if</span> (T.<span class="built_in">Read</span>(w.row, c+<span class="string">&quot;lock&quot;</span>, [<span class="number">0</span>, max])) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">T.<span class="built_in">Write</span>(w.row, c+<span class="string">&quot;data&quot;</span>, start_ts_, w.value);</span><br><span class="line">T.<span class="built_in">Write</span>(w.row, c+<span class="string">&quot;lock&quot;</span>, start_ts_, &#123;primary.row, primary.col&#125;);</span><br><span class="line"><span class="keyword">return</span> T.<span class="built_in">Commit</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Prewrite成功后，则进入第二阶段<strong>commit</strong>：</p><ul><li>从Oracle获取全局唯一的时间戳commit_ts；</li><li>向primary key所在节点发起commit请求；</li><li>primary commit成功后则标记为事务成功了，紧接着就是向secondaries发起commit请求（事实上这里primary commit成功后，即可响应client，后续异步往secondaries发起commit即可）；</li><li>存储节点的处理：<ul><li>首先是检查key的lock是否合法；</li><li>往c:write写入{key,commit_ts}&#x3D;&gt;{start_ts}；</li><li>清除c:lock中内容，释放锁；</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Commit</span><span class="params">()</span> </span>&#123;</span><br><span class="line">Write primary = write_[<span class="number">0</span>];</span><br><span class="line"><span class="function">vector&lt;Write&gt; <span class="title">secondaries</span><span class="params">(write_.begin() + <span class="number">1</span>, write_.end())</span></span>;</span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">Prewrite</span>(primary, primary)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">for</span> (Write w : secondaries)</span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">Prewrite</span>(w, primary)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> commit_ts = oracle.<span class="built_in">GetTimestamp</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Commit primary first.</span></span><br><span class="line">Write p = primary;</span><br><span class="line">bigtable::Txn T = bigtable::<span class="built_in">StartRowTransaction</span>(p.row);</span><br><span class="line"><span class="keyword">if</span> (!T.<span class="built_in">Read</span>(p.row, p.col+<span class="string">&quot;lock&quot;</span>, [start_ts_, start_ts_]))</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// aborted while working</span></span><br><span class="line">T.<span class="built_in">Write</span>(p.row, p.col+<span class="string">&quot;write&quot;</span>, commit_ts, start_ts_); <span class="comment">// Pointer to data written at start_ts_</span></span><br><span class="line">T.<span class="built_in">Erase</span>(p.row, p.col+<span class="string">&quot;lock&quot;</span>, commit_ts); <span class="comment">// 应该是start_ts_</span></span><br><span class="line"><span class="keyword">if</span>(!T.<span class="built_in">Commit</span>()) <span class="keyword">return</span> <span class="literal">false</span>;  <span class="comment">// commit point</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Second phase: write our write records for secondary cells.</span></span><br><span class="line"><span class="keyword">for</span> (Write w:secondaries) &#123;</span><br><span class="line">bigtable::<span class="built_in">write</span>(w.row, w.col+<span class="string">&quot;write&quot;</span>, commit_ts, start_ts_);</span><br><span class="line">bigtable::<span class="built_in">Erase</span>(w.row, w.col+<span class="string">&quot;lock&quot;</span>, commit_ts);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Percolator的读取操作则相对简单，由于c:write记录了key的commit记录，client读取key的时候会先从c:write找到start_ts_，然后到c:data查找相对应的数据，具体流程：</p><ul><li>检查[0, start_ts_]内是否存在锁，若存在，则意味着有未commit的事务，client则必须进行等待和cleanup操作；</li><li>否则，获取最新的commit记录，从c:write中获取start_ts；</li><li>根据{key, start_ts}从c:data中获取数据；</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Get</span><span class="params">(Row row, Column c, string* value)</span> </span>&#123;</span><br><span class="line"><span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">bigtable::Txn = bigtable::<span class="built_in">StartRowTransaction</span>(row);</span><br><span class="line"><span class="comment">// Check for locks that signal concurrent writes.</span></span><br><span class="line"><span class="keyword">if</span> (T.<span class="built_in">Read</span>(row, c+<span class="string">&quot;locks&quot;</span>, [<span class="number">0</span>, start_ts_])) &#123;</span><br><span class="line"><span class="comment">// There is a pending lock; try to clean it and wait</span></span><br><span class="line"><span class="built_in">BackoffAndMaybeCleanupLock</span>(row, c);</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Find the latest write below our start_timestamp.</span></span><br><span class="line">latest_write = T.<span class="built_in">Read</span>(row, c+<span class="string">&quot;write&quot;</span>, [<span class="number">0</span>, start_ts_]);</span><br><span class="line"><span class="keyword">if</span>(!latest_write.<span class="built_in">found</span>()) <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// no data</span></span><br><span class="line"><span class="type">int</span> data_ts = latest_write.<span class="built_in">start_timestamp</span>();</span><br><span class="line">*value = T.<span class="built_in">Read</span>(row, c+<span class="string">&quot;data&quot;</span>, [data_ts, data_ts]);</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至于事务处理过程中如何应对异常：若commit一个事务时出现了异常，导致前面Prepare阶段的锁留下来，为避免阻塞住后来的事务，Percolator采取lazy的方式清理这些锁，即访问到了这个key才会去处理。</p><p>Prewrite阶段遇到锁冲突会直接返回失败，因此锁的清理是在读阶段进行的。当事务执行过程中commit失败时，事务会留下一个commit point（Primary Key写入c:write了），但可能留下一些锁没有清理。另一个事务发现锁冲突时，会去Primary上查找primary lock是否存在。如果存在，说明前面的事务没有提交，进行roll back；如果不存在，则需要检查c:write是否已经被写入，写入了就说明事务已经被成功提交，此时执行Roll Forward（在secondaries上将c:lock替换成c:write）。BigTable的行级事务避免了数据竞争。</p><h3 id="Timestamps"><a href="#Timestamps" class="headerlink" title="Timestamps"></a>Timestamps</h3><p>时间戳oracle是一个分配严格递增时间戳的服务器，考虑到每个事务需要调用oracle两次，因此oracle需要具备很好的可扩展性。oracle会定期分配一定范围的时间戳，并把该范围的最大值持久化存储，这样如果服务器挂了就直接从上次范围的最大值作为开始值进行分配。为了减少RPC消耗，Percolator的worker会维持一个长连接RPC到oracle，低频批量地获取时间戳。</p><p>事务协议使用严格递增的时间戳，保证了Get操作能看到所有在start_ts之前已提交的写操作。</p><h3 id="Notifications"><a href="#Notifications" class="headerlink" title="Notifications"></a>Notifications</h3><p>Percolator提供了一种方法来触发和运行事务，用户编写的代码即observer会表的变化而触发，observer会被放入Percolator worker中，随着每一个tablet服务器运行。每个observer都会向Percolator注册一个function和它感兴趣的列，一旦这些列发生了变化就会调用function。</p><p>与数据库中的触发器不一样，假设写操作触发了observer，但他们会运行在各自的事务中，产生的结果不是原子的。Percolator提供了一种保证：对于一个被观察列的变化，至多一个observer的事务被提交。反之则不然，对于一个被观察列的多次变化，可能只会触发一次observer事务。</p><p>为了实现通知机制，Percolator为每个被监测的列额外提供一个“acknowledgment”列。包含最近一次observer事务的开始时间戳。当被监测的列发生改变时，Percolator启动一个事务来处理通知，该事务读取被监测列和它对应的acknowledgment列，判断acknowledgment列的时间戳是否在被检测列之前，若是则意味着可以开启observer事务，否则意味着已经有observer被运行了。</p><p>为了实现通知机制，Percolator需要高效找到被观察的脏cell，其在BigTable的locality group维护了一个特殊的“notify”列，表示该cell是否为脏，当一个事务对被监测列进行写入时，同时会写对应的notify cell。每个Percolator的worker指定几个线程负责扫描这些脏cell。 </p><p>Percolator的通知机制主要是异步实现的，当改变发生时，并不是立刻以同步方式调用observer，而是写入一个notify列，等worker线程扫描到才会调用observer。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Percolator的一大特点就是构建在仅支持单行事务的BigTable之上，提供了良好的跨行事务，实现了比较简洁的分布式事务。但其性能本身不够高效，每个work都需要发送大量的RPC（比如获取两次事务timestamp，比如可能读取secondary的lock列是指向primary的，还要多读取一次），虽然论文提到了一些合并RPC，延迟发送，提高并行和增大BatchSize等措施来优化RPC的调用，但Percolator对于写协议本身也要需要多次在BigTable做持久化，读的话则可能遇到由于先写primary再同步到其他参与者导致的锁被持有而等待的问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Percolator&quot;&gt;&lt;a href=&quot;#Percolator&quot; class=&quot;headerlink&quot; title=&quot;Percolator&quot;&gt;&lt;/a&gt;Percolator&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本文是谷歌的经典论文，介绍了一个对大型数据集做增量</summary>
      
    
    
    
    
    <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>《UCB cs294》Required Reading 3</title>
    <link href="http://yoursite.com/2021/05/15/%E3%80%8AUCB-cs294%E3%80%8BRequired-Reading-3/"/>
    <id>http://yoursite.com/2021/05/15/%E3%80%8AUCB-cs294%E3%80%8BRequired-Reading-3/</id>
    <published>2021-05-15T15:41:29.000Z</published>
    <updated>2021-05-15T15:43:47.748Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《UCB-cs294》Required-Reading-3"><a href="#《UCB-cs294》Required-Reading-3" class="headerlink" title="《UCB cs294》Required Reading 3"></a>《UCB cs294》Required Reading 3</h1><h2 id="论文一"><a href="#论文一" class="headerlink" title="论文一"></a>论文一</h2><p>《Hidden Technical Debt in Machine Learning Systems》这篇文章是谷歌基于多年的机器学习系统开发和使用经验总结出来的，着重强调了在机器学习系统中出现的technical debt：</p><ul><li>Complex Models Erode Boundaries</li></ul><p>在一般的软件开发中，人们会使用封装、模块化等抽象手段，但机器学习系统由于其依赖大量外部数据，特征的改变会影响全局。论文提到的解决方法是模型隔离和监测模型中的变化。</p><ul><li>Data Dependencies Cost More than Code Dependencies</li></ul><p>这个主要是将对数据依赖关系的分析成本比通常的代码依赖关系分析要高，一是因为数据依赖关系不够稳定，可能存在把一个输出当成另一个地方外部输入的使用；二是存在不必要的数据依赖。对于这两个问题，论文的解决方法是做数据输入的版本控制和定期检查去除不必要的依赖。</p><ul><li>Feedback Loops</li></ul><p>机器学习系统的一大特征就是他们未来的更新很可能会影响到自身，导致analysis debt的出现，在模型部署之前很难预测模型的行为。Direct Feedback Loops是指模型直接影响自身的特征选择，Hidden Feedback Loops是指系统之间间接影响对方，这种Feedback Loops的问题更加严重。</p><ul><li>ML-System Anti-Patterns</li></ul><p>在实际的机器学习系统中，仅仅一小部分代码是用于学习和预测的，慢慢地，系统中会出现各种系统设计的Anti-Patterns。比如出现大量的机器学习库代码，论文的建议是使用胶水语言封装API；数据准备阶段积攒了更多的输入信号，混杂着各种数据操作；机器学习实验过程中，代码出现了一些不必要的条件分支，导致技债出现；还有就是一些代码的code smell比较差。</p><ul><li>Configuration Debt</li></ul><p>由于机器学习系统和算法比较复杂，大型的ML系统往往依赖大量的配置，因此需要关注配置的可维护性、易读性、需要做code review提交到库中。</p><ul><li>Dealing with Changes in the External World</li></ul><p>这个指的是ML系统与外部世界有较多的交互，外部世界的变化会影响系统、影响模型等。因此需要高效的监控预警配套。</p><ul><li>Other Areas of ML-related Debt</li></ul><p>Data Testing Debt，提供基本的、完整的代码测试；Reproducibility Debt，真实的ML系统由于外部世界的变化、并行学习中的随机等等难以保持严格的可重复性；Process Management Debt，模型的管理问题，真实的系统可能存在数以百计的模型，如何分配资源、控制优先级等很重要；Cultural Debt，文化问题，研究员和系统工程师可能存在沟通不当的情况。</p><h2 id="论文二"><a href="#论文二" class="headerlink" title="论文二"></a>论文二</h2><p>《TFX: A TensorFlow-Based Production-Scale Machine Learning Platform》这篇论文主要介绍了Google开发的一个机器学习平台TFX。TFX最大的特点就是将机器学习所需的各个组件部分集成在一起，提供训练模型、分析验证模型和模型部署的完整工作流，避免了机器学习pipeline各个部分的割裂。</p><h3 id="platform-overview"><a href="#platform-overview" class="headerlink" title="platform overview"></a>platform overview</h3><p>作为一个机器学习平台，不单单只关注机器学习算法，还需要考虑到依赖分布式系统架构促使数据和模型的并行，机器学习工作流便于搭建，拥有集中的仓库跟踪保存训练过的多模型等等。</p><p>TFX的设计主要考虑了以下几点：</p><ul><li>平台能应对多种学习任务，除了选用tensorflow作为核心算法库，还支持数据验证分析和可视化工具、模型验证评估和推断工具等；</li><li>持续训练，TFX考虑支持多种持续训练策略；</li><li>易用的配置与工具；</li><li>生产级别的可靠性与可扩展性；</li></ul><p>基于上述的特点，把多种组建模块集成在一起，就形成了下图的平台。</p><p><img src="https://pic.imgdb.cn/item/609ab7ced1a9ae528fce25e3.png"></p><h3 id="DATA-ANALYSIS-TRANSFORMATION-AND-VALIDATION"><a href="#DATA-ANALYSIS-TRANSFORMATION-AND-VALIDATION" class="headerlink" title="DATA ANALYSIS, TRANSFORMATION, AND VALIDATION"></a>DATA ANALYSIS, TRANSFORMATION, AND VALIDATION</h3><p>对于机器学习来说，了解数据并及时发现异常数据是至关重要的，有利于避免下游数据出错。这一章主要讲TFX将数据分析、转换和验证作为独立又相互关联的部分。</p><p>数据分析的时候，需要对输入的数据集进行统计，输出一系列统计数据，如连续型数据需要分位数、直方图等等，离散数据需要top-K值和频率等等。</p><p>数据转换则是对数据进行格式转换，如将特征转换为特定的整数。这里的关键是保证在训练和推断期间确保转换逻辑的一致性。TFX会将任何数据转换导出为经过训练的模型的一部分，从而避免了不一致的问题。</p><p>数据验证则是使用schema来描述数据规范，每个算法团队维护自己的schema，数据验证时可以快速确认数据集的异常情况，如何进行修正，反映出数据的变化情况。</p><h3 id="MODEL-TRAINING"><a href="#MODEL-TRAINING" class="headerlink" title="MODEL TRAINING"></a>MODEL TRAINING</h3><p>TFX的设计一大核心是尽可能流水线地、自动化地完成训练生产模型，支持训练使用Tensorflow配置的所有模型。TFX使用了warm starting来在模型质量和模型时效性之间达到一个平衡，这是迁移学习使用的技术，讲一个训练好的基准模型应用另一个场景。</p><p>论文另外一点就是TFX用了定义和描述模型的API——model specification API，通过对Tensorflow的封装减少代码冗余，提高开发速度。</p><h3 id="MODEL-EVALUATION-AND-VALIDATION"><a href="#MODEL-EVALUATION-AND-VALIDATION" class="headerlink" title="MODEL EVALUATION AND VALIDATION"></a>MODEL EVALUATION AND VALIDATION</h3><p>模型的评估是模型上线前验证模型有效性的关键步骤，TFX对于好模型的定义主要是：<strong>safe to serve</strong>和<strong>the desired prediction quality</strong>，前者关心的是模型完整性，不会使得推断服务crash，资源使用少，后者则主要是模型预测准确率。</p><p>TFX一方面也使用了各种准确率标准来做AB Test，根据不同的产品团队要求提供告警配置，另一方面也支持对数据集根据feature做slice切分，帮助更好地评估模型在不同feature伤的表现。</p><h3 id="MODEL-SERVING"><a href="#MODEL-SERVING" class="headerlink" title="MODEL SERVING"></a>MODEL SERVING</h3><p>最后则是模型服务，TFX主要依赖Tensorflow serving去做这个事情，通过多用户隔离和快速的训练数据反序列化来满足系统的低时延和高性能要求，总的来说提供了一套工业级的模型上线推断服务。</p><h2 id="论文三"><a href="#论文三" class="headerlink" title="论文三"></a>论文三</h2><p>《Towards Unified Data and Lifecycle Management for Deep Learning》这篇论文主要关注了深度学习中数据和生命周期管理系统的实现，提出了一个modelHub系统，包括了三个部分，一种DSL来帮助泛化对模型的探索、查询，一种新的模型版本管理系统（Dlv）和一种读参数优化的参数归档存储系统（PAS）。</p><p><img src="https://pic.imgdb.cn/item/609febea6ae4f77d357d697a.png"></p><p>ModelHub主要分为local组件和远程组件，local functionality包括了一些DNN系统与本地计算机集群的集成，remote functionality则是不同用户组共享模型与其版本。</p><p>DLV是一种通过命令行工具的版本控制系统，可以用来与其他本地或者远程组件进行交互，替代了传统的git&#x2F;svn可以方便其更好地描述查询建模过程中生成的artifacts内部结构。另外通过DQL模块可以帮助研究员开发新模型。Model Learning模块本质是特定DNN系统的wrapper。</p><p>至于本地仓库的PAS则是用来存储大量的模型学习参数，PAS的目标是在不影响查询性能的情况下，尽可能维护大量学习的模型信息。其中一大特点是使用了一种新的近似模型评估技术，适用于分段存储PAS。由于浮点算数表示中浮点数具备高熵，难以压缩，PAS提供了按字节分割的浮点矩阵存储方案，通过分割高阶和低阶尾数位，浮点数矩阵按块存储，第一个块由8个高位组成，其余的每个块被分割为一个字节，由于高阶位具有低熵，能更好地做压缩。</p><p>这篇论文描述了如何通过ModelHub去解决一些在管理和调整深度学习模型中的关键数据管理挑战：</p><ul><li>通过调整网络结构和超参数，更容易优化潜在的模型效果；</li><li>减少跟踪模型的负担；</li><li>在不影响查询和检索性能的前提下，尽可能多地存储大量模型和构造快照；</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《UCB-cs294》Required-Reading-3&quot;&gt;&lt;a href=&quot;#《UCB-cs294》Required-Reading-3&quot; class=&quot;headerlink&quot; title=&quot;《UCB cs294》Required Reading 3&quot;&gt;&lt;/a</summary>
      
    
    
    
    
    <category term="AiSys" scheme="http://yoursite.com/tags/AiSys/"/>
    
  </entry>
  
  <entry>
    <title>C++ atomics, from basic to advanced</title>
    <link href="http://yoursite.com/2021/05/12/C-atomics-from-basic-to-advanced/"/>
    <id>http://yoursite.com/2021/05/12/C-atomics-from-basic-to-advanced/</id>
    <published>2021-05-11T17:00:54.000Z</published>
    <updated>2021-05-11T17:01:19.589Z</updated>
    
    <content type="html"><![CDATA[<h1 id="C-atomics-from-basic-to-advanced"><a href="#C-atomics-from-basic-to-advanced" class="headerlink" title="C++ atomics, from basic to advanced"></a>C++ atomics, from basic to advanced</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在C++11中引入了对多线程的支持，同时也带来了关于mutex和atomic相关的一些列标准，定义了memory model。这篇文章将关注C++11带来的一个无锁编程工具——atomics。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="无锁编程"><a href="#无锁编程" class="headerlink" title="无锁编程"></a>无锁编程</h3><p>在文章开始之前，首先来关注一些无锁编程这个概念（lock free——不使用锁来保持代码同步）。一般人在使用无锁编程或者了解这个概念之前，会先入为主地认为无锁编程性能更快，相对使用锁来同步拥有更好的运行速度。</p><p>实际上，无论是lock free还是更严格的wait free都没有直接跟运行速度有直接关系，他们关联的是“steps”，但在程序运行过程中“step”的运行时间不一定是一样的。无锁编程的优势在于通过减少阻塞和等待来提高并发的可能性，消除race condition、死锁等潜在危机。因此在使用无锁编程之前应该先测试程序，观察代码的算法逻辑是否有问题。</p><p>接下来我们开始了解C++的原子操作。</p><h3 id="C-atomics"><a href="#C-atomics" class="headerlink" title="C++ atomics"></a>C++ atomics</h3><h4 id="何谓原子操作"><a href="#何谓原子操作" class="headerlink" title="何谓原子操作"></a>何谓原子操作</h4><p>原子操作是一种以单个事务来执行的操作，这是一个“不可再分且不可并行的”操作，其他线程只能看到操作完成前或者完成后的资源状态，不存在中间状态可视。</p><p>从底层来看，原子操作是一些硬件指令，其原子性是由硬件保证的，C++11对原子操作抽象出统一的接口，避免使用时嵌入平台相关的代码来支持跨平台使用。</p><p>先来看看如果多线程中没有原子操作会发生什么情况：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> x = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// ===============</span></span><br><span class="line"><span class="comment">// Thread 1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">int</span> tmp = x; <span class="comment">// 0</span></span><br><span class="line">  ++tmp; <span class="comment">// 1</span></span><br><span class="line">  x = tmp; <span class="comment">// 1</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Thread 2</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">int</span> tmp = x; <span class="comment">// 0</span></span><br><span class="line">  ++tmp; <span class="comment">// 1</span></span><br><span class="line">  x = tmp; <span class="comment">// 1!!! What!!!</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这是一个很典型的Read-modify-write的递增场景，多线程环境下就会出现data race。为什么会这样，以一个简易的计算机架构图来举例，这里存在三级缓存，变量在内存中初始化好为0，由于这里没有同步机制，每个CPU都从主存中将变量取出来（此时变量都是0），在寄存器中进行递增，最后将递增后的值1写回内存。</p><p><img src="https://pic.imgdb.cn/item/609ab7ced1a9ae528fce263b.png"></p><p>那么我们怎么在C++中进行数据共享呢？在C++11之前是没有标准的线程库的，在C++11之后引入了std::atomic模版类来提供原子操作。一个简单例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::atomic&lt;<span class="type">int</span>&gt; <span class="title">x</span><span class="params">(<span class="number">0</span>)</span></span>; <span class="comment">// Not support std::atomic&lt;int&gt; x = 0</span></span><br><span class="line">++x; <span class="comment">// now atomic operation</span></span><br></pre></td></tr></table></figure><h4 id="std-atomic的使用"><a href="#std-atomic的使用" class="headerlink" title="std::atomic的使用"></a>std::atomic的使用</h4><p>std::atomic是一个模版，那么哪些类型可以实例画该模版呢？按照标准的说法，需要是Trivially Copyable的类型，简单来说就是满足三个条件：</p><ul><li>连续的内存；</li><li>拷贝对象意味着按bit拷贝（memcpy）；</li><li>没有虚函数；</li></ul><p>用代码来表达则是自定义结构满足下面5个条件：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">std::is_trivially_copyable&lt;T&gt;::value</span><br><span class="line">std::is_copy_constructible&lt;T&gt;::value</span><br><span class="line">std::is_move_constructible&lt;T&gt;::value</span><br><span class="line">std::is_copy_assignable&lt;T&gt;::value</span><br><span class="line">std::is_move_assignable&lt;T&gt;::value</span><br></pre></td></tr></table></figure><p>那么对于一个合法的<code>std::atomic&lt;T&gt;</code> 类型来说，它能进行哪些操作？一个是assignment，则读写操作；另一个则是特定的原子操作和跟类型T相关的其他操作。下面几种操作要么编译失败、要么是非原子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">int</span>&gt; x&#123;<span class="number">0</span>&#125;;</span><br><span class="line">x *= <span class="number">2</span>; <span class="comment">// compile error</span></span><br><span class="line">x = x + <span class="number">1</span>; <span class="comment">// Not atomic: Atomic read followed by atomic write</span></span><br><span class="line">x = x * <span class="number">2</span>; <span class="comment">// Not atomic: Atomic read followed by atomic write</span></span><br></pre></td></tr></table></figure><p>还有一个就是原子自增不支持浮点数。其他的原子操作包括CAS、exchange等等；</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">std::atomic&lt;T&gt; x;</span><br><span class="line">T y = x.<span class="built_in">load</span>(); <span class="comment">// same sa T y = x</span></span><br><span class="line">x.<span class="built_in">store</span>(y); <span class="comment">// same as x = y</span></span><br><span class="line">T z = x.<span class="built_in">exchange</span>(y); <span class="comment">// Atomically: z = x; x = y;</span></span><br><span class="line"><span class="comment">// if x == y, make x=z and return true</span></span><br><span class="line"><span class="comment">// Otherwise, set y=x and return false</span></span><br><span class="line"><span class="comment">// 还有一个compare_exchange_week，x == y也可能会失败，主要是因为某些平台会对锁有类似超时释放的操作，满足其高效调度</span></span><br><span class="line"><span class="type">bool</span> success = x.<span class="built_in">compare_exchange_strong</span>(y, z);</span><br></pre></td></tr></table></figure><p>这里重点看一下CAS的使用，CAS在大多数无锁算法中都有应用，除了原子自增外，CAS还支持递增浮点数，进行乘法运算：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">int</span>&gt; x&#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="type">int</span> x0 = x;</span><br><span class="line"><span class="keyword">while</span> (!x.<span class="built_in">compare_exchange_strong</span>(x0, x0+<span class="number">1</span>)) &#123;&#125;</span><br><span class="line"><span class="keyword">while</span> (!x.<span class="built_in">compare_exchange_strong</span>(x0, x0*<span class="number">2</span>)) &#123;&#125;</span><br><span class="line"><span class="comment">// fetch_xxx() == some operators</span></span><br><span class="line">std::atomic&lt;<span class="type">int</span>&gt; x&#123;<span class="number">0</span>&#125;;</span><br><span class="line">x.<span class="built_in">fetch_add</span>(y); <span class="comment">// same as x += y</span></span><br><span class="line"><span class="type">int</span> z = x.<span class="built_in">fetch_add</span>(y); <span class="comment">// same as z = (x += y) - y;</span></span><br></pre></td></tr></table></figure><h4 id="std-atomic与无锁的关系"><a href="#std-atomic与无锁的关系" class="headerlink" title="std::atomic与无锁的关系"></a>std::atomic与无锁的关系</h4><p>这里有一个关键的信息：std::atomic并不意味着一定是无锁的；首先来看下面的代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">long</span> x; <span class="comment">// lock free</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span> &#123; <span class="type">long</span> x; &#125;; <span class="comment">// lock free</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span> &#123; <span class="type">long</span> x; <span class="type">long</span> y; &#125;; <span class="comment">// run-time and platfrom dependent. x86 is lock free</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">C</span> &#123; <span class="type">long</span> x; <span class="type">long</span> y; <span class="type">long</span> z; &#125;; <span class="comment">// &gt; 16 bytes. not lock free</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">D</span> &#123; <span class="type">long</span> x; <span class="type">int</span> y; &#125;; <span class="comment">// alignment 16 bytes. lock free</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">E</span> &#123; <span class="type">long</span> x; <span class="type">int</span> y; &#125;; <span class="comment">// 12 bytes. not lock free</span></span><br></pre></td></tr></table></figure><p>判断atomc是否无锁可以通过一个成员函数<code>std::atomic&lt;T&gt;::is_lock_free()</code>，这是一个运行时的判断（C++17提供了编译时判断<code>constexpr is_always_lock_free()</code>），之所以会出现无锁不确定的情况主要是因为对齐alignment。</p><p>假设atomic是无锁的，但也有可能出现两个atomic变量互相等待的情况，假设存在这样的场景，两个atomic变量：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">int</span>&gt; x[N];</span><br><span class="line"><span class="comment">// thread 1</span></span><br><span class="line">++x[<span class="number">0</span>];</span><br><span class="line"><span class="comment">// thread 2</span></span><br><span class="line">++x[<span class="number">1</span>];</span><br></pre></td></tr></table></figure><p>这种情况下就会出现两个atomic变量互相等待的可能性，主要是因为这两个操作都是在同一个cache line，都从主存到CPU来回写入，因为两个CPU可能互斥访问同一个cache line，这就是所谓的false sharing。一个提高性能解决这个问题的方式是将每个线程的数据对齐到充满整个cache line。（NUMA机器上，可能是整个page）</p><p><img src="https://pic.imgdb.cn/item/609ab7ced1a9ae528fce268d.png"></p><h4 id="memory-barrier"><a href="#memory-barrier" class="headerlink" title="memory barrier"></a>memory barrier</h4><p>memory barrier控制着某个CPU对内存的修改被另一个CPU可见的方式，这是一个对所有CPU的全局控制。这是通过硬件实现，确定指令的特定操作顺序。简单来说，就是CPU在执行指令的时候不一定按照编写顺序来执行，从而挖掘更多并行能力。</p><p>如果仔细观察std::atomic相关操作的参数，会发现其还接受一个memory_order的枚举作为参数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">enum</span> <span class="title class_">memory_order</span> &#123;</span><br><span class="line">memory_order_relaxed,</span><br><span class="line">memory_order_consume,</span><br><span class="line">memory_order_acquire,</span><br><span class="line">memory_order_release,</span><br><span class="line">memory_order_acq_rel,</span><br><span class="line">memory_order_seq_cst</span><br><span class="line">&#125; memory_order;</span><br></pre></td></tr></table></figure><ul><li>memory_order_relaxed：不对执行顺序做任何保证，即该原子操作指令可以任由编译器重排或者CPU乱序执行；</li><li>memory_order_acquire：当前线程里，所有在该原子操作之后的读操作，都不能重排到该原子操作指令之前执行。原子操作指令先读；</li><li>memory_order_release：当前线程里，所有在该原子操作之前的写操作，都不能重排到该原子操作指令之后执行。原子操作最后写；</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">Thread1</span><span class="params">(<span class="type">int</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> t=<span class="number">1</span>;</span><br><span class="line">    a.<span class="built_in">store</span>(t,memory_order_relaxed);</span><br><span class="line">    b.<span class="built_in">store</span>(<span class="number">2</span>,memory_order_release); <span class="comment">// a必须在b之前完成写入</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Thread2</span><span class="params">(<span class="type">int</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(b.<span class="built_in">load</span>(memory_order_acquire)!=<span class="number">2</span>); <span class="comment">// b必须在a之前读 </span></span><br><span class="line">    cout＜＜a.<span class="built_in">load</span>(memory_order_relaxed)＜＜endl;<span class="comment">//输出1. </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>memory_order_acq_rel：包含memory_order_acquire和memory_order_release两个标志；</li><li>memory_order_seq_cst：默认标志。顺序一致，确保代码在线程中的执行顺序与顺序看到的代码顺序一致，禁止重拍指令和乱序执行；</li></ul><p>改变memory order参数，在一定程度上可能会提高程序的性能，从代码中表达出程序员的意图。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>C++的atomic操作在一定条件下能很好提高程序的性能，并且也提高了易用性，但也存在很多容易踩坑的地方，因此在使用前仍然需要做详细的设计。使用atomic的时机也需要细细斟酌，对于不适用的地方使用无锁或者atomic操作可能收效甚微。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.infoq.com/news/2014/10/cpp-lock-free-programming/">https://www.infoq.com/news/2014/10/cpp-lock-free-programming/</a></p><p><a href="https://www.youtube.com/watch?v=ZQFzMfHIxng">https://www.youtube.com/watch?v=ZQFzMfHIxng</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;C-atomics-from-basic-to-advanced&quot;&gt;&lt;a href=&quot;#C-atomics-from-basic-to-advanced&quot; class=&quot;headerlink&quot; title=&quot;C++ atomics, from basic to a</summary>
      
    
    
    
    
    <category term="C++" scheme="http://yoursite.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Macro Free in C++</title>
    <link href="http://yoursite.com/2021/04/19/Macro-Free-in-C/"/>
    <id>http://yoursite.com/2021/04/19/Macro-Free-in-C/</id>
    <published>2021-04-18T16:43:30.000Z</published>
    <updated>2021-04-18T16:43:53.781Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Macro-Free-In-Cpp"><a href="#Macro-Free-In-Cpp" class="headerlink" title="Macro Free In Cpp"></a>Macro Free In Cpp</h1><blockquote><p>One of C++’s aims is to make C’s preprocessor redundant because I consider its actions inherently error prone</p></blockquote><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>C预处理器本质是一个文本替换工具，用来在实际编译之前进行一定的预处理操作，一般情况下#开头的预处理操作并不认为是语言本身的一部分，因为编译器永远看不到这些宏定义符号。</p><p>以C++来说，用宏的目的并不是出于性能的缘由，更多的只是为了减少重复的代码和进行条件编译。随着modern cpp的发展，越来越的新特性加入使得对宏的使用依赖进一步降低。本文将关注如何使用C++新特性替换C预处理程序。</p><h2 id="如何替代宏的使用"><a href="#如何替代宏的使用" class="headerlink" title="如何替代宏的使用"></a>如何替代宏的使用</h2><ol><li>表达式别名</li></ol><p>有一些宏定义会用在表达式别名，替换后的文本会被识别为C++表达式，对于这种情况比较简单的是使用常量表达式或者lambda替换宏，</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> PI 3.14</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SEVEN 3 + 4</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FILENAME <span class="string">&quot;header.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SUM a + b</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">summer</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> a = <span class="number">1</span>, b=<span class="number">2</span>;</span><br><span class="line">  <span class="type">int</span> c = SUM;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ========================&gt;&gt;&gt;</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">auto</span> PI = <span class="number">3.14</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">auto</span> SEVEN = <span class="number">3</span> + <span class="number">4</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">auto</span> FILENAME = <span class="string">&quot;header.h&quot;</span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">summer</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> a = <span class="number">1</span>, b=<span class="number">2</span>;</span><br><span class="line">  <span class="keyword">auto</span> SUM = [&amp;a, &amp;b]() &#123; <span class="keyword">return</span> a + b; &#125;;</span><br><span class="line">  <span class="type">int</span> c = <span class="built_in">SUM</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>类型别名</li></ol><p>类型别名是一个类似于对象的宏，其替换文本可以识别为C ++类型表达式。对于这种，可以使用C++的别名声明来替换：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> A T</span></span><br><span class="line"><span class="comment">// ========================&gt;&gt;&gt;</span></span><br><span class="line"><span class="keyword">using</span> A = T;</span><br></pre></td></tr></table></figure><ol start="3"><li>参数表达式</li></ol><p>参数表达式是一种类似于函数的宏，替换文本后会扩展为表达式或语句。对于这种使用，C++中的最佳实践是使用内联模版函数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> MIN(A, B) ((A) &lt; (B) ? (A) : (B))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ASSIGN(A, B) &#123; B = A; &#125;</span></span><br><span class="line"><span class="comment">// ========================&gt;&gt;&gt;</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T1, <span class="keyword">typename</span> T2&gt;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">auto</span> <span class="title">MIN</span><span class="params">(T1&amp;&amp; A, T2&amp;&amp; B)</span></span></span><br><span class="line"><span class="function">  -&gt; <span class="title">decltype</span><span class="params">(((A) &lt; (B) ? (A) : (B)))</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> ((A) &lt; (B) ? (A) : (B));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T1, <span class="keyword">typename</span> T2&gt;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">ASSIGN</span><span class="params">(T1&amp;&amp; A, T2&amp;&amp; B)</span> </span>&#123;</span><br><span class="line">  B = A;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里引入了内联，自动的推导类型和完美转发等modern c++的特性。完美转发使得调用方可以根据需要决定参数传递的类型。</p><ol start="4"><li>参数化类型别名</li></ol><p>这种其实就是模版别名，在C++11之前需要用宏去实现。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> AliasMap(T) std::map<span class="string">&lt;std::string, T&gt;</span>;</span></span><br><span class="line"><span class="comment">// ========================&gt;&gt;&gt;</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">using</span> AliasMap = std::map&lt;std::string, T&gt;;</span><br></pre></td></tr></table></figure><ol start="5"><li>条件编译</li></ol><p>目前绝大多数开源的C++项目都会依赖宏来进行条件编译，其本质意义是通过定义宏与否来改变某个定义&#x2F;声明。</p><p>比如存在一个绘制三角形的API，但其具体实现会根据操作系统而变化，通过预处理器就可以很好地实现类似的兼容：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">draw_triangle</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="meta">#<span class="keyword">if</span> _WIN32</span></span><br><span class="line">    <span class="comment">// Windows triangle drawing code here </span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="comment">// Linux triangle drawing code here</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中某个分支的代码会在进行编译之前被去掉，这样编译时就不会出现API未定义的错误。</p><p>在C++17中有了新的语法特性<code>if constexpr</code>，我们可以用来替代一部分<code>#if … #else</code>的使用。以下面的使用为例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">do_sth</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="meta">#<span class="keyword">if</span> DEBUG_MODE</span></span><br><span class="line">    <span class="built_in">log</span>();</span><br><span class="line">  <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="comment">// …</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ========================&gt;&gt;&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">do_sth</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(DEBUG_MODE)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">log</span>(); </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="comment">// …</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用<code>if constexpr</code>的好处是其只会检查语法错误，像宏那样的使用方式，一旦<code>DEBUG_MODE</code>出现typo的错误，编译器是无法准确辨识的。</p><p>当然<code>if constexpr</code>的使用也是有其不足之处的，以上面的<code>draw_triangle</code>函数为例，即便某个条件分支不会被使用，你仍然需要有相关冗余的声明。所以对于这种情况，个人建议还是不需要使用<code>if constexpr</code>替代宏。</p><ol start="6"><li>源码位置</li></ol><p>目前几乎所有的断言或者宏会用到宏，比如需要使用<code>__LINE__, __FILE__, __func__</code> 等定位断言的位置，又或者需要断言开关等等。</p><p>要想替代对这些宏的使用则需要用上C++20的<code>std::source_location</code>，该类可以表示关于源码的具体信息，例如文件名、行号以及函数名。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string_view&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;source_location&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">log</span><span class="params">(std::string_view message,</span></span></span><br><span class="line"><span class="params"><span class="function">         <span class="type">const</span> std::source_location&amp; location = std::source_location::current())</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;info:&quot;</span></span><br><span class="line">      &lt;&lt; __FILE__ &lt;&lt; <span class="string">&#x27;:&#x27;</span></span><br><span class="line">              &lt;&lt; __LINE__ &lt;&lt; <span class="string">&#x27; &#x27;</span></span><br><span class="line">              &lt;&lt; message &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">  <span class="comment">// ========================&gt;&gt;&gt;</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;info:&quot;</span></span><br><span class="line">              &lt;&lt; location.<span class="built_in">file_name</span>() &lt;&lt; <span class="string">&#x27;:&#x27;</span></span><br><span class="line">              &lt;&lt; location.<span class="built_in">line</span>() &lt;&lt; <span class="string">&#x27; &#x27;</span></span><br><span class="line">              &lt;&lt; message &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这里提供了一些更“现代”的C++写法来替换不够安全的、使用了宏定义的老式代码，事实上C++的发展过程中一直在提出一些减少预处理宏使用依赖的方案。但从目前来看，还是有不少预处理使用无法替换，即便如此，个人认为适当使用宏和合适的，其AST的生成功能是非常强大的工具，并且某种情况下能使得代码更加易读。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>《cppcon 2019》——<a href="https://www.youtube.com/watch?v=c6NkeF1eChs">https://www.youtube.com/watch?v=c6NkeF1eChs</a></li><li>《Rejuvenating C++ Programs through Demacrofication》——<a href="https://www.stroustrup.com/icsm-2012-demacro.pdf">https://www.stroustrup.com/icsm-2012-demacro.pdf</a></li><li>《if statement》——<a href="https://en.cppreference.com/w/cpp/language/if">https://en.cppreference.com/w/cpp/language/if</a></li><li>《The year is 2017 - Is the preprocessor still needed in C++?》——<a href="https://foonathan.net/2017/05/preprocessor/">https://foonathan.net/2017/05/preprocessor/</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Macro-Free-In-Cpp&quot;&gt;&lt;a href=&quot;#Macro-Free-In-Cpp&quot; class=&quot;headerlink&quot; title=&quot;Macro Free In Cpp&quot;&gt;&lt;/a&gt;Macro Free In Cpp&lt;/h1&gt;&lt;blockquote&gt;
</summary>
      
    
    
    
    
    <category term="C++" scheme="http://yoursite.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>《UCB cs294》Required Reading 2</title>
    <link href="http://yoursite.com/2021/04/14/%E3%80%8AUCB-cs294%E3%80%8BRequired-Reading-2/"/>
    <id>http://yoursite.com/2021/04/14/%E3%80%8AUCB-cs294%E3%80%8BRequired-Reading-2/</id>
    <published>2021-04-13T16:16:18.000Z</published>
    <updated>2021-04-13T16:17:45.360Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《UCB-AI-Sys-cs294》Required-Reading-2"><a href="#《UCB-AI-Sys-cs294》Required-Reading-2" class="headerlink" title="《UCB AI-Sys cs294》Required Reading 2"></a>《UCB AI-Sys cs294》Required Reading 2</h1><h2 id="论文一"><a href="#论文一" class="headerlink" title="论文一"></a>论文一</h2><p>课程推荐的第一篇文章《SysML: The New Frontier of Machine Learning Systems》，这其实一个会议的白皮书，主要的研究方向是设计实现一类系统，来满足支持部署机器学习模型，这是一个计算机系统和机器学习交叉的会议。</p><p>白皮书里将机器学习系统遇到的问题分为两种：一种是高层次的问题，主要解决的是算法、接口的设计实现；另一种则是低层次问题，主要关注的是硬件、调度等底层优化。该文也仔细分析了机器学习系统中遇到的瓶颈，比如部署相关的设计、成本问题以及实用性是否合适。</p><h2 id="论文二"><a href="#论文二" class="headerlink" title="论文二"></a>论文二</h2><p>第二篇论文是《A Few Useful Things to Know About Machine Learning》，这是一片机器学习领域的经典论文，总结了机器学习相关的12个重点实践，并以分类器来举例。</p><ol><li>Learning &#x3D; Representation + Evaluation + Optimization</li></ol><p>所有的机器学习算法都是由三个部分组成：</p><ul><li>Representation：表现数据的方式，比如用距离表现数据的knn、svm，用数表现数据的决策树；</li><li>Evaluation：用来评估分类器好坏的函数；</li><li>Optimization：用来搜索得分最高分类器的方法；</li></ul><ol start="2"><li>It’s Generalization that Counts</li></ol><p>泛化能力是很重要的，在使用分类器数据时，需要留出部分数据来做测试，避免过拟合。</p><ol start="3"><li>Data Alone Is Not Enough</li></ol><p>将泛化能力作为一个指标，仅仅有数据是不够的，还需要大量的编程工作，例如选择合适的模型，合适的评估函数、损失函数。</p><ol start="4"><li>Overfitting Has Many Faces</li></ol><p>过拟合有很多种，主要需要关注的是偏差和方差，偏差是指模型往着相同的错误方向训练，方差则是模型有学习随机信号的倾向。解决过拟合的方法一般有交叉验证、增加正则化项、进行类似卡方检验的统计显著性检验。</p><ol start="5"><li>Intuition Fails in High Dimensions</li></ol><p>一般来说，特征维度越高，就更好表达数据，但也可以引发curse of dimensionality，即样本数量相对不足，难以覆盖其输入空间，并且也难以从直觉上找出不同类别样本之间的合理边界，最终导致bias和variance的增加。</p><ol start="6"><li>Theoretical Guarantees Are Not What They Seem</li></ol><p>机器学习论文中充斥着理论保证，其存在的意义不仅在于作为评断实际决策的标准，还是设计算法的来源动力。但机器学习是一个复杂的工程，理论上可行不代表实践也是可行的。</p><ol start="7"><li>Feature Engineering Is The Key</li></ol><p>这一点主要是将特征工程的重要性，机器学习不单单是构建数据跑一次就足够了，还需要有分析结果、根据结果修改数据集的迭代过程。</p><ol start="8"><li>More Data Beats a Cleverer Algorithm</li></ol><p>数据量非常重要，数据量的增多会导致某些模型的表征能力也随之增强。</p><ol start="9"><li>Learn Many Models, Not Just One</li></ol><p>机器学习中每个模型都有其适用范围，因此模型的集成如bagging、boosting、stacking等算法就会得到很好的结果。</p><ol start="10"><li>Simplicity Does Not Imply Accuracy</li></ol><p>这里主要是Occam’s razor的一个修正，即简单的模型不一定就能很好避免过拟合或者得到很好的效果。</p><ol start="11"><li>Representable Does Not Imply Learnable</li></ol><p>机器学习具备局限性，不是所有的模型都可以学习的。另外，如果评估函数在假设空间内具备多个局部最优点，模型可能会找不到最优函数。</p><ol start="12"><li>Correlation Does Not Imply Causation</li></ol><p>机器学习只能发觉特征的相关性，但相关性并不等于因果性。</p><h2 id="论文三"><a href="#论文三" class="headerlink" title="论文三"></a>论文三</h2><p>第三篇文章《A Berkeley View of Systems Challenges for AI》是伯克利从计算机系统对机器学习的支持中，总结出来的一篇文章。</p><p>该文章将AI飞速发展的原因归结为：大数据、高扩展性的计算机系统和开源软件技术的流行。</p><p>文章还提出了机器学习相关的趋势与挑战：</p><ol><li>Mission-critical AI：人工智能开始设计一些与人类生命安全相关的领域，需要为这些机器学习任务设计更加稳定安全的决策；</li><li>Personalized AI：提供更加个性化的人工智能系统，同时需要注意用户隐私安全；</li><li>AI across organizations：每个机构、企业都有自己独特的数据，如何提供数据共享的机制，支持跨组织的人工智能系统，也是一个需要注意的挑战；</li><li>AI demands outpacing the Moore’s Law：后摩尔定律时期的AI发展需要更加关注与人工智能适配的硬件架构与系统；</li></ol><p>接下来的介绍就是关于解决上述挑战亟需深入研究的方向：</p><ol><li>Acting in dynamic environments：动态环境下的技术表现，人工智能需要在复杂性动态性更强的环境工作，能够应对突发的、不可预测的事件，并快速做出响应。这包括了Continual learning、Reinforcement learninig等系统的构建；作出更鲁棒的决策（Robust decisions）和可解释的决策（Explainable decisions）</li><li>Secure AI：这里的安全分为两个部分，一是攻击影响系统作出决策的正确性、而是攻击者获取AI训练的影响数据、破解AI加密模型。这种方向包括了构建 Secure enclaves，提供一个安全的硬件执行环境；进行对抗学习避免推理阶段和训练阶段引入了恶意的数据；构建更安全的共享数据系统；</li><li>AI-specic architectures：随着AI的发展，硬件系统架构的迭代显得越来越来重要。这包括了 Domain specic hardware，设计专用的硬件架构来提升系统性能和安全能力；Composable AI systems，为AI系统做定制的的模块化、组件化，进行模型的组合、操作行为的组合；Cloud-edge systems，设计合适的连接云端与边缘设备的AI系统，降低边缘设备的延时，充分利用云端的能力来提供更复杂的计算模型和高效的决策。</li></ol><p>下图就是上面四大趋势与九大研究方向的关联关系：</p><p><img src="https://img.imgdb.cn/item/6075c41e8322e6675cebd742.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这是这个课程的week2内容，主要是介绍了一些机器学习系统的研究方法和关注的趋势挑战。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《UCB-AI-Sys-cs294》Required-Reading-2&quot;&gt;&lt;a href=&quot;#《UCB-AI-Sys-cs294》Required-Reading-2&quot; class=&quot;headerlink&quot; title=&quot;《UCB AI-Sys cs294》Re</summary>
      
    
    
    
    
    <category term="AiSys" scheme="http://yoursite.com/tags/AiSys/"/>
    
  </entry>
  
  <entry>
    <title>Paxos Made Simple——论文阅读</title>
    <link href="http://yoursite.com/2021/03/20/Paxos-Made-Simple%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/03/20/Paxos-Made-Simple%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-03-20T15:36:18.000Z</published>
    <updated>2021-03-20T15:36:48.577Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Paxos-Made-Simple"><a href="#Paxos-Made-Simple" class="headerlink" title="Paxos Made Simple"></a>Paxos Made Simple</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Paxos——一个用于实现容错的分布式系统算法，核心是一个一致性算法——“synod”算法。基本上是根据一个一致性算法所必需满足的条件而呈现出来的，完整的Paxos算法会通过作为应用到使用状态机的分布式系统中的一致性实现部分来得出。</p><h2 id="The-Consensus-Algorithm"><a href="#The-Consensus-Algorithm" class="headerlink" title="The Consensus Algorithm"></a>The Consensus Algorithm</h2><h3 id="The-Problem"><a href="#The-Problem" class="headerlink" title="The Problem"></a>The Problem</h3><p>假设存在一个多进程集合，里面每个进程都可以发出提案，那么一致性算法需要保证一个值能够被选定，而且一旦一个值被选定，所有进程都需要能够获知选定值。</p><p>一致性safety的要求就是：</p><ul><li>只有被提出的值能够被选定；</li><li>只能有一个选定值；</li><li>只有一致同意该值，才能周知给集合里的所有进程；</li></ul><p>这里主要关注算法的safety，而不会明确要求liveness。</p><p>在该一致性算法中，有三种角色：proposers，acceptors和learners，实际实现中，一个独立的进程可以充当不止一种角色。论文的假设是基于传统的异步模型，而不是拜占庭问题模型。</p><h3 id="Choosing-a-Value"><a href="#Choosing-a-Value" class="headerlink" title="Choosing a Value"></a>Choosing a Value</h3><p>该算法在提案过程主要包括下面几个步骤</p><p>对于proposer：</p><ul><li>proposer选择一个新的提案编号n，然后向acceptors集合的每个成员发送请求，要求acceptors对请求作出响应：<ul><li>a. 一个承诺，保证不再通过任何编号小于n的提案；</li><li>b. 如果接受过其他提案的话，需要返回当前通过的编号小于n的最大编号提案；</li></ul></li><li>如果proposer从大多数acceptors收到期待的响应，则可以接着提议一个编号为n并且值为v的提案，这里的v要么是自由选择一个值（所有的响应都没接收过任何的提案），要么是从上面b响应中选出最大编号的提案的值；</li></ul><p>对于acceptor，acceptors可能会收到prepare请求和accept请求，acceptors可以忽略任何请求而不用担心算法的正确性。至于acceptors在什么情况下可以对一个请求作出回应呢，对于prepare请求可以在任何时候做出响应，而对accept请求，只要它没响应过任何编号大于n的prepare请求， acceptor就可以接受编号为n的提案。</p><p>总结起来，acceptor和proposer的算法操作可以分为两个阶段：</p><ul><li>阶段一</li></ul><p>a. proposer提出一个编号为n的提案，向大多数acceptors发送一个带有编号为n的prepare请求；</p><p>b. 如果acceptors收到了该请求，并且n比它之前响应过的prepare请求编号都大，那么它就会对该请求作出响应，返回一个保证不再通过任何编号小于n的提案的承诺，以及如果存在的话，接受过的最大编号的提案；</p><ul><li>阶段二</li></ul><p>a. 如果proposer从大多数acceptors收到响应，则会提出一个accept请求，内容包括了编号n和值为v，其中v要么是自由选择一个的值，要么是从响应中选出最大编号的提案的值；</p><p>b. 如果acceptors收到该accept请求，并且之前没有响应过大于编号n的prepare请求，那么它就会对接受该请求；</p><h3 id="Learning-a-Chosen-Value"><a href="#Learning-a-Chosen-Value" class="headerlink" title="Learning a Chosen Value"></a>Learning a Chosen Value</h3><p>为了获取到选定的值，learner必须要找出某个以及被大多数acceptors接受的提案。如果是每个acceptors都将通过的提案告知所有的learners，那么通信次数等于两者个数乘积；如果是只告诉一个特定learner，虽然通信次数减少了，但可靠行也降低了；更一般情况是，将它们的通过提案信息发送给一个特定的learners集合，其中的每个learner都可以将该信息告知所有的learners。</p><p>由于信息的丢失，learners可能无法确定一个值是否有一个大多数的acceptors通过了，为了确定选定的值，必须重新发起一次新的提案。</p><h3 id="Progress"><a href="#Progress" class="headerlink" title="Progress"></a>Progress</h3><p>假设存在这样一个场景，两个proposers轮流提议一系列递增编号的提案，但无一通过：Proposer p提出一个编号为n1的提案并且完成了phase1，然后另一个Proposer q为编号为n2(n2&gt;n1)的提案完成了phase1。因此n1提案的accept请求会被忽略，从而触发使用一个新的编号n3(n3&gt;n2)重新开始并完成phase1，同理又导致前面编号为n2的提案的accept请求被忽略。</p><p>为了保证progress的进行，必须选择一个特定proposer来作为唯一一个提议提案的。如果这个proposer可以和半数以上的acceptors通信，同时使用一个比现有通过编号都大的编号作为提案的话，就可以产生一个成功通过的提案。</p><blockquote><p>The famous result of Fischer, Lynch, and Pat- terson [1] implies that a reliable algorithm for electing a proposer must use either randomness or real time—for example, by using timeouts. However, safety is ensured regardless of the success or failure of the election.</p></blockquote><p>无论选举是否成功，proposer选举算法的安全性都是可以得到保证的。</p><h3 id="The-Implementation"><a href="#The-Implementation" class="headerlink" title="The Implementation"></a>The Implementation</h3><p>Paxos算法假设了一个多进程网络，在该算法里，每个进程都扮演了proposer，acceptor及learner的角色。Paxos算法通过选定一个leader来扮演上面提到的特定learner和proposer。Paxos一致性算法就是上述所描述的，其中请求和响应都作为普通消息发送。acceptor在发出响应消息之前，会需要可靠性存储来记录信息。</p><p>接下来就是描述一种提案编号唯一性的机制了，不同的proposer从不相交的编号集合中选择编号，并且每个proposer都会在存储设备上记录目前生成的最大编号，然后使用一个更大的编号来开始phase 1。</p><h2 id="Implementing-a-State-Machine"><a href="#Implementing-a-State-Machine" class="headerlink" title="Implementing a State Machine"></a>Implementing a State Machine</h2><p>实现分布式系统的一种简单方式是由一组客户端向一个中央服务器发出命令请求，该服务器可以看作是一个按顺序执行客户端命令的状态机。但使用单个服务器的可用性较低，因此想到了可以使用一组服务器，每个服务器独立地实现同样的状态机，只要所有服务器都产生一致的状态和输出，那么发出命令的客户端就可以采用任意一个服务器的输出了。</p><p>然而为了所有服务器的命令序列一致，需要实现一系列独立的paxos一致性算法的实例。其中第i个选定的值就是序列中的第i 个状态机命令。每一个服务器都在每一个实例中扮演这个算法的所有角色。</p><p>假设服务器的集合是固定的，一般情况下一个独立的服务器被选为了leader，它就会扮演特定的proposer角色，多个客户端发送命令到leader，leader会决定每个命令的顺序。假设存在某条命令的序号为135，那么它就会通过一致性算法的第135个实例来选定一个提案，这里的命令就是提案的值。这个提案可能成功也可能失败，失败的原因可能来自机器故障，或者是存在另一个服务器认为自己是leader，从而判断了135实例存在其他值。但该算法能够保证最多只有一个命令被选定。</p><p>这个策略的关键在于，Paxos算法中被提出的值只有phase 2才能被选定。前面说过的，phase 1完成时，要么提案的值已经确定，要么proposer可以自由提出一个值。这是正常工作的情况，论文还提到了异常情况：前一个leader失败了，选举了新leader。</p><p>新leader选出后会成为learner，假设它知道命令1-134,138及139，即对应实例，此时它需要执行实例135-137以及所有大于139的实例的phase 1。假设执行结果表明，实例135和140中被提出的提案值已经确定，但其他实例没有限制，那么该leader就可以执行实例135和140的phase 2，选定135和140的命令。</p><p>此时136和137还没确定，leader可以选择接下来的客户端请求作为命令136和137，也可以提起一个特殊的”noop”命令来填补这两个空缺。此处，noop命令不会改变状态机状态，也可以快速填补空缺。一旦这些noop命令选定了，138-140 号命令就可以被执行了。</p><p>由此1-140命令都被选定了，leader就可以继续往下推进所有大于140的实例了。</p><p>接下来讲讲空缺的产生，leader可以在提出命令141被选定之前，先提出命令142，但发送的关于141的信息可能会全部丢失，因此其他服务器可能先知道了142命令的选定，而不知道选择了什么作为命令141。这就产生了空缺。</p><p>由于 leader 的故障以及新 leader 的选举都是比较罕见的情况，因此执行状态机命令并达成一致的成本主要是phase 2的成本。在所有的一致性算法中， paxos一致性算法的phase 2的时间复杂度可能是最小的，因此paxos算法基本就是最优的。</p><p>特殊情况下，leader选举失败，导致出现多个“疑似”的leader，但paxos算法的安全性仍然可以保证不会同时有两个命令被选为第i个状态机命令。</p><p>如果服务器的集合是变化的，那么也存在某种方式来决定哪些服务器可以作为这个一致性算法的实力，论文提到的方式是通过状态机自身来实现，即当前的服务器集合作为状态的一部分，比如将在执行完第i个状态机命令后标识的服务器集合，作为一致性算法执行实例i+a的服务器集合。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Paxos-Made-Simple&quot;&gt;&lt;a href=&quot;#Paxos-Made-Simple&quot; class=&quot;headerlink&quot; title=&quot;Paxos Made Simple&quot;&gt;&lt;/a&gt;Paxos Made Simple&lt;/h1&gt;&lt;h2 id=&quot;Intro</summary>
      
    
    
    
    
    <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Better I/O Through Byte-Addressable, Persistent Memory——论文学习</title>
    <link href="http://yoursite.com/2021/03/07/Better-I-O-Through-Byte-Addressable-Persistent-Memory%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2021/03/07/Better-I-O-Through-Byte-Addressable-Persistent-Memory%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-03-06T17:26:02.000Z</published>
    <updated>2021-03-06T17:26:37.033Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Better-I-x2F-O-Through-Byte-Addressable-Persistent-Memory"><a href="#Better-I-x2F-O-Through-Byte-Addressable-Persistent-Memory" class="headerlink" title="Better I&#x2F;O Through Byte-Addressable, Persistent Memory"></a>Better I&#x2F;O Through Byte-Addressable, Persistent Memory</h1><blockquote><p>现代的计算机系统一般是通过基于块的接口来缓慢地访问持久性存储的，但近年来，像Phase-Change Memory这种基于字节寻址的持久性存储技术提供了更快速、和更细粒度的访问方式。本论文介绍了新的文件系统和硬件体系结构，具备基于字节寻址持久性内存的属性。</p></blockquote><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>新的基于字节寻址的持久性存储技术（BPRAM）消除了volatile和non- volatile存储之间的许多传统差异，尤其是随着Phase-Change Memory和memristors等技术的发展，也可像DRAM一样按字节寻址，同时能够像磁盘一样持久化。</p><p>本文通过研究文件系统来探索BPRAM的好处，并为BPRAM线了一个新的文件系统BPFS，提供了远快于传统基于块存储设备的文件系统的速度。此外，与现有系统相比，BPFS通过一种short-circuit shadow paging的新技术来提供了强大的安全性和一致性保证。</p><p>BPFS的存储方法在一些重要方面与传统文件系统也存在不同，包括不将DRAM缓冲区高速缓存用于文件系统数据，针对小型随机写入进行优化，减小了尚未持久的数据漏洞窗口。</p><h2 id="DESIGN-PRINCIPLES"><a href="#DESIGN-PRINCIPLES" class="headerlink" title="DESIGN PRINCIPLES"></a>DESIGN PRINCIPLES</h2><p>论文主要关注两个目标：</p><ul><li>设计对BPRAM的体系结构支持；</li><li>设计一个文件系统，以利用BPRAM的属性来提高性能和可靠性；</li></ul><h3 id="Expose-BPRAM-Directly-to-the-CPU"><a href="#Expose-BPRAM-Directly-to-the-CPU" class="headerlink" title="Expose BPRAM Directly to the CPU"></a>Expose BPRAM Directly to the CPU</h3><p>传统的永久性存储位于总线控制器和存储控制器的后面，由于对这些控制器的访问带来的性能损耗，即使是最快的NAND闪存SSD，延迟也要几十微秒。</p><p>论文的做法是，将BPRAM直接与DRAM并排放置在内存总线上，使得CPU能够在BPRAM的地址上加载和存储，降低访问延迟。另外，BPRAM的可寻址还能够利用高速缓存的层次结构来提高对持久性存储器的写入性能。</p><p>但将BPRAM放在内存总线上也是有一些缺点：</p><ul><li>BPRAM的流量有可能会干扰易失性存储器的访问并进一步损害整体系统性能；</li><li>系统中可用的BPRAM数量受BPRAM密度和计算机中可用DIMM插槽数量的限制；</li><li>若应用或驱动程序存在缺陷，则可能导致杂散写入，即stray write；</li></ul><p>因此不建议用BPRAM完全替代DRAM，虽然论文也提到一些论据证明这几个缺点不是很大问题的。</p><h3 id="Enforce-Ordering-and-Atomicity-in-Hardware"><a href="#Enforce-Ordering-and-Atomicity-in-Hardware" class="headerlink" title="Enforce Ordering and Atomicity in Hardware"></a>Enforce Ordering and Atomicity in Hardware</h3><p>为了保证安全性和一致性，文件系统需要清楚写入持久性存储的顺序和时间。但执行强制性的排序约束会对性能有一定的影响。文中提出了一种软件机制来声明对硬件的排序约束，软件可以发出特殊的写屏障，以分隔一组epoch的写操作，而硬件将保证每个epoch都按顺序写回到主存器中。</p><p>除了对顺序的限制外，文件系统还考虑对应对故障原子性的问题，如果由于电源故障而中断对持久性存储的写操作，则该存储器可能会处于中间状态，从而破坏了一致性。借助BPRAM，可以直接在硬件中提供一个简单的原子写入基元。</p><h3 id="Use-Short-Circuit-Shadow-Paging"><a href="#Use-Short-Circuit-Shadow-Paging" class="headerlink" title="Use Short-Circuit Shadow Paging"></a>Use Short-Circuit Shadow Paging</h3><p>大多数存储系统都使用以下两种方式来确保可靠性：WAL预写日志和影子分页shadow paging。WAL会使得大多数写入需要进行两次操作，而shadow paging则是实用写时复制来执行所有更新。由于shadow paging每次写入都会因为传播到文件系统根目录树而输出多个块，shadow paging的副本成本远超日志记录，因此目前应用都是使用前者。</p><p>但BPRAM的字节寻址能力和快速的随机写入使影子分页成为文件系统设计的一种高效方法，BPFS通过实现一种称为短路影子寻呼（SCSP）的新技术，允许BPFS在文件系统树中的任何位置提交更新，从而避免了将副本传播到文件系统根目录所产生的开销。</p><h2 id="BPFS-DESIGN-AND-IMPLEMENTATION"><a href="#BPFS-DESIGN-AND-IMPLEMENTATION" class="headerlink" title="BPFS DESIGN AND IMPLEMENTATION"></a>BPFS DESIGN AND IMPLEMENTATION</h2><h3 id="File-System-Layout"><a href="#File-System-Layout" class="headerlink" title="File System Layout"></a>File System Layout</h3><p>BPFS的持久数据结构组织成了一个具备固定大小的块的树，使得能够原子更新树的任意部分，并且块大小固定使得释放和分配都比较方便。</p><p>BPFS数据结构由三种文件组成，每种文件都由相同的树数据结构表示。</p><ul><li>索引节点文件是一个包含固定大小的索引节点数组的单个文件，每个索引节点表示文件系统中的某个文件或目录；</li><li>目录文件包含目录项数组，该目录项数组由inumber（即inode文件中inode的索引）和相应文件的名称组成；</li><li>数据文件仅包含用户数据；</li></ul><p>所有文件都是由相同的数据结构组成的，即一棵全由4K块组成的树。树的叶节点代表文件的数据（即用户数据，目录条目或索引节点），每棵树的内部节点包含了指向树的下一级的512个64位指针。文件系统的根结点就是inode文件。</p><p><img src="https://img.imgdb.cn/item/601ed79e3ffa7d37b3abbef5.png"></p><p>每个树的高度由树的根指针的低位所表示，这使得BPFS可以通过记住从中获取的数来确定给定的块是内部节点还是叶子节点。对于高度为0的树，根指针直接指向一个数据块，该数据块最多可以包含4KB的文件数据。在高度树为1的情况下，根指针指向512个指针的内部块，每个指针指向4KB数据块，总共2 MB。以此类推。内部节点没有存储文件数据。</p><p>为了简化将数据写入文件中间的任务，我们在树的任何级别使用空指针，以此表示该指针跨越的文件某个范围内的零数据。例如，如果文件的根指针是高度为5的空指针，则它表示一个空的256TB文件。空指针也可以出现在内部节点上，此文件就可以实现大型的稀疏文件的紧凑表示形式。另外，还会存储每个文件的大小以及每个根指针，若文件较大，则假定文件的尾部为零；若文件较小，则忽略文件末尾在树中的任何数据。这样能够在不更新树本身的情况下更改文件大小。</p><h3 id="Persistent-Data-Updates"><a href="#Persistent-Data-Updates" class="headerlink" title="Persistent Data Updates"></a>Persistent Data Updates</h3><p>Short-circuit shadow paging通过三种不同的方法来更新持久性数据：</p><ul><li>就地更新：由于硬件能保证这些更新是原子的，因此能对64位或更少位数的写入执行就地更新。</li><li>就地追加：就地追加利用了每个文件的根指针附带着文件大小变量。由于超出文件大小的所有数据都将被忽略，因此可以安全地就地写入这些位置，并且一旦写入了所有数据，我们就可以自动更新文件大小来扩展有效数据范围；</li><li>写时复制：在将受此操作影响的树的所有部分上执行写时复制，直到可以通过一次写操作可以提交变更的最少部分。</li></ul><p><img src="https://img.imgdb.cn/item/60217ded3ffa7d37b3d71967.png"></p><p>对于所有这些操作，必须要在提交该操作的原子写入之前和之后发出epoch barriers。这些屏障确保了提交之前所有写操作都先将被刷新到BPRAM，并且任何后续的文件系统操作都将在提交之后进行。</p><h3 id="Volatile-Data-Structures"><a href="#Volatile-Data-Structures" class="headerlink" title="Volatile Data Structures"></a>Volatile Data Structures</h3><p>该文件系统布局允许对持久状态进行高效可靠的更新，因此暂不允许将诸如哈希表之类的复杂数据结构存储在持久内存中。但考虑到这些复杂数据结构可以提高性能，因此在易失性内存中维护一些派生的数据结构。这里介绍了三个：</p><ul><li>在DRAM中存储的空闲BPRAM块列表以及释放或者分配的inumber列表，这些数据结构在每次启动时都从文件系统元数据初始化。</li><li>正在进行的写时复制操作中已释放和已分配的块列表。</li><li>第三个数据结构存储用户打开的每个目录中目录条目的缓存。</li></ul><h3 id="File-System-Operations"><a href="#File-System-Operations" class="headerlink" title="File System Operations"></a>File System Operations</h3><p>由于所有BPFS文件类型都使用BPFS树数据结构，因此的论文实现了一组核心routines——crawler，它们可以遍历这些树并可以对三种文件执行读写操作。为了执行这些操作，需要为crawler提供根指针，树的高度，文件偏移范围和回调函数。crawler到达叶节点后，它将使用适当的地址调用回调。</p><p>crawler负责更新树的高度和内部指针。更新高度的操作：先查看请求的文件偏移量是否超出当前文件树所覆盖的偏移量，如果超过了，则以原子操作使树的高度增加适当的数量。</p><p>在叶节点上，crawler将调用一个回调，如果该回调希望执行写时复制操作，它将分配一个新块，执行任何必要的更新，然后必须适当地更新任何内部节点。如果回调未进行任何修改，则crawler将返回未触及的现有指针块。如果回调仅修改了一个指针，那么crawler将就地提交该操作。如果修改了多个指针，crawler将对该指针块进行完整复制，将提交推迟到树中的高层节点。</p><p>下面是单个的文件系统操作：</p><ul><li>Open：打开文件后，BPFS会解析路径并使用目录项缓存来查找目标文件或目录；如果该文件不存在，并且请求创建，则从可用列表中声明一个新的inumber，然后以适当的偏移量将一个新的inode写入inode文件。写完后，将一个新目录项写入包含文件的目录中，最后更新易失性存储器中的目录条目缓存；</li><li>Read：读取文件时，BPFS在文件的适当范围内调用crawler。读取的回调将数据块中的数据复制到用户提供的缓冲区中，然后使用就地原子写入来更新访问时间；读取目录则是将目录加载到目录条目缓存（如果尚未缓存）中；</li><li>Write：写入文件时，可能需要对inode本身执行写时复制操作。顶层crawler对inode文件进行操作，并找到目标文件的inode，然后在此文件的适当范围上调用写crawler，并确定是否可以就地更新，如果不可以则使用写时复制。如果需要同时更新文件大小和inode内文件的根指针，将对inode块本身执行写时复制，然后将新版本返回给inode文件；</li><li>Close：关闭文件或目录后，BPFS会检查该文件或目录是否已标记为删除。如果是，crawler则到目录条目的位置写入inumber为0来表示删除。最后则更新易失性数据结构，包括空闲块列表和空闲inumber列表；</li></ul><h3 id="Multiprocessor-Operation"><a href="#Multiprocessor-Operation" class="headerlink" title="Multiprocessor Operation"></a>Multiprocessor Operation</h3><p>BPFS保证将更新按顺序提交给BPRAM。在单处理器系统上，epoch barrier通过按照创建它们的顺序将从缓存子系统中拿到epoch来强制执行此保证。</p><p>对于多处理器的情况，硬件修改可确保如果在两个不同的CPU上发出了共享状态的两个epoch，那么这些epoch将被序列化。但如果进程或线程在两个不同的CPU上执行时更新了两个不同的状态，则可以按任何顺序将更新写回PCM。为了正确实现这些更新，必须考虑三种情况：</p><ul><li>可以在单个文件系统操作期间在多个CPU上调度线程；</li><li>可以在两个不同的文件系统操作之间将线程切换到新的CPU；</li><li>两个进程可以在两个不同的CPU中更新文件系统中的两个不同位置；</li></ul><p>BPFS的当前实现尚未强制执行前两个约束。</p><h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>BPFS也存在一些局限性：</p><ul><li>其一是写入时间不像写入本身进行原子更新，这是基于性能的折衷考虑；</li><li>另一个局限性是跨越树一大块的原子操作可能需要大量额外的副本；</li><li>还有一个局限是BPRAM的整体接口实现了新的文件系统，并没有提供持久化的用户级堆；</li></ul><h2 id="HARDWARE-SUPPORT"><a href="#HARDWARE-SUPPORT" class="headerlink" title="HARDWARE SUPPORT"></a>HARDWARE SUPPORT</h2><h3 id="Phase-Change-Memory"><a href="#Phase-Change-Memory" class="headerlink" title="Phase Change Memory"></a>Phase Change Memory</h3><p>Phase change memory即PCM是一种非易失性且基于字节寻址的新型存储技术，能提供与DRAM相近的访问速度，也可以组织成类似于DRAM的阵列结构。本论文的假设是基于PCM的存储系统被组织成放置在与DDR兼容的DIMM中一组PCM芯片。</p><h3 id="Wear-Leveling-and-Write-Failures"><a href="#Wear-Leveling-and-Write-Failures" class="headerlink" title="Wear Leveling and Write Failures"></a>Wear Leveling and Write Failures</h3><p>尽管PCM比一般的NAND闪存的写耐久性更高，但考虑到PCM放置在存储器总线上而不是I&#x2F;O总线上，单元将暴露于更大更多的写入活动，因此需要进行耗损均衡：</p><ul><li>最小化写入的方式设计PCM阵列，延长使用寿命；</li><li>在每个页面内，通过旋转内存控制器级别的位来使损耗均匀；</li><li>在页面之间，可以通过定期交换虚拟页面到物理页面来使损耗均匀映射；</li></ul><h3 id="Enforcing-Atomicity"><a href="#Enforcing-Atomicity" class="headerlink" title="Enforcing Atomicity"></a>Enforcing Atomicity</h3><p>为了对8字节的写入保证原子性，必须要确保在电源故障的情况下，写入要么完全完成（所有位适当更新），要么完全失败（所有位都处于原始状态）。论文建议通过增加DIMM的容量来增强原子性，使得该电容器具有足够的能量来完成PCM子系统中正在进行的最大写入事务数。</p><h3 id="Enforcing-Ordering"><a href="#Enforcing-Ordering" class="headerlink" title="Enforcing Ordering"></a>Enforcing Ordering</h3><p>现代的高速缓存和内存控制器可以重新排列从CPU到内存的写入顺序。考虑到使用BPRAM代替DRAM，写回发生的顺序会变的很重要，例如如果高速缓存控制器在选择在写回缓冲区之前先写回指针更新，则BPRAM中的文件系统将不一致，这种不一致性一般会因为高速缓存一致性和内存屏障机制变得不可见。但如果在所有数据都写回到BPRAM之前发生电源故障，则重新引导计算机时文件系统将变得不一致。为了避免这种情况，需要遵守任何排序约束。</p><p>强制排序有多种选择。一种可能是使用直写式缓存；第二种是在每个内存屏障处刷新整个缓存，以确保所有数据都以正确的顺序到达非易失性内存中；第三种是跟踪在操作期间已修改的所有高速缓存行，以便仅刷新包含脏文件系统数据的行。</p><p>这几种方法都有明显的问题，论文的解决方法是允许软件将排序约束明确地传达给硬件，即epoch barrier。epoch是从同一线程向持久性存储器进行写入的序列，由软件发出的新型存储器屏障来界定。</p><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>本文主要介绍了一种文件系统，支持按字节寻址和持久化内存，同时也介绍了一种硬件体系来确保原子性和顺序保证。新型文件系统使用了short-circuit shadow paging的技术来提供较强的安全性和一致性保证。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Better-I-x2F-O-Through-Byte-Addressable-Persistent-Memory&quot;&gt;&lt;a href=&quot;#Better-I-x2F-O-Through-Byte-Addressable-Persistent-Memory&quot; clas</summary>
      
    
    
    
    
    <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
</feed>
